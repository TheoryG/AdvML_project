{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wed14*6.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"IONIiCZR9_kz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"ad208929-cb02-4e01-9692-9525b7e5738a","executionInfo":{"status":"ok","timestamp":1544812250117,"user_tz":300,"elapsed":59876,"user":{"displayName":"Jianfeng Zhang","photoUrl":"","userId":"00276569247713701028"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 110377 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"qaHT4EA4-GvM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a3c7840a-13e1-41f3-c05b-dbf7151327e3","executionInfo":{"status":"ok","timestamp":1544812260194,"user_tz":300,"elapsed":6590,"user":{"displayName":"Jianfeng Zhang","photoUrl":"","userId":"00276569247713701028"}}},"cell_type":"code","source":["!mkdir drive\n","!google-drive-ocamlfuse drive\n","!ls drive/DL-project"],"execution_count":2,"outputs":[{"output_type":"stream","text":[" Residual-net.ipynb   web1.ipynb  'wed14*6.ipynb'\n"],"name":"stdout"}]},{"metadata":{"id":"SfnMwZwy-JUk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3e46a467-e834-4c1e-9c63-0c0bbf2a005c","executionInfo":{"status":"ok","timestamp":1544812262075,"user_tz":300,"elapsed":832,"user":{"displayName":"Jianfeng Zhang","photoUrl":"","userId":"00276569247713701028"}}},"cell_type":"code","source":["cd /content/drive/DL-project"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/DL-project\n"],"name":"stdout"}]},{"metadata":{"id":"krVNCtpg-Lqe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":8515},"outputId":"41250099-8158-4b89-bb82-17c7cfdd8e3e","executionInfo":{"status":"ok","timestamp":1544822078638,"user_tz":300,"elapsed":9803940,"user":{"displayName":"Jianfeng Zhang","photoUrl":"","userId":"00276569247713701028"}}},"cell_type":"code","source":["import keras\n","import numpy as np\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers import Conv2D, Dense, Input, add, Activation, Flatten, AveragePooling2D\n","from keras.callbacks import LearningRateScheduler, TensorBoard\n","from keras.regularizers import l2\n","from keras import optimizers\n","from keras.models import Model\n","\n","DEPTH              = 14\n","WIDE               = 6\n","\n","IN_FILTERS         = 16\n","\n","CLASS_NUM          = 10\n","IMG_ROWS, IMG_COLS = 32, 32\n","IMG_CHANNELS       = 3\n","\n","BATCH_SIZE         = 128\n","EPOCHS             = 200\n","ITERATIONS         = 30000 // BATCH_SIZE + 1\n","WEIGHT_DECAY       = 0.0005\n","LOG_FILE_PATH      = './w_resnet/'\n","\n","\n","from keras import backend as K\n","# set up the GPU memory \n","\n","\n","if('tensorflow' == K.backend()):\n","    import tensorflow as tf\n","    from keras.backend.tensorflow_backend import set_session\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","\n","def scheduler(epoch):\n","    if epoch < 60:\n","        return 0.1\n","    if epoch < 120:\n","        return 0.02\n","    if epoch < 160:\n","        return 0.004\n","    return 0.0008\n","\n","def color_preprocessing(x_train,x_test):\n","    x_train = x_train.astype('float32')\n","    x_test = x_test.astype('float32')\n","    mean = [125.3, 123.0, 113.9]\n","    std  = [63.0,  62.1,  66.7]\n","    for i in range(3):\n","        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n","        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n","\n","    return x_train, x_test\n","\n","def wide_residual_network(img_input,classes_num,depth,k):\n","    print('Wide-Resnet %dx%d' %(depth, k))\n","    n_filters  = [16, 16*k, 32*k, 64*k]\n","    n_stack    = (depth - 4) // 6\n","\n","    def conv3x3(x,filters):\n","        return Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding='same',\n","        kernel_initializer='he_normal',\n","        kernel_regularizer=l2(WEIGHT_DECAY),\n","        use_bias=False)(x)\n","\n","    def bn_relu(x):\n","        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n","        x = Activation('relu')(x)\n","        return x\n","\n","    def residual_block(x,out_filters,increase=False):\n","        global IN_FILTERS\n","        stride = (1,1)\n","        if increase:\n","            stride = (2,2)\n","            \n","        o1 = bn_relu(x)\n","        \n","        conv_1 = Conv2D(out_filters,\n","            kernel_size=(3,3),strides=stride,padding='same',\n","            kernel_initializer='he_normal',\n","            kernel_regularizer=l2(WEIGHT_DECAY),\n","            use_bias=False)(o1)\n","\n","        o2 = bn_relu(conv_1)\n","        \n","        conv_2 = Conv2D(out_filters, \n","            kernel_size=(3,3), strides=(1,1), padding='same',\n","            kernel_initializer='he_normal',\n","            kernel_regularizer=l2(WEIGHT_DECAY),\n","            use_bias=False)(o2)\n","        if increase or IN_FILTERS != out_filters:\n","            proj = Conv2D(out_filters,\n","                                kernel_size=(1,1),strides=stride,padding='same',\n","                                kernel_initializer='he_normal',\n","                                kernel_regularizer=l2(WEIGHT_DECAY),\n","                                use_bias=False)(o1)\n","            block = add([conv_2, proj])\n","        else:\n","            block = add([conv_2,x])\n","        return block\n","\n","    def wide_residual_layer(x,out_filters,increase=False):\n","        global IN_FILTERS\n","        x = residual_block(x,out_filters,increase)\n","        IN_FILTERS = out_filters\n","        for _ in range(1,int(n_stack)):\n","            x = residual_block(x,out_filters)\n","        return x\n","\n","\n","    x = conv3x3(img_input,n_filters[0])\n","    x = wide_residual_layer(x,n_filters[1])\n","    x = wide_residual_layer(x,n_filters[2],increase=True)\n","    x = wide_residual_layer(x,n_filters[3],increase=True)\n","    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D((8,8))(x)\n","    x = Flatten()(x)\n","    x = Dense(classes_num,\n","        activation='softmax',\n","        kernel_initializer='he_normal',\n","        kernel_regularizer=l2(WEIGHT_DECAY),\n","        use_bias=False)(x)\n","    return x\n","\n","if __name__ == '__main__':\n","\n","    # load data\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    y_train = keras.utils.to_categorical(y_train, CLASS_NUM)\n","    y_test = keras.utils.to_categorical(y_test, CLASS_NUM)\n","    \n","    # color preprocessing\n","    x_train, x_test = color_preprocessing(x_train, x_test)\n","\n","    # build network\n","    img_input = Input(shape=(IMG_ROWS,IMG_COLS,IMG_CHANNELS))\n","    output = wide_residual_network(img_input,CLASS_NUM,DEPTH,WIDE)\n","    resnet = Model(img_input, output)\n","    print(resnet.summary())\n","    # set optimizer\n","    sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n","    resnet.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","\n","    # set callback\n","    tb_cb = TensorBoard(log_dir=LOG_FILE_PATH, histogram_freq=0)\n","    change_lr = LearningRateScheduler(scheduler)\n","    cbks = [change_lr,tb_cb]\n","\n","    # set data augmentation\n","    print('Using real-time data augmentation.')\n","    datagen = ImageDataGenerator(horizontal_flip=True,\n","            width_shift_range=0.125,height_shift_range=0.125,fill_mode='reflect')\n","\n","    datagen.fit(x_train)\n","\n","    # start training\n","    resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n","                        steps_per_epoch=ITERATIONS,\n","                        epochs=EPOCHS,\n","                        callbacks=cbks,\n","                        validation_data=(x_test, y_test))\n","    resnet.save('wresnet.h5')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 51s 0us/step\n","Wide-Resnet 14x6\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 96)   13824       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 96)   384         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 96)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 96)   82944       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 96)   1536        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 32, 32, 96)   0           conv2d_3[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 96)   384         add_1[0][0]                      \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 96)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 16, 16, 192)  165888      activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 16, 16, 192)  768         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 16, 16, 192)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 16, 16, 192)  331776      activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 192)  18432       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 16, 16, 192)  0           conv2d_6[0][0]                   \n","                                                                 conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 16, 16, 192)  768         add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 16, 16, 192)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 8, 8, 384)    663552      activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 8, 8, 384)    1536        conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 8, 8, 384)    0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 8, 8, 384)    1327104     activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 8, 8, 384)    73728       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 8, 8, 384)    0           conv2d_9[0][0]                   \n","                                                                 conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 8, 8, 384)    1536        add_3[0][0]                      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 8, 8, 384)    0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 1, 1, 384)    0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 384)          0           average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           3840        flatten_1[0][0]                  \n","==================================================================================================\n","Total params: 2,688,496\n","Trainable params: 2,685,776\n","Non-trainable params: 2,720\n","__________________________________________________________________________________________________\n","None\n","Using real-time data augmentation.\n","Epoch 1/200\n","235/235 [==============================] - 54s 229ms/step - loss: 3.1813 - acc: 0.4596 - val_loss: 2.6667 - val_acc: 0.5339\n","Epoch 2/200\n","235/235 [==============================] - 49s 210ms/step - loss: 2.2277 - acc: 0.6147 - val_loss: 2.1337 - val_acc: 0.5991\n","Epoch 3/200\n","235/235 [==============================] - 48s 206ms/step - loss: 1.7355 - acc: 0.6695 - val_loss: 1.7519 - val_acc: 0.6448\n","Epoch 4/200\n","235/235 [==============================] - 48s 206ms/step - loss: 1.4332 - acc: 0.7053 - val_loss: 2.1720 - val_acc: 0.4909\n","Epoch 5/200\n","235/235 [==============================] - 48s 206ms/step - loss: 1.2348 - acc: 0.7318 - val_loss: 1.6165 - val_acc: 0.6352\n","Epoch 6/200\n","235/235 [==============================] - 48s 206ms/step - loss: 1.1072 - acc: 0.7501 - val_loss: 1.4651 - val_acc: 0.6748\n","Epoch 7/200\n","235/235 [==============================] - 48s 206ms/step - loss: 1.0284 - acc: 0.7648 - val_loss: 1.3672 - val_acc: 0.6753\n","Epoch 8/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.9789 - acc: 0.7738 - val_loss: 1.4549 - val_acc: 0.6368\n","Epoch 9/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.9384 - acc: 0.7887 - val_loss: 1.6558 - val_acc: 0.6134\n","Epoch 10/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.9204 - acc: 0.7895 - val_loss: 1.2160 - val_acc: 0.7183\n","Epoch 11/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8837 - acc: 0.8023 - val_loss: 1.2510 - val_acc: 0.7226\n","Epoch 12/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8916 - acc: 0.8030 - val_loss: 1.3781 - val_acc: 0.6735\n","Epoch 13/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8712 - acc: 0.8115 - val_loss: 1.4381 - val_acc: 0.6628\n","Epoch 14/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8696 - acc: 0.8135 - val_loss: 1.0471 - val_acc: 0.7540\n","Epoch 15/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8594 - acc: 0.8183 - val_loss: 1.0155 - val_acc: 0.7738\n","Epoch 16/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8602 - acc: 0.8191 - val_loss: 1.0801 - val_acc: 0.7493\n","Epoch 17/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8515 - acc: 0.8260 - val_loss: 1.1021 - val_acc: 0.7618\n","Epoch 18/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8604 - acc: 0.8231 - val_loss: 1.0677 - val_acc: 0.7674\n","Epoch 19/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8453 - acc: 0.8307 - val_loss: 1.5070 - val_acc: 0.6571\n","Epoch 20/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8619 - acc: 0.8270 - val_loss: 1.1047 - val_acc: 0.7555\n","Epoch 21/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8519 - acc: 0.8316 - val_loss: 1.9059 - val_acc: 0.6139\n","Epoch 22/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8469 - acc: 0.8345 - val_loss: 1.0871 - val_acc: 0.7720\n","Epoch 23/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8491 - acc: 0.8356 - val_loss: 1.0224 - val_acc: 0.7812\n","Epoch 24/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8412 - acc: 0.8389 - val_loss: 1.1061 - val_acc: 0.7665\n","Epoch 25/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8615 - acc: 0.8347 - val_loss: 1.0022 - val_acc: 0.7959\n","Epoch 26/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8485 - acc: 0.8394 - val_loss: 1.2849 - val_acc: 0.7240\n","Epoch 27/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8528 - acc: 0.8359 - val_loss: 1.0824 - val_acc: 0.7730\n","Epoch 28/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8594 - acc: 0.8392 - val_loss: 1.0420 - val_acc: 0.7777\n","Epoch 29/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8514 - acc: 0.8378 - val_loss: 1.0989 - val_acc: 0.7796\n","Epoch 30/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8718 - acc: 0.8332 - val_loss: 1.0228 - val_acc: 0.7942\n","Epoch 31/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8530 - acc: 0.8417 - val_loss: 0.9352 - val_acc: 0.8214\n","Epoch 32/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8455 - acc: 0.8458 - val_loss: 1.0934 - val_acc: 0.7891\n","Epoch 33/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8598 - acc: 0.8415 - val_loss: 1.4234 - val_acc: 0.6997\n","Epoch 34/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8549 - acc: 0.8452 - val_loss: 0.9647 - val_acc: 0.8158\n","Epoch 35/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8716 - acc: 0.8397 - val_loss: 1.0375 - val_acc: 0.8023\n","Epoch 36/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8526 - acc: 0.8466 - val_loss: 0.8894 - val_acc: 0.8335\n","Epoch 37/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8606 - acc: 0.8424 - val_loss: 1.0515 - val_acc: 0.7916\n","Epoch 38/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8639 - acc: 0.8438 - val_loss: 0.9244 - val_acc: 0.8292\n","Epoch 39/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8519 - acc: 0.8474 - val_loss: 1.1324 - val_acc: 0.7715\n","Epoch 40/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8669 - acc: 0.8437 - val_loss: 1.3799 - val_acc: 0.7174\n","Epoch 41/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8548 - acc: 0.8476 - val_loss: 1.2599 - val_acc: 0.7479\n","Epoch 42/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8630 - acc: 0.8468 - val_loss: 1.2427 - val_acc: 0.7256\n","Epoch 43/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8464 - acc: 0.8509 - val_loss: 1.1982 - val_acc: 0.7514\n","Epoch 44/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8574 - acc: 0.8476 - val_loss: 1.1921 - val_acc: 0.7626\n","Epoch 45/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8578 - acc: 0.8471 - val_loss: 1.1269 - val_acc: 0.7668\n","Epoch 46/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8616 - acc: 0.8478 - val_loss: 1.1535 - val_acc: 0.7647\n","Epoch 47/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8665 - acc: 0.8465 - val_loss: 1.2795 - val_acc: 0.7535\n","Epoch 48/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8656 - acc: 0.8474 - val_loss: 1.1617 - val_acc: 0.7605\n","Epoch 49/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8534 - acc: 0.8515 - val_loss: 1.0681 - val_acc: 0.7849\n","Epoch 50/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8657 - acc: 0.8509 - val_loss: 1.2221 - val_acc: 0.7532\n","Epoch 51/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8545 - acc: 0.8505 - val_loss: 1.2864 - val_acc: 0.7327\n","Epoch 52/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8558 - acc: 0.8498 - val_loss: 1.0865 - val_acc: 0.7829\n","Epoch 53/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8642 - acc: 0.8478 - val_loss: 1.0467 - val_acc: 0.7872\n","Epoch 54/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8467 - acc: 0.8522 - val_loss: 1.2766 - val_acc: 0.7446\n","Epoch 55/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8639 - acc: 0.8484 - val_loss: 0.9147 - val_acc: 0.8344\n","Epoch 56/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.8582 - acc: 0.8528 - val_loss: 1.0777 - val_acc: 0.7893\n","Epoch 57/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8614 - acc: 0.8503 - val_loss: 1.0841 - val_acc: 0.7898\n","Epoch 58/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8742 - acc: 0.8501 - val_loss: 1.0486 - val_acc: 0.7917\n","Epoch 59/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8479 - acc: 0.8560 - val_loss: 1.0162 - val_acc: 0.8087\n","Epoch 60/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.8781 - acc: 0.8467 - val_loss: 0.9259 - val_acc: 0.8343\n","Epoch 61/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.7317 - acc: 0.8973 - val_loss: 0.7249 - val_acc: 0.8925\n","Epoch 62/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.6419 - acc: 0.9154 - val_loss: 0.6939 - val_acc: 0.8933\n","Epoch 63/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.5995 - acc: 0.9207 - val_loss: 0.6534 - val_acc: 0.8982\n","Epoch 64/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.5560 - acc: 0.9289 - val_loss: 0.6386 - val_acc: 0.8956\n","Epoch 65/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.5350 - acc: 0.9292 - val_loss: 0.6012 - val_acc: 0.9029\n","Epoch 66/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.5080 - acc: 0.9306 - val_loss: 0.6615 - val_acc: 0.8783\n","Epoch 67/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4971 - acc: 0.9293 - val_loss: 0.6096 - val_acc: 0.8933\n","Epoch 68/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4862 - acc: 0.9301 - val_loss: 0.6249 - val_acc: 0.8814\n","Epoch 69/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4618 - acc: 0.9344 - val_loss: 0.6042 - val_acc: 0.8879\n","Epoch 70/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4647 - acc: 0.9316 - val_loss: 0.5862 - val_acc: 0.8920\n","Epoch 71/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4525 - acc: 0.9329 - val_loss: 0.6347 - val_acc: 0.8788\n","Epoch 72/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4475 - acc: 0.9330 - val_loss: 0.5935 - val_acc: 0.8884\n","Epoch 73/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4479 - acc: 0.9321 - val_loss: 0.6190 - val_acc: 0.8792\n","Epoch 74/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4431 - acc: 0.9300 - val_loss: 0.6377 - val_acc: 0.8741\n","Epoch 75/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.4483 - acc: 0.9296 - val_loss: 0.6097 - val_acc: 0.8791\n","Epoch 76/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4357 - acc: 0.9311 - val_loss: 0.6501 - val_acc: 0.8659\n","Epoch 77/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4407 - acc: 0.9314 - val_loss: 0.5774 - val_acc: 0.8887\n","Epoch 78/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4400 - acc: 0.9297 - val_loss: 0.5836 - val_acc: 0.8810\n","Epoch 79/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4370 - acc: 0.9318 - val_loss: 0.6279 - val_acc: 0.8774\n","Epoch 80/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4439 - acc: 0.9284 - val_loss: 0.6025 - val_acc: 0.8845\n","Epoch 81/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4332 - acc: 0.9321 - val_loss: 0.7283 - val_acc: 0.8493\n","Epoch 82/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4310 - acc: 0.9340 - val_loss: 0.5689 - val_acc: 0.8934\n","Epoch 83/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4402 - acc: 0.9292 - val_loss: 0.6750 - val_acc: 0.8612\n","Epoch 84/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4319 - acc: 0.9328 - val_loss: 0.8107 - val_acc: 0.8305\n","Epoch 85/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4436 - acc: 0.9290 - val_loss: 0.5733 - val_acc: 0.8896\n","Epoch 86/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4305 - acc: 0.9348 - val_loss: 0.6495 - val_acc: 0.8688\n","Epoch 87/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4362 - acc: 0.9333 - val_loss: 0.6437 - val_acc: 0.8707\n","Epoch 88/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4401 - acc: 0.9311 - val_loss: 0.6334 - val_acc: 0.8753\n","Epoch 89/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4263 - acc: 0.9356 - val_loss: 0.5982 - val_acc: 0.8821\n","Epoch 90/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4440 - acc: 0.9302 - val_loss: 0.5975 - val_acc: 0.8872\n","Epoch 91/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.4275 - acc: 0.9372 - val_loss: 0.6429 - val_acc: 0.8754\n","Epoch 92/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4340 - acc: 0.9354 - val_loss: 0.6360 - val_acc: 0.8717\n","Epoch 93/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4370 - acc: 0.9339 - val_loss: 0.6682 - val_acc: 0.8723\n","Epoch 94/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4306 - acc: 0.9367 - val_loss: 0.6802 - val_acc: 0.8703\n","Epoch 95/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4517 - acc: 0.9282 - val_loss: 0.6125 - val_acc: 0.8838\n","Epoch 96/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4344 - acc: 0.9373 - val_loss: 0.6096 - val_acc: 0.8807\n","Epoch 97/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4290 - acc: 0.9387 - val_loss: 0.5855 - val_acc: 0.8938\n","Epoch 98/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4446 - acc: 0.9324 - val_loss: 0.5965 - val_acc: 0.8899\n","Epoch 99/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4275 - acc: 0.9376 - val_loss: 0.6569 - val_acc: 0.8694\n","Epoch 100/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4437 - acc: 0.9317 - val_loss: 0.5682 - val_acc: 0.8977\n","Epoch 101/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4296 - acc: 0.9389 - val_loss: 0.6782 - val_acc: 0.8669\n","Epoch 102/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.4399 - acc: 0.9344 - val_loss: 0.7283 - val_acc: 0.8549\n","Epoch 103/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4344 - acc: 0.9369 - val_loss: 0.6348 - val_acc: 0.8763\n","Epoch 104/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4283 - acc: 0.9377 - val_loss: 0.6190 - val_acc: 0.8857\n","Epoch 105/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4468 - acc: 0.9338 - val_loss: 0.6335 - val_acc: 0.8821\n","Epoch 106/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4376 - acc: 0.9350 - val_loss: 0.6256 - val_acc: 0.8800\n","Epoch 107/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4326 - acc: 0.9395 - val_loss: 0.6125 - val_acc: 0.8834\n","Epoch 108/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4432 - acc: 0.9349 - val_loss: 0.6093 - val_acc: 0.8838\n","Epoch 109/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4319 - acc: 0.9391 - val_loss: 0.6252 - val_acc: 0.8822\n","Epoch 110/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4392 - acc: 0.9357 - val_loss: 0.6482 - val_acc: 0.8799\n","Epoch 111/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4324 - acc: 0.9390 - val_loss: 0.6655 - val_acc: 0.8740\n","Epoch 112/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4299 - acc: 0.9400 - val_loss: 0.6024 - val_acc: 0.8902\n","Epoch 113/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4452 - acc: 0.9353 - val_loss: 0.6225 - val_acc: 0.8797\n","Epoch 114/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4272 - acc: 0.9418 - val_loss: 0.6340 - val_acc: 0.8830\n","Epoch 115/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4456 - acc: 0.9338 - val_loss: 0.5830 - val_acc: 0.8929\n","Epoch 116/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.4309 - acc: 0.9400 - val_loss: 0.6712 - val_acc: 0.8635\n","Epoch 117/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4283 - acc: 0.9409 - val_loss: 0.6493 - val_acc: 0.8851\n","Epoch 118/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4369 - acc: 0.9380 - val_loss: 0.6678 - val_acc: 0.8723\n","Epoch 119/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4279 - acc: 0.9410 - val_loss: 0.6570 - val_acc: 0.8747\n","Epoch 120/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.4385 - acc: 0.9386 - val_loss: 0.5858 - val_acc: 0.8988\n","Epoch 121/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.3777 - acc: 0.9598 - val_loss: 0.5145 - val_acc: 0.9182\n","Epoch 122/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.3410 - acc: 0.9728 - val_loss: 0.4925 - val_acc: 0.9230\n","Epoch 123/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.3285 - acc: 0.9759 - val_loss: 0.4905 - val_acc: 0.9235\n","Epoch 124/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.3157 - acc: 0.9801 - val_loss: 0.4855 - val_acc: 0.9208\n","Epoch 125/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.3090 - acc: 0.9803 - val_loss: 0.4803 - val_acc: 0.9218\n","Epoch 126/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2994 - acc: 0.9830 - val_loss: 0.4816 - val_acc: 0.9224\n","Epoch 127/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2942 - acc: 0.9831 - val_loss: 0.4768 - val_acc: 0.9213\n","Epoch 128/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2879 - acc: 0.9844 - val_loss: 0.4718 - val_acc: 0.9242\n","Epoch 129/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2818 - acc: 0.9852 - val_loss: 0.4712 - val_acc: 0.9232\n","Epoch 130/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2789 - acc: 0.9852 - val_loss: 0.4752 - val_acc: 0.9216\n","Epoch 131/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2713 - acc: 0.9860 - val_loss: 0.4635 - val_acc: 0.9238\n","Epoch 132/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2663 - acc: 0.9870 - val_loss: 0.4724 - val_acc: 0.9215\n","Epoch 133/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.2594 - acc: 0.9882 - val_loss: 0.4722 - val_acc: 0.9218\n","Epoch 134/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2542 - acc: 0.9888 - val_loss: 0.4579 - val_acc: 0.9225\n","Epoch 135/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2533 - acc: 0.9877 - val_loss: 0.4582 - val_acc: 0.9230\n","Epoch 136/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2460 - acc: 0.9898 - val_loss: 0.4654 - val_acc: 0.9199\n","Epoch 137/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2429 - acc: 0.9892 - val_loss: 0.4525 - val_acc: 0.9231\n","Epoch 138/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2394 - acc: 0.9904 - val_loss: 0.4606 - val_acc: 0.9236\n","Epoch 139/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2378 - acc: 0.9886 - val_loss: 0.4604 - val_acc: 0.9218\n","Epoch 140/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2320 - acc: 0.9905 - val_loss: 0.4516 - val_acc: 0.9229\n","Epoch 141/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2284 - acc: 0.9905 - val_loss: 0.4486 - val_acc: 0.9226\n","Epoch 142/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2251 - acc: 0.9903 - val_loss: 0.4514 - val_acc: 0.9219\n","Epoch 143/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2242 - acc: 0.9896 - val_loss: 0.4578 - val_acc: 0.9206\n","Epoch 144/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.2193 - acc: 0.9915 - val_loss: 0.4498 - val_acc: 0.9220\n","Epoch 145/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2161 - acc: 0.9911 - val_loss: 0.4451 - val_acc: 0.9221\n","Epoch 146/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2136 - acc: 0.9912 - val_loss: 0.4562 - val_acc: 0.9201\n","Epoch 147/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2102 - acc: 0.9917 - val_loss: 0.4466 - val_acc: 0.9224\n","Epoch 148/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2078 - acc: 0.9913 - val_loss: 0.4361 - val_acc: 0.9234\n","Epoch 149/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.2042 - acc: 0.9915 - val_loss: 0.4493 - val_acc: 0.9208\n","Epoch 150/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.2017 - acc: 0.9923 - val_loss: 0.4296 - val_acc: 0.9235\n","Epoch 151/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1990 - acc: 0.9923 - val_loss: 0.4506 - val_acc: 0.9197\n","Epoch 152/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1972 - acc: 0.9919 - val_loss: 0.4444 - val_acc: 0.9209\n","Epoch 153/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1956 - acc: 0.9918 - val_loss: 0.4487 - val_acc: 0.9203\n","Epoch 154/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1926 - acc: 0.9916 - val_loss: 0.4384 - val_acc: 0.9209\n","Epoch 155/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1933 - acc: 0.9908 - val_loss: 0.4672 - val_acc: 0.9152\n","Epoch 156/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.1899 - acc: 0.9913 - val_loss: 0.4458 - val_acc: 0.9163\n","Epoch 157/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1868 - acc: 0.9924 - val_loss: 0.4346 - val_acc: 0.9189\n","Epoch 158/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1866 - acc: 0.9915 - val_loss: 0.4444 - val_acc: 0.9177\n","Epoch 159/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1801 - acc: 0.9927 - val_loss: 0.4384 - val_acc: 0.9179\n","Epoch 160/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.1843 - acc: 0.9904 - val_loss: 0.4486 - val_acc: 0.9160\n","Epoch 161/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1766 - acc: 0.9930 - val_loss: 0.4213 - val_acc: 0.9216\n","Epoch 162/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1706 - acc: 0.9952 - val_loss: 0.4152 - val_acc: 0.9232\n","Epoch 163/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1678 - acc: 0.9961 - val_loss: 0.4199 - val_acc: 0.9235\n","Epoch 164/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1662 - acc: 0.9962 - val_loss: 0.4149 - val_acc: 0.9228\n","Epoch 165/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1642 - acc: 0.9971 - val_loss: 0.4138 - val_acc: 0.9242\n","Epoch 166/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1633 - acc: 0.9970 - val_loss: 0.4119 - val_acc: 0.9232\n","Epoch 167/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1637 - acc: 0.9966 - val_loss: 0.4065 - val_acc: 0.9255\n","Epoch 168/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1621 - acc: 0.9970 - val_loss: 0.4096 - val_acc: 0.9257\n","Epoch 169/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1615 - acc: 0.9974 - val_loss: 0.4108 - val_acc: 0.9252\n","Epoch 170/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1612 - acc: 0.9970 - val_loss: 0.4071 - val_acc: 0.9266\n","Epoch 171/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1598 - acc: 0.9975 - val_loss: 0.4068 - val_acc: 0.9255\n","Epoch 172/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1589 - acc: 0.9973 - val_loss: 0.4050 - val_acc: 0.9271\n","Epoch 173/200\n","235/235 [==============================] - 48s 206ms/step - loss: 0.1588 - acc: 0.9972 - val_loss: 0.4061 - val_acc: 0.9269\n","Epoch 174/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1575 - acc: 0.9977 - val_loss: 0.4075 - val_acc: 0.9275\n","Epoch 175/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1569 - acc: 0.9976 - val_loss: 0.4084 - val_acc: 0.9263\n","Epoch 176/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1564 - acc: 0.9977 - val_loss: 0.4089 - val_acc: 0.9272\n","Epoch 177/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1553 - acc: 0.9978 - val_loss: 0.4072 - val_acc: 0.9263\n","Epoch 178/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1538 - acc: 0.9983 - val_loss: 0.4084 - val_acc: 0.9270\n","Epoch 179/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1534 - acc: 0.9982 - val_loss: 0.4086 - val_acc: 0.9254\n","Epoch 180/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1538 - acc: 0.9978 - val_loss: 0.4019 - val_acc: 0.9270\n","Epoch 181/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1524 - acc: 0.9977 - val_loss: 0.4049 - val_acc: 0.9261\n","Epoch 182/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1522 - acc: 0.9981 - val_loss: 0.4054 - val_acc: 0.9258\n","Epoch 183/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1522 - acc: 0.9978 - val_loss: 0.4048 - val_acc: 0.9263\n","Epoch 184/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1499 - acc: 0.9986 - val_loss: 0.4017 - val_acc: 0.9259\n","Epoch 185/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1506 - acc: 0.9983 - val_loss: 0.4054 - val_acc: 0.9264\n","Epoch 186/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1489 - acc: 0.9984 - val_loss: 0.4029 - val_acc: 0.9263\n","Epoch 187/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1497 - acc: 0.9980 - val_loss: 0.4069 - val_acc: 0.9260\n","Epoch 188/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1480 - acc: 0.9986 - val_loss: 0.4031 - val_acc: 0.9263\n","Epoch 189/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1475 - acc: 0.9986 - val_loss: 0.4027 - val_acc: 0.9263\n","Epoch 190/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1469 - acc: 0.9986 - val_loss: 0.4025 - val_acc: 0.9267\n","Epoch 191/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1467 - acc: 0.9986 - val_loss: 0.4006 - val_acc: 0.9257\n","Epoch 192/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1457 - acc: 0.9986 - val_loss: 0.4005 - val_acc: 0.9260\n","Epoch 193/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1459 - acc: 0.9983 - val_loss: 0.4053 - val_acc: 0.9251\n","Epoch 194/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1455 - acc: 0.9981 - val_loss: 0.4021 - val_acc: 0.9259\n","Epoch 195/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1455 - acc: 0.9981 - val_loss: 0.4046 - val_acc: 0.9261\n","Epoch 196/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1437 - acc: 0.9986 - val_loss: 0.3991 - val_acc: 0.9260\n","Epoch 197/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1433 - acc: 0.9985 - val_loss: 0.3984 - val_acc: 0.9265\n","Epoch 198/200\n","235/235 [==============================] - 48s 205ms/step - loss: 0.1436 - acc: 0.9985 - val_loss: 0.4020 - val_acc: 0.9255\n","Epoch 199/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1421 - acc: 0.9985 - val_loss: 0.4004 - val_acc: 0.9257\n","Epoch 200/200\n","235/235 [==============================] - 48s 204ms/step - loss: 0.1421 - acc: 0.9982 - val_loss: 0.3920 - val_acc: 0.9272\n"],"name":"stdout"}]}]}