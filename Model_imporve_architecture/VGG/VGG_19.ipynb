{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0OgeTxpROCCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "outputId": "051e9111-978b-4130-e1fb-50ad1dabb3cd"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-75a4d519ca25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GGC2nho7EYXL",
        "colab_type": "code",
        "outputId": "286cd278-59d1-4656-85c9-4474e8da86c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls drive/Deep_learning_project"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive’: File exists\n",
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8RoZmdgeElgC",
        "colab_type": "code",
        "outputId": "2289c1fe-192e-4678-8f40-007121a243b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/Deep_learning_project"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Deep_learning_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PSYUsEWQDexx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from data_utility import *\n",
        "\n",
        "iterations      = 200\n",
        "batch_size      = 250\n",
        "total_epoch     = 164\n",
        "weight_decay    = 0.0003\n",
        "dropout_rate    = 0.5\n",
        "momentum_rate   = 0.9\n",
        "log_save_path   = './vgg_logs'\n",
        "model_save_path = './model/'\n",
        "\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ bias_variable()\n",
        "# ├─ conv2d()           With Batch Normalization\n",
        "# ├─ max_pool()\n",
        "# └─ global_avg_pool()\n",
        "# ========================================================== #\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "def max_pool(input, k_size=1, stride=1, name=None):\n",
        "    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME',name=name)\n",
        "\n",
        "def batch_norm(input):\n",
        "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ _random_crop() \n",
        "# ├─ _random_flip_leftright()\n",
        "# ├─ data_augmentation()\n",
        "# ├─ data_preprocessing()\n",
        "# └─ learning_rate_schedule()\n",
        "# ========================================================== #\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "        oshape = np.shape(batch[0])\n",
        "        \n",
        "        if padding:\n",
        "            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
        "        new_batch = []\n",
        "        npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "        for i in range(len(batch)):\n",
        "            new_batch.append(batch[i])\n",
        "            if padding:\n",
        "                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                          mode='constant', constant_values=0)\n",
        "            nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "            nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                                        nw:nw + crop_shape[1]]\n",
        "        return new_batch\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "        for i in range(len(batch)):\n",
        "            if bool(random.getrandbits(1)):\n",
        "                batch[i] = np.fliplr(batch[i])\n",
        "        return batch\n",
        "\n",
        "def data_preprocessing(x_train,x_test):\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    x_train[:,:,:,0] = (x_train[:,:,:,0] - np.mean(x_train[:,:,:,0])) / np.std(x_train[:,:,:,0])\n",
        "    x_train[:,:,:,1] = (x_train[:,:,:,1] - np.mean(x_train[:,:,:,1])) / np.std(x_train[:,:,:,1])\n",
        "    x_train[:,:,:,2] = (x_train[:,:,:,2] - np.mean(x_train[:,:,:,2])) / np.std(x_train[:,:,:,2])\n",
        "\n",
        "    x_test[:,:,:,0] = (x_test[:,:,:,0] - np.mean(x_test[:,:,:,0])) / np.std(x_test[:,:,:,0])\n",
        "    x_test[:,:,:,1] = (x_test[:,:,:,1] - np.mean(x_test[:,:,:,1])) / np.std(x_test[:,:,:,1])\n",
        "    x_test[:,:,:,2] = (x_test[:,:,:,2] - np.mean(x_test[:,:,:,2])) / np.std(x_test[:,:,:,2])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "def learning_rate_schedule(epoch_num):\n",
        "      if epoch_num < 81:\n",
        "        return 0.1\n",
        "      elif epoch_num < 121:\n",
        "        return 0.01\n",
        "      else:\n",
        "        return 0.001\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [32,32], 4)\n",
        "    return batch\n",
        "\n",
        "def run_testing(sess,ep):\n",
        "    acc = 0.0\n",
        "    loss = 0.0\n",
        "    pre_index = 0\n",
        "    add = 1000\n",
        "    for it in range(10):\n",
        "        batch_x = test_x[pre_index:pre_index+add]\n",
        "        batch_y = test_y[pre_index:pre_index+add]\n",
        "        pre_index = pre_index + add\n",
        "        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False})\n",
        "        loss += loss_ / 10.0\n",
        "        acc += acc_ / 10.0\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=loss), \n",
        "                            tf.Summary.Value(tag=\"test_accuracy\", simple_value=acc)])\n",
        "    return acc, loss, summary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evqLqKWgDeyC",
        "colab_type": "code",
        "outputId": "0355d370-a28a-4944-f6af-728ee19dfde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8619
        }
      },
      "cell_type": "code",
      "source": [
        "# ========================================================== #\n",
        "# ├─ main()\n",
        "# Training and Testing \n",
        "# Save train/teset loss and acc for visualization\n",
        "# Save Model in ./model\n",
        "# ========================================================== #\n",
        "tf.reset_default_graph() \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_x, train_y, test_x, test_y = prepare_data()\n",
        "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
        "\n",
        "    # define placeholder x, y_ , keep_prob, learning_rate\n",
        "    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
        "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "    learning_rate = tf.placeholder(tf.float32)\n",
        "    train_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "    # build_network\n",
        "\n",
        "    W_conv1_1 = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_1 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1) + b_conv1_1))\n",
        "    \n",
        "    W_conv1_2 = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_2 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2) + b_conv1_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool1\")\n",
        "\n",
        "    W_conv2_1 = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_1 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1) + b_conv2_1))\n",
        "    \n",
        "\n",
        "    W_conv2_2 = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_2 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2) + b_conv2_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool2\")\n",
        "\n",
        "    W_conv3_1 = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_1 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\n",
        "\n",
        "\n",
        "    W_conv3_2 = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_2 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2) + b_conv3_2))\n",
        "\n",
        "    W_conv3_3 = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_3 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3) + b_conv3_3))\n",
        "\n",
        "    W_conv3_4 = tf.get_variable('conv3_4', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_4 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_4) + b_conv3_4))\n",
        "    output  = max_pool(output, 2, 2, \"pool3\")\n",
        "\n",
        "    W_conv4_1 = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1) + b_conv4_1))\n",
        "\n",
        "\n",
        "    W_conv4_2 = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2) + b_conv4_2))\n",
        "\n",
        "    W_conv4_3 = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3) + b_conv4_3))\n",
        "\n",
        "    W_conv4_4 = tf.get_variable('conv4_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_4 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_4)) + b_conv4_4)\n",
        "    output  = max_pool(output, 2, 2)\n",
        "\n",
        "    W_conv5_1 = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1) + b_conv5_1))\n",
        "\n",
        "    W_conv5_2 = tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2) + b_conv5_2))\n",
        "\n",
        "    W_conv5_3 = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3) + b_conv5_3))\n",
        "\n",
        "    W_conv5_4 = tf.get_variable('conv5_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_4 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_4) + b_conv5_4))\n",
        "\n",
        "    # output = tf.contrib.layers.flatten(output)\n",
        "    output = tf.reshape(output,[-1,2*2*512])\n",
        "\n",
        "    W_fc1 = tf.get_variable('fc1', shape=[2048,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc1 = bias_variable([4096])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1) + b_fc1) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    \n",
        "    W_fc2 = tf.get_variable('fc7', shape=[4096,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc2 = bias_variable([4096])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2) + b_fc2) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "\n",
        "\n",
        "    W_fc3 = tf.get_variable('fc3', shape=[4096,10], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc3 = bias_variable([10])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3) + b_fc3) )\n",
        "    # output  = tf.reshape(output,[-1,10])\n",
        "\n",
        "\n",
        "\n",
        "    # loss function: cross_entropy\n",
        "    # train_step: training operation\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
        "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\n",
        "    \n",
        "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "    # initial an saver to save model\n",
        "    saver = tf.train.Saver()\n",
        "   \n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
        "\n",
        "        # epoch = 164 \n",
        "        # make sure [bath_size * iteration = data_set_number]\n",
        "\n",
        "        for ep in range(1,total_epoch+1):\n",
        "            lr = learning_rate_schedule(ep)\n",
        "            pre_index = 0\n",
        "            train_acc = 0.0\n",
        "            train_loss = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"\\nepoch %d/%d:\" %(ep,total_epoch))\n",
        "\n",
        "            for it in range(1,iterations+1):\n",
        "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
        "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
        "\n",
        "                batch_x = data_augmentation(batch_x)\n",
        "\n",
        "                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True})\n",
        "                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "\n",
        "                train_loss += batch_loss\n",
        "                train_acc  += batch_acc\n",
        "                pre_index  += batch_size\n",
        "\n",
        "                if it == iterations:\n",
        "                    train_loss /= iterations\n",
        "                    train_acc /= iterations\n",
        "\n",
        "                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
        "                                          tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
        "\n",
        "                    val_acc, val_loss, test_summary = run_testing(sess,ep)\n",
        "\n",
        "                    summary_writer.add_summary(train_summary, ep)\n",
        "                    summary_writer.add_summary(test_summary, ep)\n",
        "                    summary_writer.flush()\n",
        "\n",
        "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
        "                else:\n",
        "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\" %(it, iterations, train_loss / it, train_acc / it) , end='\\r')\n",
        "\n",
        "        save_path = saver.save(sess, model_save_path)\n",
        "        print(\"Model saved in file: %s\" % save_path)  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======Loading data======\n",
            "DataSet aready exist!\n",
            "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
            "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
            "Train data: (50000, 32, 32, 3) (50000, 10)\n",
            "Test data : (10000, 32, 32, 3) (10000, 10)\n",
            "======Load finished======\n",
            "======Shuffling data======\n",
            "======Prepare Finished======\n",
            "\n",
            "epoch 1/164:\n",
            "iteration: 200/200, cost_time: 92s, train_loss: 1.9081, train_acc: 0.2887, test_loss: 1.6721, test_acc: 0.3577\n",
            "\n",
            "epoch 2/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 1.4417, train_acc: 0.5172, test_loss: 1.3233, test_acc: 0.5310\n",
            "\n",
            "epoch 3/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 1.0972, train_acc: 0.6771, test_loss: 0.9340, test_acc: 0.6746\n",
            "\n",
            "epoch 4/164:\n",
            "iteration: 200/200, cost_time: 89s, train_loss: 0.8791, train_acc: 0.7672, test_loss: 0.8852, test_acc: 0.7011\n",
            "\n",
            "epoch 5/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.7329, train_acc: 0.8204, test_loss: 0.7085, test_acc: 0.7623\n",
            "\n",
            "epoch 6/164:\n",
            "iteration: 200/200, cost_time: 89s, train_loss: 0.6402, train_acc: 0.8549, test_loss: 0.6653, test_acc: 0.7806\n",
            "\n",
            "epoch 7/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.5778, train_acc: 0.8788, test_loss: 0.6062, test_acc: 0.8006\n",
            "\n",
            "epoch 8/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.5297, train_acc: 0.8927, test_loss: 0.5409, test_acc: 0.8188\n",
            "\n",
            "epoch 9/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.4876, train_acc: 0.9076, test_loss: 0.5328, test_acc: 0.8250\n",
            "\n",
            "epoch 10/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.4564, train_acc: 0.9197, test_loss: 0.5555, test_acc: 0.8197\n",
            "\n",
            "epoch 11/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.4331, train_acc: 0.9261, test_loss: 0.5748, test_acc: 0.8101\n",
            "\n",
            "epoch 12/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.4066, train_acc: 0.9359, test_loss: 0.5284, test_acc: 0.8299\n",
            "\n",
            "epoch 13/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3851, train_acc: 0.9425, test_loss: 0.5340, test_acc: 0.8307\n",
            "\n",
            "epoch 14/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3730, train_acc: 0.9479, test_loss: 0.4891, test_acc: 0.8466\n",
            "\n",
            "epoch 15/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3596, train_acc: 0.9514, test_loss: 0.5338, test_acc: 0.8340\n",
            "\n",
            "epoch 16/164:\n",
            "iteration: 200/200, cost_time: 94s, train_loss: 0.3439, train_acc: 0.9550, test_loss: 0.4908, test_acc: 0.8387\n",
            "\n",
            "epoch 17/164:\n",
            "iteration: 200/200, cost_time: 137s, train_loss: 0.3353, train_acc: 0.9581, test_loss: 0.5560, test_acc: 0.8346\n",
            "\n",
            "epoch 18/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3219, train_acc: 0.9615, test_loss: 0.4629, test_acc: 0.8497\n",
            "\n",
            "epoch 19/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3192, train_acc: 0.9632, test_loss: 0.4345, test_acc: 0.8603\n",
            "\n",
            "epoch 20/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.3048, train_acc: 0.9665, test_loss: 0.4527, test_acc: 0.8554\n",
            "\n",
            "epoch 21/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2989, train_acc: 0.9675, test_loss: 0.4595, test_acc: 0.8598\n",
            "\n",
            "epoch 22/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2983, train_acc: 0.9692, test_loss: 0.5766, test_acc: 0.8248\n",
            "\n",
            "epoch 23/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2881, train_acc: 0.9713, test_loss: 0.4716, test_acc: 0.8515\n",
            "\n",
            "epoch 24/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2815, train_acc: 0.9731, test_loss: 0.4284, test_acc: 0.8626\n",
            "\n",
            "epoch 25/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2816, train_acc: 0.9723, test_loss: 0.4682, test_acc: 0.8550\n",
            "\n",
            "epoch 26/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2739, train_acc: 0.9737, test_loss: 0.4949, test_acc: 0.8506\n",
            "\n",
            "epoch 27/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2702, train_acc: 0.9744, test_loss: 0.4356, test_acc: 0.8633\n",
            "\n",
            "epoch 28/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2605, train_acc: 0.9762, test_loss: 0.4586, test_acc: 0.8607\n",
            "\n",
            "epoch 29/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2587, train_acc: 0.9767, test_loss: 0.4186, test_acc: 0.8732\n",
            "\n",
            "epoch 30/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2564, train_acc: 0.9779, test_loss: 0.4135, test_acc: 0.8718\n",
            "\n",
            "epoch 31/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2564, train_acc: 0.9777, test_loss: 0.4347, test_acc: 0.8643\n",
            "\n",
            "epoch 32/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2543, train_acc: 0.9780, test_loss: 0.4163, test_acc: 0.8709\n",
            "\n",
            "epoch 33/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2503, train_acc: 0.9798, test_loss: 0.4222, test_acc: 0.8713\n",
            "\n",
            "epoch 34/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2443, train_acc: 0.9801, test_loss: 0.4116, test_acc: 0.8725\n",
            "\n",
            "epoch 35/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2431, train_acc: 0.9803, test_loss: 0.4179, test_acc: 0.8709\n",
            "\n",
            "epoch 36/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2426, train_acc: 0.9811, test_loss: 0.4186, test_acc: 0.8709\n",
            "\n",
            "epoch 37/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2380, train_acc: 0.9810, test_loss: 0.4325, test_acc: 0.8679\n",
            "\n",
            "epoch 38/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2308, train_acc: 0.9830, test_loss: 0.4473, test_acc: 0.8682\n",
            "\n",
            "epoch 39/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2339, train_acc: 0.9833, test_loss: 0.3977, test_acc: 0.8782\n",
            "\n",
            "epoch 40/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2210, train_acc: 0.9834, test_loss: 0.4006, test_acc: 0.8796\n",
            "\n",
            "epoch 41/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2267, train_acc: 0.9844, test_loss: 0.4488, test_acc: 0.8668\n",
            "\n",
            "epoch 42/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2248, train_acc: 0.9842, test_loss: 0.4778, test_acc: 0.8616\n",
            "\n",
            "epoch 43/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2255, train_acc: 0.9845, test_loss: 0.4370, test_acc: 0.8745\n",
            "\n",
            "epoch 44/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2226, train_acc: 0.9838, test_loss: 0.4247, test_acc: 0.8713\n",
            "\n",
            "epoch 45/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2240, train_acc: 0.9850, test_loss: 0.4361, test_acc: 0.8712\n",
            "\n",
            "epoch 46/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2220, train_acc: 0.9849, test_loss: 0.4338, test_acc: 0.8714\n",
            "\n",
            "epoch 47/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2212, train_acc: 0.9848, test_loss: 0.4756, test_acc: 0.8619\n",
            "\n",
            "epoch 48/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2223, train_acc: 0.9850, test_loss: 0.4143, test_acc: 0.8762\n",
            "\n",
            "epoch 49/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2165, train_acc: 0.9853, test_loss: 0.4028, test_acc: 0.8822\n",
            "\n",
            "epoch 50/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2114, train_acc: 0.9866, test_loss: 0.4321, test_acc: 0.8744\n",
            "\n",
            "epoch 51/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2220, train_acc: 0.9853, test_loss: 0.3920, test_acc: 0.8831\n",
            "\n",
            "epoch 52/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2171, train_acc: 0.9863, test_loss: 0.4806, test_acc: 0.8635\n",
            "\n",
            "epoch 53/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2161, train_acc: 0.9856, test_loss: 0.3656, test_acc: 0.8870\n",
            "\n",
            "epoch 54/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2166, train_acc: 0.9854, test_loss: 0.4401, test_acc: 0.8774\n",
            "\n",
            "epoch 55/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2213, train_acc: 0.9857, test_loss: 0.4027, test_acc: 0.8800\n",
            "\n",
            "epoch 56/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2081, train_acc: 0.9875, test_loss: 0.3931, test_acc: 0.8783\n",
            "\n",
            "epoch 57/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2116, train_acc: 0.9874, test_loss: 0.4846, test_acc: 0.8614\n",
            "\n",
            "epoch 58/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2108, train_acc: 0.9869, test_loss: 0.5211, test_acc: 0.8512\n",
            "\n",
            "epoch 59/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2100, train_acc: 0.9870, test_loss: 0.4167, test_acc: 0.8795\n",
            "\n",
            "epoch 60/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1995, train_acc: 0.9888, test_loss: 0.4441, test_acc: 0.8735\n",
            "\n",
            "epoch 61/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2079, train_acc: 0.9872, test_loss: 0.4062, test_acc: 0.8792\n",
            "\n",
            "epoch 62/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2063, train_acc: 0.9877, test_loss: 0.4833, test_acc: 0.8614\n",
            "\n",
            "epoch 63/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2045, train_acc: 0.9869, test_loss: 0.4191, test_acc: 0.8787\n",
            "\n",
            "epoch 64/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2070, train_acc: 0.9880, test_loss: 0.4540, test_acc: 0.8644\n",
            "\n",
            "epoch 65/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2018, train_acc: 0.9882, test_loss: 0.4432, test_acc: 0.8722\n",
            "\n",
            "epoch 66/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2037, train_acc: 0.9879, test_loss: 0.4482, test_acc: 0.8665\n",
            "\n",
            "epoch 67/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2054, train_acc: 0.9881, test_loss: 0.4081, test_acc: 0.8804\n",
            "\n",
            "epoch 68/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2026, train_acc: 0.9882, test_loss: 0.4012, test_acc: 0.8823\n",
            "\n",
            "epoch 69/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2024, train_acc: 0.9884, test_loss: 0.4602, test_acc: 0.8667\n",
            "\n",
            "epoch 70/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1965, train_acc: 0.9882, test_loss: 0.4277, test_acc: 0.8747\n",
            "\n",
            "epoch 71/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1996, train_acc: 0.9887, test_loss: 0.3960, test_acc: 0.8825\n",
            "\n",
            "epoch 72/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2029, train_acc: 0.9886, test_loss: 0.4076, test_acc: 0.8801\n",
            "\n",
            "epoch 73/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1960, train_acc: 0.9891, test_loss: 0.4254, test_acc: 0.8739\n",
            "\n",
            "epoch 74/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2005, train_acc: 0.9883, test_loss: 0.3892, test_acc: 0.8831\n",
            "\n",
            "epoch 75/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2007, train_acc: 0.9878, test_loss: 0.4331, test_acc: 0.8785\n",
            "\n",
            "epoch 76/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1980, train_acc: 0.9890, test_loss: 0.4459, test_acc: 0.8700\n",
            "\n",
            "epoch 77/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1958, train_acc: 0.9895, test_loss: 0.3859, test_acc: 0.8874\n",
            "\n",
            "epoch 78/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2002, train_acc: 0.9887, test_loss: 0.3984, test_acc: 0.8840\n",
            "\n",
            "epoch 79/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.2025, train_acc: 0.9885, test_loss: 0.4073, test_acc: 0.8809\n",
            "\n",
            "epoch 80/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1929, train_acc: 0.9897, test_loss: 0.3739, test_acc: 0.8907\n",
            "\n",
            "epoch 81/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.1090, train_acc: 0.9751, test_loss: 0.2678, test_acc: 0.9211\n",
            "\n",
            "epoch 82/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0696, train_acc: 0.9855, test_loss: 0.2670, test_acc: 0.9250\n",
            "\n",
            "epoch 83/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0539, train_acc: 0.9901, test_loss: 0.2715, test_acc: 0.9266\n",
            "\n",
            "epoch 84/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0469, train_acc: 0.9926, test_loss: 0.2804, test_acc: 0.9258\n",
            "\n",
            "epoch 85/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0412, train_acc: 0.9934, test_loss: 0.2851, test_acc: 0.9269\n",
            "\n",
            "epoch 86/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0360, train_acc: 0.9949, test_loss: 0.2827, test_acc: 0.9295\n",
            "\n",
            "epoch 87/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0330, train_acc: 0.9960, test_loss: 0.3013, test_acc: 0.9283\n",
            "\n",
            "epoch 88/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0299, train_acc: 0.9965, test_loss: 0.2980, test_acc: 0.9279\n",
            "\n",
            "epoch 89/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0278, train_acc: 0.9969, test_loss: 0.2941, test_acc: 0.9309\n",
            "\n",
            "epoch 90/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0249, train_acc: 0.9975, test_loss: 0.3020, test_acc: 0.9271\n",
            "\n",
            "epoch 91/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0242, train_acc: 0.9974, test_loss: 0.3039, test_acc: 0.9289\n",
            "\n",
            "epoch 92/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0213, train_acc: 0.9982, test_loss: 0.3029, test_acc: 0.9312\n",
            "\n",
            "epoch 93/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0192, train_acc: 0.9982, test_loss: 0.3105, test_acc: 0.9308\n",
            "\n",
            "epoch 94/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0198, train_acc: 0.9984, test_loss: 0.3173, test_acc: 0.9306\n",
            "\n",
            "epoch 95/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0185, train_acc: 0.9986, test_loss: 0.3243, test_acc: 0.9277\n",
            "\n",
            "epoch 96/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0188, train_acc: 0.9988, test_loss: 0.3325, test_acc: 0.9262\n",
            "\n",
            "epoch 97/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0190, train_acc: 0.9987, test_loss: 0.3182, test_acc: 0.9301\n",
            "\n",
            "epoch 98/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0165, train_acc: 0.9991, test_loss: 0.3166, test_acc: 0.9322\n",
            "\n",
            "epoch 99/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0155, train_acc: 0.9989, test_loss: 0.3305, test_acc: 0.9288\n",
            "\n",
            "epoch 100/164:\n",
            "iteration: 200/200, cost_time: 89s, train_loss: 0.0146, train_acc: 0.9992, test_loss: 0.3252, test_acc: 0.9317\n",
            "\n",
            "epoch 101/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0153, train_acc: 0.9991, test_loss: 0.3321, test_acc: 0.9302\n",
            "\n",
            "epoch 102/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0138, train_acc: 0.9993, test_loss: 0.3296, test_acc: 0.9302\n",
            "\n",
            "epoch 103/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0149, train_acc: 0.9993, test_loss: 0.3374, test_acc: 0.9280\n",
            "\n",
            "epoch 104/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0118, train_acc: 0.9994, test_loss: 0.3300, test_acc: 0.9311\n",
            "\n",
            "epoch 105/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0132, train_acc: 0.9994, test_loss: 0.3341, test_acc: 0.9287\n",
            "\n",
            "epoch 106/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0127, train_acc: 0.9993, test_loss: 0.3336, test_acc: 0.9304\n",
            "\n",
            "epoch 107/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0127, train_acc: 0.9995, test_loss: 0.3399, test_acc: 0.9300\n",
            "\n",
            "epoch 108/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0114, train_acc: 0.9995, test_loss: 0.3512, test_acc: 0.9292\n",
            "\n",
            "epoch 109/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0125, train_acc: 0.9994, test_loss: 0.3480, test_acc: 0.9302\n",
            "\n",
            "epoch 110/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0120, train_acc: 0.9995, test_loss: 0.3431, test_acc: 0.9286\n",
            "\n",
            "epoch 111/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0113, train_acc: 0.9996, test_loss: 0.3550, test_acc: 0.9285\n",
            "\n",
            "epoch 112/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0116, train_acc: 0.9995, test_loss: 0.3474, test_acc: 0.9301\n",
            "\n",
            "epoch 113/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0123, train_acc: 0.9994, test_loss: 0.3497, test_acc: 0.9276\n",
            "\n",
            "epoch 114/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0131, train_acc: 0.9996, test_loss: 0.3495, test_acc: 0.9275\n",
            "\n",
            "epoch 115/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0115, train_acc: 0.9996, test_loss: 0.3704, test_acc: 0.9244\n",
            "\n",
            "epoch 116/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0130, train_acc: 0.9995, test_loss: 0.3472, test_acc: 0.9295\n",
            "\n",
            "epoch 117/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0117, train_acc: 0.9997, test_loss: 0.3513, test_acc: 0.9293\n",
            "\n",
            "epoch 118/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0117, train_acc: 0.9997, test_loss: 0.3445, test_acc: 0.9287\n",
            "\n",
            "epoch 119/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0110, train_acc: 0.9997, test_loss: 0.3744, test_acc: 0.9243\n",
            "\n",
            "epoch 120/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0128, train_acc: 0.9996, test_loss: 0.3559, test_acc: 0.9267\n",
            "\n",
            "epoch 121/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0096, train_acc: 0.9987, test_loss: 0.3435, test_acc: 0.9280\n",
            "\n",
            "epoch 122/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0078, train_acc: 0.9990, test_loss: 0.3406, test_acc: 0.9293\n",
            "\n",
            "epoch 123/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0083, train_acc: 0.9988, test_loss: 0.3390, test_acc: 0.9299\n",
            "\n",
            "epoch 124/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0065, train_acc: 0.9993, test_loss: 0.3383, test_acc: 0.9310\n",
            "\n",
            "epoch 125/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0071, train_acc: 0.9992, test_loss: 0.3381, test_acc: 0.9313\n",
            "\n",
            "epoch 126/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0063, train_acc: 0.9994, test_loss: 0.3366, test_acc: 0.9311\n",
            "\n",
            "epoch 127/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0064, train_acc: 0.9993, test_loss: 0.3368, test_acc: 0.9320\n",
            "\n",
            "epoch 128/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0064, train_acc: 0.9994, test_loss: 0.3383, test_acc: 0.9319\n",
            "\n",
            "epoch 129/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0062, train_acc: 0.9994, test_loss: 0.3398, test_acc: 0.9318\n",
            "\n",
            "epoch 130/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0056, train_acc: 0.9994, test_loss: 0.3400, test_acc: 0.9317\n",
            "\n",
            "epoch 131/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0062, train_acc: 0.9993, test_loss: 0.3409, test_acc: 0.9325\n",
            "\n",
            "epoch 132/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0064, train_acc: 0.9992, test_loss: 0.3399, test_acc: 0.9331\n",
            "\n",
            "epoch 133/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0061, train_acc: 0.9994, test_loss: 0.3398, test_acc: 0.9323\n",
            "\n",
            "epoch 134/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0063, train_acc: 0.9993, test_loss: 0.3403, test_acc: 0.9319\n",
            "\n",
            "epoch 135/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0059, train_acc: 0.9994, test_loss: 0.3405, test_acc: 0.9323\n",
            "\n",
            "epoch 136/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0053, train_acc: 0.9996, test_loss: 0.3386, test_acc: 0.9327\n",
            "\n",
            "epoch 137/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0058, train_acc: 0.9994, test_loss: 0.3400, test_acc: 0.9333\n",
            "\n",
            "epoch 138/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0055, train_acc: 0.9995, test_loss: 0.3425, test_acc: 0.9330\n",
            "\n",
            "epoch 139/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0051, train_acc: 0.9996, test_loss: 0.3411, test_acc: 0.9331\n",
            "\n",
            "epoch 140/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0049, train_acc: 0.9997, test_loss: 0.3417, test_acc: 0.9332\n",
            "\n",
            "epoch 141/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0055, train_acc: 0.9995, test_loss: 0.3403, test_acc: 0.9332\n",
            "\n",
            "epoch 142/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0053, train_acc: 0.9995, test_loss: 0.3405, test_acc: 0.9334\n",
            "\n",
            "epoch 143/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0052, train_acc: 0.9996, test_loss: 0.3425, test_acc: 0.9334\n",
            "\n",
            "epoch 144/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9997, test_loss: 0.3426, test_acc: 0.9331\n",
            "\n",
            "epoch 145/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0050, train_acc: 0.9996, test_loss: 0.3438, test_acc: 0.9331\n",
            "\n",
            "epoch 146/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0051, train_acc: 0.9995, test_loss: 0.3429, test_acc: 0.9334\n",
            "\n",
            "epoch 147/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0050, train_acc: 0.9997, test_loss: 0.3430, test_acc: 0.9332\n",
            "\n",
            "epoch 148/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0051, train_acc: 0.9996, test_loss: 0.3437, test_acc: 0.9332\n",
            "\n",
            "epoch 149/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9997, test_loss: 0.3434, test_acc: 0.9324\n",
            "\n",
            "epoch 150/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0049, train_acc: 0.9997, test_loss: 0.3434, test_acc: 0.9329\n",
            "\n",
            "epoch 151/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9996, test_loss: 0.3459, test_acc: 0.9332\n",
            "\n",
            "epoch 152/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9997, test_loss: 0.3442, test_acc: 0.9333\n",
            "\n",
            "epoch 153/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0049, train_acc: 0.9997, test_loss: 0.3442, test_acc: 0.9335\n",
            "\n",
            "epoch 154/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9996, test_loss: 0.3444, test_acc: 0.9328\n",
            "\n",
            "epoch 155/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0046, train_acc: 0.9996, test_loss: 0.3441, test_acc: 0.9342\n",
            "\n",
            "epoch 156/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0047, train_acc: 0.9996, test_loss: 0.3438, test_acc: 0.9339\n",
            "\n",
            "epoch 157/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0046, train_acc: 0.9997, test_loss: 0.3436, test_acc: 0.9338\n",
            "\n",
            "epoch 158/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0051, train_acc: 0.9995, test_loss: 0.3446, test_acc: 0.9337\n",
            "\n",
            "epoch 159/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0046, train_acc: 0.9998, test_loss: 0.3444, test_acc: 0.9339\n",
            "\n",
            "epoch 160/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0048, train_acc: 0.9997, test_loss: 0.3448, test_acc: 0.9339\n",
            "\n",
            "epoch 161/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0045, train_acc: 0.9997, test_loss: 0.3449, test_acc: 0.9338\n",
            "\n",
            "epoch 162/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0045, train_acc: 0.9996, test_loss: 0.3449, test_acc: 0.9331\n",
            "\n",
            "epoch 163/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0042, train_acc: 0.9997, test_loss: 0.3469, test_acc: 0.9333\n",
            "\n",
            "epoch 164/164:\n",
            "iteration: 200/200, cost_time: 90s, train_loss: 0.0045, train_acc: 0.9997, test_loss: 0.3456, test_acc: 0.9335\n",
            "Model saved in file: ./model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GmycezuI1ov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run Tensorflow in the background - note that we specify the log \n",
        "# directory we want to look at\n",
        "LOG_DIR = 'vgg_logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "na3znlzmI2xL",
        "colab_type": "code",
        "outputId": "9ad6245c-5146-4459-87a8-d5e0aebb648d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "# Download and unzip ngrok - you will only need to do this once per session\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-11 03:27:37--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.87.35.92, 54.165.51.142, 54.152.127.232, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.87.35.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  4.35MB/s    in 1.2s    \n",
            "\n",
            "2018-12-11 03:27:43 (4.35 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ngrok                   y\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "addiYD_SI26U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Launch the ngrok background process\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwMMwLarI3DY",
        "colab_type": "code",
        "outputId": "246d7de5-26b4-4e63-ec28-0dc7ec37218d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the public URL and be sorted!\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://161753ea.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AF4fPmM7DezN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}