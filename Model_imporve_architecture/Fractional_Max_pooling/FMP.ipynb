{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FMP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kps7cMvnCWKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "307b984f-e5ab-4f86-b442-e3f8c27e9cb2"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110377 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yp0RFBnRCq-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3312d6c9-9316-470c-8461-b4032fe74751"
      },
      "cell_type": "code",
      "source": [
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls /content/drive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive’: File exists\n",
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            "'Colab Notebooks'  'logs (da070523)'  'model (88ddb4be)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oKp9_1izCq87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def _print_download_progress(count, block_size, total_size):\n",
        "    \"\"\"\n",
        "    Function used for printing the download progress.\n",
        "    Used as a call-back function in maybe_download_and_extract().\n",
        "    \"\"\"\n",
        "\n",
        "    # Percentage completion.\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "\n",
        "    # Limit it because rounding errors may cause it to exceed 100%.\n",
        "    pct_complete = min(1.0, pct_complete)\n",
        "\n",
        "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "\n",
        "    # Print it.\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def _maybe_download_and_extract(url, download_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the data if it doesn't already exist.\n",
        "    Assumes the url is a tar-ball file.\n",
        "    :param url:\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    :param download_dir:\n",
        "        Directory where the downloaded file is saved.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filename for saving the file downloaded from the internet.\n",
        "    # Use the filename from the URL and add it to the download_dir.\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists.\n",
        "    # If it exists then we assume it has also been extracted,\n",
        "    # otherwise we need to download and extract it now.\n",
        "    if not os.path.exists(file_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=file_path,\n",
        "                                                  reporthook=_print_download_progress)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data has apparently already been downloaded and unpacked.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eL1P9y1oCq6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def one_hot_encoded(class_numbers, num_classes=None):\n",
        "    \"\"\"\n",
        "    Generate the One-Hot encoded class-labels from an array of integers.\n",
        "    For example, if class_number=2 and num_classes=4 then\n",
        "    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
        "    :param class_numbers:\n",
        "        Array of integers with class-numbers.\n",
        "        Assume the integers are from zero to num_classes-1 inclusive.\n",
        "    :param num_classes:\n",
        "        Number of classes. If None then use max(class_numbers)+1.\n",
        "    :return:\n",
        "        2-dim array of shape: [len(class_numbers), num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the number of classes if None is provided.\n",
        "    # Assumes the lowest class-number is zero.\n",
        "    if num_classes is None:\n",
        "        num_classes = np.max(class_numbers) + 1\n",
        "\n",
        "    return np.eye(num_classes, dtype=float)[class_numbers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jC-prX_UCzTh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "########################################################################\n",
        "\n",
        "# Directory where you want to download and save the data-set.\n",
        "# Set this before you start calling any of the functions below.\n",
        "data_path = \"data/CIFAR-10/\"\n",
        "\n",
        "# URL for the data-set on the internet.\n",
        "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "########################################################################\n",
        "# Various constants for the size of the images.\n",
        "# Use these constants in your own program.\n",
        "\n",
        "# Width and height of each image.\n",
        "img_size = 32\n",
        "\n",
        "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
        "num_channels = 3\n",
        "\n",
        "# Length of an image when flattened to a 1-dim array.\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "\n",
        "# Number of classes.\n",
        "num_classes = 10\n",
        "\n",
        "########################################################################\n",
        "# Various constants used to allocate arrays of the correct size.\n",
        "\n",
        "# Number of files for the training-set.\n",
        "_num_files_train = 5\n",
        "\n",
        "# Number of images for each batch-file in the training-set.\n",
        "_images_per_file = 10000\n",
        "\n",
        "# Total number of images in the training-set.\n",
        "# This is used to pre-allocate arrays for efficiency.\n",
        "_num_images_train = _num_files_train * _images_per_file\n",
        "\n",
        "########################################################################\n",
        "# Private functions for downloading, unpacking and loading data-files.\n",
        "\n",
        "\n",
        "def _get_file_path(filename=\"\"):\n",
        "    \"\"\"\n",
        "    Return the full path of a data-file for the data-set.\n",
        "    If filename==\"\" then return the directory of the files.\n",
        "    \"\"\"\n",
        "\n",
        "    return os.path.join(data_path, \"cifar-10-batches-py/\", filename)\n",
        "\n",
        "\n",
        "def _unpickle(filename):\n",
        "    \"\"\"\n",
        "    Unpickle the given file and return the data.\n",
        "    Note that the appropriate dir-name is prepended the filename.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create full path for the file.\n",
        "    file_path = _get_file_path(filename)\n",
        "\n",
        "    print(\"Loading data: \" + file_path)\n",
        "\n",
        "    with open(file_path, mode='rb') as file:\n",
        "        # In Python 3.X it is important to set the encoding,\n",
        "        # otherwise an exception is raised here.\n",
        "        data = pickle.load(file, encoding='bytes')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def _convert_images(raw):\n",
        "    \"\"\"\n",
        "    Convert images from the CIFAR-10 format and\n",
        "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
        "    where the pixels are floats between 0.0 and 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the raw images from the data-files to floating-points.\n",
        "    raw_float = np.array(raw, dtype=float) / 255.0\n",
        "\n",
        "    # Reshape the array to 4-dimensions.\n",
        "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
        "\n",
        "    # Reorder the indices of the array.\n",
        "    images = images.transpose([0, 2, 3, 1])\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def _load_data(filename):\n",
        "    \"\"\"\n",
        "    Load a pickled data-file from the CIFAR-10 data-set\n",
        "    and return the converted images (see above) and the class-number\n",
        "    for each image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the pickled data-file.\n",
        "    data = _unpickle(filename)\n",
        "\n",
        "    # Get the raw images.\n",
        "    raw_images = data[b'data']\n",
        "\n",
        "    # Get the class-numbers for each image. Convert to numpy-array.\n",
        "    cls = np.array(data[b'labels'])\n",
        "\n",
        "    # Convert the images.\n",
        "    images = _convert_images(raw_images)\n",
        "\n",
        "    return images, cls\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Public functions that you may call to download the data-set from\n",
        "# the internet and load the data into memory.\n",
        "\n",
        "\n",
        "def maybe_download_and_extract():\n",
        "    \"\"\"\n",
        "    Download and extract the CIFAR-10 data-set if it doesn't already exist\n",
        "    in data_path (set this variable first to the desired path).\n",
        "    \"\"\"\n",
        "\n",
        "    _maybe_download_and_extract(url=data_url, download_dir=data_path)\n",
        "\n",
        "\n",
        "def load_class_names():\n",
        "    \"\"\"\n",
        "    Load the names for the classes in the CIFAR-10 data-set.\n",
        "    Returns a list with the names. Example: names[3] is the name\n",
        "    associated with class-number 3.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the class-names from the pickled file.\n",
        "    raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
        "\n",
        "    # Convert from binary strings.\n",
        "    names = [x.decode('utf-8') for x in raw]\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def load_training_data():\n",
        "    \"\"\"\n",
        "    Load all the training-data for the CIFAR-10 data-set.\n",
        "    The data-set is split into 5 data-files which are merged here.\n",
        "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
        "    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n",
        "    cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
        "\n",
        "    # Begin-index for the current batch.\n",
        "    begin = 0\n",
        "\n",
        "    # For each data-file.\n",
        "    for i in range(_num_files_train):\n",
        "        # Load the images and class-numbers from the data-file.\n",
        "        images_batch, cls_batch = _load_data(filename=\"data_batch_\" + str(i + 1))\n",
        "\n",
        "        # Number of images in this batch.\n",
        "        num_images = len(images_batch)\n",
        "\n",
        "        # End-index for the current batch.\n",
        "        end = begin + num_images\n",
        "\n",
        "        # Store the images into the array.\n",
        "        images[begin:end, :] = images_batch\n",
        "\n",
        "        # Store the class-numbers into the array.\n",
        "        cls[begin:end] = cls_batch\n",
        "\n",
        "        # The begin-index for the next batch is the current end-index.\n",
        "        begin = end\n",
        "\n",
        "    return images, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"\n",
        "    Load all the test-data for the CIFAR-10 data-set.\n",
        "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
        "    \"\"\"\n",
        "\n",
        "    images, cls = _load_data(filename=\"test_batch\")\n",
        "\n",
        "    return images, one_hot_encoded(class_numbers=cls, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unI48clUCzbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "        oshape = np.shape(batch[0])\n",
        "        \n",
        "        if padding:\n",
        "            oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "        new_batch = []\n",
        "        npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "        for i in range(len(batch)):\n",
        "            new_batch.append(batch[i])\n",
        "            if padding:\n",
        "                new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode=\"constant\", constant_values=0)\n",
        "            nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "            nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "            new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]\n",
        "        return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "        for i in range(len(batch)):\n",
        "            if bool(random.getrandbits(1)):\n",
        "                batch[i] = np.fliplr(batch[i])\n",
        "        return batch\n",
        "\n",
        "\n",
        "def data_augmentation(batch, crop_shape, size):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [size,size], crop_shape)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNpHK8JpC4pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20553
        },
        "outputId": "4e114fa7-e273-46a1-9213-f06acb47cfe8"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 100\n",
        "num_filters = 32\n",
        "window_size = 3\n",
        "pooling_ratio = [1, 1.41, 1.41, 1]\n",
        "learning_rate = 0.001\n",
        "crop_shape = 4\n",
        "log_save_path = \"/content/drive/logs/\"\n",
        "model_save_path = \"/content/drive/model/\"\n",
        "\n",
        "maybe_download_and_extract()\n",
        "X_train, y_train = load_training_data()\n",
        "X_test, y_test = load_test_data()\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# z-score\n",
        "X_train = (X_train - np.mean(X_train, axis=(0, 1, 2, 3))) / np.std(X_train,axis=(0,1,2,3))\n",
        "X_test = (X_test - np.mean(X_test, axis=(0, 1, 2, 3))) / np.std(X_test,axis=(0,1,2,3))\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, img_size, img_size, num_channels])\n",
        "y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv1 = tf.get_variable(\"W_conv1\", [window_size, window_size, num_channels, num_filters])\n",
        "b_conv1 = tf.get_variable(\"b_conv1\", [num_filters])\n",
        "h_conv1 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv1)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv2 = tf.get_variable(\"W_conv2\", [window_size, window_size, num_filters, num_filters])\n",
        "b_conv2 = tf.get_variable(\"b_conv2\", [num_filters])\n",
        "h_conv2 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv2)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_pool1 = tf.nn.fractional_max_pool(h_conv2, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout1 = tf.nn.dropout(h_pool1, 0.8)\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv3 = tf.get_variable(\"W_conv3\", [window_size, window_size, num_filters, num_filters * 2])\n",
        "b_conv3 = tf.get_variable(\"b_conv3\", [num_filters * 2])\n",
        "h_conv3 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(dropout1, W_conv3, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv3)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv4 = tf.get_variable(\"W_conv4\", [window_size, window_size, num_filters * 2, num_filters * 2])\n",
        "b_conv4 = tf.get_variable(\"b_conv4\", [num_filters * 2])\n",
        "h_conv4 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv4)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_pool2 = tf.nn.fractional_max_pool(h_conv4, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout2 = tf.nn.dropout(h_pool2, 0.8)\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv5 = tf.get_variable(\"W_conv5\", [window_size, window_size, num_filters * 2, num_filters * 3])\n",
        "b_conv5 = tf.get_variable(\"b_conv5\", [num_filters * 3])\n",
        "h_conv5 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(dropout2, W_conv5, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv5)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv6 = tf.get_variable(\"W_conv6\", [window_size, window_size, num_filters * 3, num_filters * 3])\n",
        "b_conv6 = tf.get_variable(\"b_conv6\", [num_filters * 3])\n",
        "h_conv6 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv5, W_conv6, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv6)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_pool3 = tf.nn.fractional_max_pool(h_conv6, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout3 = tf.nn.dropout(h_pool3, 0.7)\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv7 = tf.get_variable(\"W_conv7\", [window_size, window_size, num_filters * 3, num_filters * 4])\n",
        "b_conv7 = tf.get_variable(\"b_conv7\", [num_filters * 4])\n",
        "h_conv7 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(dropout3, W_conv7, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv7)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv8 = tf.get_variable(\"W_conv8\", [window_size, window_size, num_filters * 4, num_filters * 4])\n",
        "b_conv8 = tf.get_variable(\"b_conv8\", [num_filters * 4])\n",
        "h_conv8 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv7, W_conv8, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv8)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_pool4 = tf.nn.fractional_max_pool(h_conv8, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout4 = tf.nn.dropout(h_pool4, 0.7)\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv9 = tf.get_variable(\"W_conv9\", [window_size, window_size, num_filters * 4, num_filters * 5])\n",
        "b_conv9 = tf.get_variable(\"b_conv9\", [num_filters * 5])\n",
        "h_conv9 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(dropout4, W_conv9, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv9)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv10 = tf.get_variable(\"W_conv10\", [window_size, window_size, num_filters * 5, num_filters * 5])\n",
        "b_conv10 = tf.get_variable(\"b_conv10\", [num_filters * 5])\n",
        "h_conv10 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv9, W_conv10, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv10)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_poo5 = tf.nn.fractional_max_pool(h_conv10, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout5 = tf.nn.dropout(h_poo5, 0.6)\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv11 = tf.get_variable(\"W_conv11\", [window_size, window_size, num_filters * 5, num_filters * 6])\n",
        "b_conv11 = tf.get_variable(\"b_conv11\", [num_filters * 6])\n",
        "h_conv11 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(dropout5, W_conv11, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv11)))\n",
        "\n",
        "# 2D convolution layer\n",
        "W_conv12 = tf.get_variable(\"W_conv12\", [window_size, window_size, num_filters * 6, num_filters * 6])\n",
        "b_conv12 = tf.get_variable(\"b_conv12\", [num_filters * 6])\n",
        "h_conv12 = tf.contrib.layers.batch_norm(tf.nn.relu(tf.add(tf.nn.conv2d(h_conv11, W_conv12, strides=[1, 1, 1, 1], padding=\"SAME\"), b_conv12)))\n",
        "\n",
        "# fractional max pooling layer\n",
        "h_pool6 = tf.nn.fractional_max_pool(h_conv12, pooling_ratio, pseudo_random=True, overlapping=True)[0]\n",
        "\n",
        "# dropout layer\n",
        "dropout6 = tf.nn.dropout(h_pool6, 0.6)\n",
        "\n",
        "# reshape layer\n",
        "dim = dropout6.get_shape()[1].value ** 2 * dropout6.get_shape()[3].value\n",
        "reshape = tf.reshape(dropout6, [-1, dim])\n",
        "\n",
        "# output layer\n",
        "W_conv13 = tf.get_variable(\"W_conv13\", [dim, num_classes])\n",
        "b_conv13 = tf.get_variable(\"b_conv13\", [num_classes])\n",
        "output = tf.add(tf.matmul(reshape, W_conv13), b_conv13)\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
        "    total_batch = int(X_train.shape[0] / batch_size)\n",
        "    total_batch_test = int(X_test.shape[0] / batch_size)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print(\"epoch %d/%d:\" % (i + 1, epochs))\n",
        "        \n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        \n",
        "        for batch in range(total_batch):\n",
        "            batch_x = X_train[batch * batch_size: (batch + 1) * batch_size, :]\n",
        "            batch_y = y_train[batch * batch_size: (batch + 1) * batch_size, :]\n",
        "            \n",
        "            batch_x = data_augmentation(batch_x, crop_shape, img_size)\n",
        "            \n",
        "            _, batch_loss, batch_acc = sess.run([optimizer, loss, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
        "            \n",
        "            train_loss += batch_loss\n",
        "            train_acc += batch_acc\n",
        "        \n",
        "            if batch == total_batch - 1:\n",
        "                train_loss /= total_batch\n",
        "                train_acc /= total_batch\n",
        "                train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
        "\n",
        "                test_loss = 0\n",
        "                test_acc = 0\n",
        "                for test_batch in range(total_batch_test):\n",
        "                    batch_x_test = X_test[test_batch * batch_size: (test_batch + 1) * batch_size, :]\n",
        "                    batch_y_test = y_test[test_batch * batch_size: (test_batch + 1) * batch_size, :]\n",
        "                    batch_loss_test, batch_acc_test = sess.run([loss, accuracy], feed_dict={x: batch_x_test, y: batch_y_test})\n",
        "                    \n",
        "                    test_loss += batch_loss_test\n",
        "                    test_acc += batch_acc_test\n",
        "\n",
        "                test_loss /= total_batch_test\n",
        "                test_acc /= total_batch_test\n",
        "                test_summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=test_loss), tf.Summary.Value(tag=\"test_accuracy\", simple_value=test_acc)])\n",
        "                \n",
        "                summary_writer.add_summary(train_summary, i)\n",
        "                summary_writer.add_summary(test_summary, i)\n",
        "                summary_writer.flush()\n",
        "                print(\"iteration: %d/%d, train_loss: %.4f, train_accuracy: %.4f, test_loss: %.4f, test_accuracy: %.4f\" % (batch + 1, total_batch, train_loss, train_acc, test_loss, test_acc))\n",
        "            elif (batch + 1) % 100 == 0:\n",
        "                print(\"iteration: %d/%d, train_loss: %.4f, train_accuracy: %.4f\" % (batch + 1, total_batch, train_loss / (batch + 1), train_acc / (batch + 1)))\n",
        "    save_path = saver.save(sess, model_save_path)\n",
        "    print(\"Model saved in file: %s\" % save_path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data has apparently already been downloaded and unpacked.\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
            "epoch 1/200:\n",
            "iteration: 100/500, train_loss: 2.6539, train_accuracy: 22.7300\n",
            "iteration: 200/500, train_loss: 2.3645, train_accuracy: 26.9050\n",
            "iteration: 300/500, train_loss: 2.2054, train_accuracy: 30.4333\n",
            "iteration: 400/500, train_loss: 2.0993, train_accuracy: 32.8100\n",
            "iteration: 500/500, train_loss: 2.0145, train_accuracy: 34.7400, test_loss: 1.6428, test_accuracy: 44.7400\n",
            "epoch 2/200:\n",
            "iteration: 100/500, train_loss: 1.5964, train_accuracy: 44.8300\n",
            "iteration: 200/500, train_loss: 1.5795, train_accuracy: 46.2050\n",
            "iteration: 300/500, train_loss: 1.5733, train_accuracy: 46.6967\n",
            "iteration: 400/500, train_loss: 1.5906, train_accuracy: 46.4575\n",
            "iteration: 500/500, train_loss: 1.5698, train_accuracy: 47.0700, test_loss: 1.4284, test_accuracy: 50.9200\n",
            "epoch 3/200:\n",
            "iteration: 100/500, train_loss: 1.3617, train_accuracy: 53.0600\n",
            "iteration: 200/500, train_loss: 1.3452, train_accuracy: 53.8100\n",
            "iteration: 300/500, train_loss: 1.3332, train_accuracy: 54.2733\n",
            "iteration: 400/500, train_loss: 1.3144, train_accuracy: 54.7825\n",
            "iteration: 500/500, train_loss: 1.3285, train_accuracy: 54.4540, test_loss: 1.2423, test_accuracy: 56.0800\n",
            "epoch 4/200:\n",
            "iteration: 100/500, train_loss: 1.2466, train_accuracy: 56.1900\n",
            "iteration: 200/500, train_loss: 1.2051, train_accuracy: 57.7750\n",
            "iteration: 300/500, train_loss: 1.1684, train_accuracy: 59.0100\n",
            "iteration: 400/500, train_loss: 1.1479, train_accuracy: 59.5625\n",
            "iteration: 500/500, train_loss: 1.1248, train_accuracy: 60.5000, test_loss: 1.0171, test_accuracy: 64.0100\n",
            "epoch 5/200:\n",
            "iteration: 100/500, train_loss: 1.0596, train_accuracy: 63.1800\n",
            "iteration: 200/500, train_loss: 1.1219, train_accuracy: 60.6600\n",
            "iteration: 300/500, train_loss: 1.0880, train_accuracy: 61.4600\n",
            "iteration: 400/500, train_loss: 1.0619, train_accuracy: 62.4250\n",
            "iteration: 500/500, train_loss: 1.0363, train_accuracy: 63.3320, test_loss: 0.9415, test_accuracy: 66.4500\n",
            "epoch 6/200:\n",
            "iteration: 100/500, train_loss: 0.9231, train_accuracy: 67.6900\n",
            "iteration: 200/500, train_loss: 0.9264, train_accuracy: 67.5900\n",
            "iteration: 300/500, train_loss: 0.9237, train_accuracy: 67.6767\n",
            "iteration: 400/500, train_loss: 0.9187, train_accuracy: 67.9125\n",
            "iteration: 500/500, train_loss: 0.9005, train_accuracy: 68.5060, test_loss: 0.8324, test_accuracy: 70.7300\n",
            "epoch 7/200:\n",
            "iteration: 100/500, train_loss: 0.8300, train_accuracy: 70.9000\n",
            "iteration: 200/500, train_loss: 0.8324, train_accuracy: 70.9600\n",
            "iteration: 300/500, train_loss: 0.8174, train_accuracy: 71.5033\n",
            "iteration: 400/500, train_loss: 0.8152, train_accuracy: 71.5975\n",
            "iteration: 500/500, train_loss: 0.8058, train_accuracy: 71.9340, test_loss: 0.7907, test_accuracy: 73.0300\n",
            "epoch 8/200:\n",
            "iteration: 100/500, train_loss: 0.7796, train_accuracy: 73.1700\n",
            "iteration: 200/500, train_loss: 0.7739, train_accuracy: 73.3850\n",
            "iteration: 300/500, train_loss: 0.7626, train_accuracy: 73.8033\n",
            "iteration: 400/500, train_loss: 0.7650, train_accuracy: 73.7175\n",
            "iteration: 500/500, train_loss: 0.7608, train_accuracy: 73.8740, test_loss: 0.7493, test_accuracy: 74.0800\n",
            "epoch 9/200:\n",
            "iteration: 100/500, train_loss: 0.7213, train_accuracy: 75.0700\n",
            "iteration: 200/500, train_loss: 0.7249, train_accuracy: 74.6950\n",
            "iteration: 300/500, train_loss: 0.7181, train_accuracy: 75.0033\n",
            "iteration: 400/500, train_loss: 0.7483, train_accuracy: 74.2625\n",
            "iteration: 500/500, train_loss: 0.7518, train_accuracy: 74.2220, test_loss: 0.7327, test_accuracy: 74.9800\n",
            "epoch 10/200:\n",
            "iteration: 100/500, train_loss: 0.7076, train_accuracy: 76.1100\n",
            "iteration: 200/500, train_loss: 0.6970, train_accuracy: 76.4250\n",
            "iteration: 300/500, train_loss: 0.6872, train_accuracy: 76.7400\n",
            "iteration: 400/500, train_loss: 0.6821, train_accuracy: 76.8450\n",
            "iteration: 500/500, train_loss: 0.6743, train_accuracy: 77.1300, test_loss: 0.6867, test_accuracy: 76.2200\n",
            "epoch 11/200:\n",
            "iteration: 100/500, train_loss: 0.6376, train_accuracy: 78.4200\n",
            "iteration: 200/500, train_loss: 0.6458, train_accuracy: 77.9450\n",
            "iteration: 300/500, train_loss: 0.6354, train_accuracy: 78.2433\n",
            "iteration: 400/500, train_loss: 0.6385, train_accuracy: 78.1650\n",
            "iteration: 500/500, train_loss: 0.6310, train_accuracy: 78.3360, test_loss: 0.6514, test_accuracy: 77.5800\n",
            "epoch 12/200:\n",
            "iteration: 100/500, train_loss: 0.6625, train_accuracy: 77.6300\n",
            "iteration: 200/500, train_loss: 0.6636, train_accuracy: 77.5550\n",
            "iteration: 300/500, train_loss: 0.6663, train_accuracy: 77.4300\n",
            "iteration: 400/500, train_loss: 0.6573, train_accuracy: 77.6725\n",
            "iteration: 500/500, train_loss: 0.6443, train_accuracy: 78.1560, test_loss: 0.6292, test_accuracy: 78.6900\n",
            "epoch 13/200:\n",
            "iteration: 100/500, train_loss: 0.5924, train_accuracy: 79.8100\n",
            "iteration: 200/500, train_loss: 0.5984, train_accuracy: 79.5600\n",
            "iteration: 300/500, train_loss: 0.5941, train_accuracy: 79.5900\n",
            "iteration: 400/500, train_loss: 0.6057, train_accuracy: 79.2100\n",
            "iteration: 500/500, train_loss: 0.6353, train_accuracy: 78.4220, test_loss: 1.0077, test_accuracy: 67.2900\n",
            "epoch 14/200:\n",
            "iteration: 100/500, train_loss: 0.9357, train_accuracy: 69.5600\n",
            "iteration: 200/500, train_loss: 0.8166, train_accuracy: 72.8050\n",
            "iteration: 300/500, train_loss: 0.7609, train_accuracy: 74.6033\n",
            "iteration: 400/500, train_loss: 0.7340, train_accuracy: 75.5275\n",
            "iteration: 500/500, train_loss: 0.7027, train_accuracy: 76.4640, test_loss: 0.6159, test_accuracy: 79.2100\n",
            "epoch 15/200:\n",
            "iteration: 100/500, train_loss: 0.5615, train_accuracy: 80.7900\n",
            "iteration: 200/500, train_loss: 0.5657, train_accuracy: 80.4900\n",
            "iteration: 300/500, train_loss: 0.5565, train_accuracy: 80.8400\n",
            "iteration: 400/500, train_loss: 0.5556, train_accuracy: 80.8825\n",
            "iteration: 500/500, train_loss: 0.5502, train_accuracy: 81.0920, test_loss: 0.5852, test_accuracy: 80.2800\n",
            "epoch 16/200:\n",
            "iteration: 100/500, train_loss: 0.5220, train_accuracy: 82.5300\n",
            "iteration: 200/500, train_loss: 0.5240, train_accuracy: 82.2050\n",
            "iteration: 300/500, train_loss: 0.5195, train_accuracy: 82.3200\n",
            "iteration: 400/500, train_loss: 0.5195, train_accuracy: 82.2475\n",
            "iteration: 500/500, train_loss: 0.5158, train_accuracy: 82.3980, test_loss: 0.5640, test_accuracy: 80.7800\n",
            "epoch 17/200:\n",
            "iteration: 100/500, train_loss: 0.5272, train_accuracy: 82.6000\n",
            "iteration: 200/500, train_loss: 0.5916, train_accuracy: 80.5650\n",
            "iteration: 300/500, train_loss: 0.6302, train_accuracy: 79.2233\n",
            "iteration: 400/500, train_loss: 0.6362, train_accuracy: 79.0175\n",
            "iteration: 500/500, train_loss: 0.6151, train_accuracy: 79.6140, test_loss: 0.5726, test_accuracy: 80.7200\n",
            "epoch 18/200:\n",
            "iteration: 100/500, train_loss: 0.5121, train_accuracy: 82.6300\n",
            "iteration: 200/500, train_loss: 0.5165, train_accuracy: 82.4700\n",
            "iteration: 300/500, train_loss: 0.5106, train_accuracy: 82.6667\n",
            "iteration: 400/500, train_loss: 0.5115, train_accuracy: 82.6000\n",
            "iteration: 500/500, train_loss: 0.5176, train_accuracy: 82.5060, test_loss: 0.6308, test_accuracy: 79.3300\n",
            "epoch 19/200:\n",
            "iteration: 100/500, train_loss: 0.5169, train_accuracy: 82.3400\n",
            "iteration: 200/500, train_loss: 0.5325, train_accuracy: 81.8850\n",
            "iteration: 300/500, train_loss: 0.6108, train_accuracy: 79.6600\n",
            "iteration: 400/500, train_loss: 0.6051, train_accuracy: 80.0100\n",
            "iteration: 500/500, train_loss: 0.6219, train_accuracy: 79.5820, test_loss: 0.7876, test_accuracy: 74.0700\n",
            "epoch 20/200:\n",
            "iteration: 100/500, train_loss: 0.6034, train_accuracy: 79.3100\n",
            "iteration: 200/500, train_loss: 0.5713, train_accuracy: 80.4000\n",
            "iteration: 300/500, train_loss: 0.5497, train_accuracy: 81.1733\n",
            "iteration: 400/500, train_loss: 0.5383, train_accuracy: 81.5925\n",
            "iteration: 500/500, train_loss: 0.5259, train_accuracy: 82.0420, test_loss: 0.5285, test_accuracy: 81.8900\n",
            "epoch 21/200:\n",
            "iteration: 100/500, train_loss: 0.4640, train_accuracy: 84.4100\n",
            "iteration: 200/500, train_loss: 0.4681, train_accuracy: 84.1700\n",
            "iteration: 300/500, train_loss: 0.4641, train_accuracy: 84.2300\n",
            "iteration: 400/500, train_loss: 0.4641, train_accuracy: 84.1350\n",
            "iteration: 500/500, train_loss: 0.4626, train_accuracy: 84.1500, test_loss: 0.5142, test_accuracy: 82.3700\n",
            "epoch 22/200:\n",
            "iteration: 100/500, train_loss: 0.4464, train_accuracy: 84.7400\n",
            "iteration: 200/500, train_loss: 0.4525, train_accuracy: 84.5550\n",
            "iteration: 300/500, train_loss: 0.4472, train_accuracy: 84.7433\n",
            "iteration: 400/500, train_loss: 0.4495, train_accuracy: 84.7200\n",
            "iteration: 500/500, train_loss: 0.4446, train_accuracy: 84.8760, test_loss: 0.4961, test_accuracy: 83.6400\n",
            "epoch 23/200:\n",
            "iteration: 100/500, train_loss: 0.4189, train_accuracy: 85.9000\n",
            "iteration: 200/500, train_loss: 0.4268, train_accuracy: 85.6650\n",
            "iteration: 300/500, train_loss: 0.4280, train_accuracy: 85.5600\n",
            "iteration: 400/500, train_loss: 0.4429, train_accuracy: 85.0850\n",
            "iteration: 500/500, train_loss: 0.4436, train_accuracy: 85.0140, test_loss: 0.4963, test_accuracy: 83.5400\n",
            "epoch 24/200:\n",
            "iteration: 100/500, train_loss: 0.4144, train_accuracy: 85.6800\n",
            "iteration: 200/500, train_loss: 0.4234, train_accuracy: 85.5250\n",
            "iteration: 300/500, train_loss: 0.4285, train_accuracy: 85.3100\n",
            "iteration: 400/500, train_loss: 0.4317, train_accuracy: 85.2550\n",
            "iteration: 500/500, train_loss: 0.4321, train_accuracy: 85.3440, test_loss: 0.5129, test_accuracy: 82.9100\n",
            "epoch 25/200:\n",
            "iteration: 100/500, train_loss: 0.4308, train_accuracy: 85.1300\n",
            "iteration: 200/500, train_loss: 0.4242, train_accuracy: 85.5200\n",
            "iteration: 300/500, train_loss: 0.4141, train_accuracy: 85.8200\n",
            "iteration: 400/500, train_loss: 0.4419, train_accuracy: 84.9050\n",
            "iteration: 500/500, train_loss: 0.4638, train_accuracy: 84.1920, test_loss: 0.5194, test_accuracy: 82.4700\n",
            "epoch 26/200:\n",
            "iteration: 100/500, train_loss: 0.4605, train_accuracy: 84.1100\n",
            "iteration: 200/500, train_loss: 0.4493, train_accuracy: 84.6000\n",
            "iteration: 300/500, train_loss: 0.4348, train_accuracy: 85.1267\n",
            "iteration: 400/500, train_loss: 0.4321, train_accuracy: 85.1800\n",
            "iteration: 500/500, train_loss: 0.4330, train_accuracy: 85.1780, test_loss: 0.5054, test_accuracy: 83.2300\n",
            "epoch 27/200:\n",
            "iteration: 100/500, train_loss: 0.4469, train_accuracy: 85.0200\n",
            "iteration: 200/500, train_loss: 0.4424, train_accuracy: 85.2050\n",
            "iteration: 300/500, train_loss: 0.4567, train_accuracy: 84.9633\n",
            "iteration: 400/500, train_loss: 0.4632, train_accuracy: 84.5500\n",
            "iteration: 500/500, train_loss: 0.4528, train_accuracy: 84.8180, test_loss: 0.4787, test_accuracy: 84.2900\n",
            "epoch 28/200:\n",
            "iteration: 100/500, train_loss: 0.4117, train_accuracy: 86.5900\n",
            "iteration: 200/500, train_loss: 0.4519, train_accuracy: 85.4600\n",
            "iteration: 300/500, train_loss: 0.4456, train_accuracy: 85.4600\n",
            "iteration: 400/500, train_loss: 0.4413, train_accuracy: 85.4600\n",
            "iteration: 500/500, train_loss: 0.4438, train_accuracy: 85.3600, test_loss: 0.7974, test_accuracy: 74.4800\n",
            "epoch 29/200:\n",
            "iteration: 100/500, train_loss: 0.5470, train_accuracy: 81.6800\n",
            "iteration: 200/500, train_loss: 0.4965, train_accuracy: 83.2350\n",
            "iteration: 300/500, train_loss: 0.4647, train_accuracy: 84.2533\n",
            "iteration: 400/500, train_loss: 0.4580, train_accuracy: 84.6000\n",
            "iteration: 500/500, train_loss: 0.4425, train_accuracy: 85.0860, test_loss: 0.4744, test_accuracy: 84.2600\n",
            "epoch 30/200:\n",
            "iteration: 100/500, train_loss: 0.3796, train_accuracy: 87.0700\n",
            "iteration: 200/500, train_loss: 0.3805, train_accuracy: 87.0150\n",
            "iteration: 300/500, train_loss: 0.3762, train_accuracy: 87.0100\n",
            "iteration: 400/500, train_loss: 0.3768, train_accuracy: 87.0500\n",
            "iteration: 500/500, train_loss: 0.3750, train_accuracy: 87.1080, test_loss: 0.4501, test_accuracy: 85.2500\n",
            "epoch 31/200:\n",
            "iteration: 100/500, train_loss: 0.3665, train_accuracy: 87.6600\n",
            "iteration: 200/500, train_loss: 0.3637, train_accuracy: 87.6000\n",
            "iteration: 300/500, train_loss: 0.3618, train_accuracy: 87.6933\n",
            "iteration: 400/500, train_loss: 0.3643, train_accuracy: 87.5800\n",
            "iteration: 500/500, train_loss: 0.3610, train_accuracy: 87.6400, test_loss: 0.4404, test_accuracy: 85.1500\n",
            "epoch 32/200:\n",
            "iteration: 100/500, train_loss: 0.3466, train_accuracy: 87.8400\n",
            "iteration: 200/500, train_loss: 0.3547, train_accuracy: 87.5300\n",
            "iteration: 300/500, train_loss: 0.3511, train_accuracy: 87.6100\n",
            "iteration: 400/500, train_loss: 0.3524, train_accuracy: 87.7100\n",
            "iteration: 500/500, train_loss: 0.3584, train_accuracy: 87.6060, test_loss: 0.5637, test_accuracy: 81.5100\n",
            "epoch 33/200:\n",
            "iteration: 100/500, train_loss: 0.3977, train_accuracy: 86.3500\n",
            "iteration: 200/500, train_loss: 0.3880, train_accuracy: 86.5700\n",
            "iteration: 300/500, train_loss: 0.3759, train_accuracy: 87.0933\n",
            "iteration: 400/500, train_loss: 0.3916, train_accuracy: 86.5475\n",
            "iteration: 500/500, train_loss: 0.5182, train_accuracy: 83.5860, test_loss: 0.7653, test_accuracy: 76.0100\n",
            "epoch 34/200:\n",
            "iteration: 100/500, train_loss: 0.5462, train_accuracy: 81.5600\n",
            "iteration: 200/500, train_loss: 0.5116, train_accuracy: 82.6100\n",
            "iteration: 300/500, train_loss: 0.4836, train_accuracy: 83.5800\n",
            "iteration: 400/500, train_loss: 0.4658, train_accuracy: 84.1425\n",
            "iteration: 500/500, train_loss: 0.4499, train_accuracy: 84.6960, test_loss: 0.4665, test_accuracy: 84.5700\n",
            "epoch 35/200:\n",
            "iteration: 100/500, train_loss: 0.3697, train_accuracy: 87.4500\n",
            "iteration: 200/500, train_loss: 0.3684, train_accuracy: 87.4850\n",
            "iteration: 300/500, train_loss: 0.3629, train_accuracy: 87.6767\n",
            "iteration: 400/500, train_loss: 0.3598, train_accuracy: 87.7650\n",
            "iteration: 500/500, train_loss: 0.3593, train_accuracy: 87.7960, test_loss: 0.4323, test_accuracy: 85.2400\n",
            "epoch 36/200:\n",
            "iteration: 100/500, train_loss: 0.3490, train_accuracy: 87.9600\n",
            "iteration: 200/500, train_loss: 0.3490, train_accuracy: 87.9150\n",
            "iteration: 300/500, train_loss: 0.3424, train_accuracy: 88.1533\n",
            "iteration: 400/500, train_loss: 0.3428, train_accuracy: 88.1525\n",
            "iteration: 500/500, train_loss: 0.3407, train_accuracy: 88.3100, test_loss: 0.4377, test_accuracy: 85.7100\n",
            "epoch 37/200:\n",
            "iteration: 100/500, train_loss: 0.3172, train_accuracy: 89.3000\n",
            "iteration: 200/500, train_loss: 0.3273, train_accuracy: 88.8100\n",
            "iteration: 300/500, train_loss: 0.3271, train_accuracy: 88.7800\n",
            "iteration: 400/500, train_loss: 0.3304, train_accuracy: 88.6825\n",
            "iteration: 500/500, train_loss: 0.3294, train_accuracy: 88.7140, test_loss: 0.4241, test_accuracy: 85.9100\n",
            "epoch 38/200:\n",
            "iteration: 100/500, train_loss: 0.3278, train_accuracy: 88.7700\n",
            "iteration: 200/500, train_loss: 0.3262, train_accuracy: 88.9150\n",
            "iteration: 300/500, train_loss: 0.3196, train_accuracy: 88.9367\n",
            "iteration: 400/500, train_loss: 0.3214, train_accuracy: 88.8450\n",
            "iteration: 500/500, train_loss: 0.3209, train_accuracy: 88.8700, test_loss: 0.4462, test_accuracy: 85.2700\n",
            "epoch 39/200:\n",
            "iteration: 100/500, train_loss: 0.2947, train_accuracy: 90.1000\n",
            "iteration: 200/500, train_loss: 0.3074, train_accuracy: 89.5450\n",
            "iteration: 300/500, train_loss: 0.3043, train_accuracy: 89.6233\n",
            "iteration: 400/500, train_loss: 0.3096, train_accuracy: 89.4125\n",
            "iteration: 500/500, train_loss: 0.3096, train_accuracy: 89.3860, test_loss: 0.4459, test_accuracy: 85.6700\n",
            "epoch 40/200:\n",
            "iteration: 100/500, train_loss: 0.3007, train_accuracy: 89.5600\n",
            "iteration: 200/500, train_loss: 0.3128, train_accuracy: 89.1650\n",
            "iteration: 300/500, train_loss: 0.3105, train_accuracy: 89.2267\n",
            "iteration: 400/500, train_loss: 0.3114, train_accuracy: 89.2525\n",
            "iteration: 500/500, train_loss: 0.3130, train_accuracy: 89.2680, test_loss: 0.4338, test_accuracy: 85.6300\n",
            "epoch 41/200:\n",
            "iteration: 100/500, train_loss: 0.3044, train_accuracy: 89.8000\n",
            "iteration: 200/500, train_loss: 0.3100, train_accuracy: 89.4650\n",
            "iteration: 300/500, train_loss: 0.3077, train_accuracy: 89.5200\n",
            "iteration: 400/500, train_loss: 0.3065, train_accuracy: 89.4550\n",
            "iteration: 500/500, train_loss: 0.3147, train_accuracy: 89.2360, test_loss: 0.4452, test_accuracy: 86.1900\n",
            "epoch 42/200:\n",
            "iteration: 100/500, train_loss: 0.3082, train_accuracy: 89.3100\n",
            "iteration: 200/500, train_loss: 0.3354, train_accuracy: 88.3700\n",
            "iteration: 300/500, train_loss: 0.3282, train_accuracy: 88.6433\n",
            "iteration: 400/500, train_loss: 0.3342, train_accuracy: 88.5500\n",
            "iteration: 500/500, train_loss: 0.3305, train_accuracy: 88.6920, test_loss: 0.4278, test_accuracy: 85.8600\n",
            "epoch 43/200:\n",
            "iteration: 100/500, train_loss: 0.4039, train_accuracy: 87.1900\n",
            "iteration: 200/500, train_loss: 0.4088, train_accuracy: 86.7100\n",
            "iteration: 300/500, train_loss: 0.3918, train_accuracy: 87.1100\n",
            "iteration: 400/500, train_loss: 0.3776, train_accuracy: 87.4800\n",
            "iteration: 500/500, train_loss: 0.3634, train_accuracy: 87.8840, test_loss: 0.4331, test_accuracy: 85.8600\n",
            "epoch 44/200:\n",
            "iteration: 100/500, train_loss: 0.3030, train_accuracy: 89.9000\n",
            "iteration: 200/500, train_loss: 0.3019, train_accuracy: 89.6400\n",
            "iteration: 300/500, train_loss: 0.2978, train_accuracy: 89.7467\n",
            "iteration: 400/500, train_loss: 0.2979, train_accuracy: 89.7850\n",
            "iteration: 500/500, train_loss: 0.2943, train_accuracy: 89.9020, test_loss: 0.4218, test_accuracy: 86.3500\n",
            "epoch 45/200:\n",
            "iteration: 100/500, train_loss: 0.2837, train_accuracy: 90.5000\n",
            "iteration: 200/500, train_loss: 0.2894, train_accuracy: 90.1250\n",
            "iteration: 300/500, train_loss: 0.2831, train_accuracy: 90.3200\n",
            "iteration: 400/500, train_loss: 0.2863, train_accuracy: 90.2175\n",
            "iteration: 500/500, train_loss: 0.2838, train_accuracy: 90.3220, test_loss: 0.4357, test_accuracy: 86.2500\n",
            "epoch 46/200:\n",
            "iteration: 100/500, train_loss: 0.2727, train_accuracy: 90.6900\n",
            "iteration: 200/500, train_loss: 0.2826, train_accuracy: 90.1750\n",
            "iteration: 300/500, train_loss: 0.2779, train_accuracy: 90.3000\n",
            "iteration: 400/500, train_loss: 0.2813, train_accuracy: 90.1200\n",
            "iteration: 500/500, train_loss: 0.2798, train_accuracy: 90.1920, test_loss: 0.4376, test_accuracy: 86.5100\n",
            "epoch 47/200:\n",
            "iteration: 100/500, train_loss: 0.2669, train_accuracy: 90.8800\n",
            "iteration: 200/500, train_loss: 0.2813, train_accuracy: 90.1500\n",
            "iteration: 300/500, train_loss: 0.2773, train_accuracy: 90.2100\n",
            "iteration: 400/500, train_loss: 0.2810, train_accuracy: 90.1700\n",
            "iteration: 500/500, train_loss: 0.2799, train_accuracy: 90.2440, test_loss: 0.4146, test_accuracy: 86.7500\n",
            "epoch 48/200:\n",
            "iteration: 100/500, train_loss: 0.2669, train_accuracy: 90.6300\n",
            "iteration: 200/500, train_loss: 0.2767, train_accuracy: 90.4100\n",
            "iteration: 300/500, train_loss: 0.2736, train_accuracy: 90.4633\n",
            "iteration: 400/500, train_loss: 0.2762, train_accuracy: 90.3300\n",
            "iteration: 500/500, train_loss: 0.2831, train_accuracy: 90.2240, test_loss: 0.6667, test_accuracy: 80.2500\n",
            "epoch 49/200:\n",
            "iteration: 100/500, train_loss: 0.3828, train_accuracy: 87.6200\n",
            "iteration: 200/500, train_loss: 0.3595, train_accuracy: 88.2150\n",
            "iteration: 300/500, train_loss: 0.3364, train_accuracy: 88.8433\n",
            "iteration: 400/500, train_loss: 0.3276, train_accuracy: 89.0550\n",
            "iteration: 500/500, train_loss: 0.3350, train_accuracy: 88.9240, test_loss: 0.5705, test_accuracy: 81.7300\n",
            "epoch 50/200:\n",
            "iteration: 100/500, train_loss: 0.3640, train_accuracy: 87.4300\n",
            "iteration: 200/500, train_loss: 0.3590, train_accuracy: 87.8250\n",
            "iteration: 300/500, train_loss: 0.3385, train_accuracy: 88.4400\n",
            "iteration: 400/500, train_loss: 0.3248, train_accuracy: 88.8975\n",
            "iteration: 500/500, train_loss: 0.3157, train_accuracy: 89.1840, test_loss: 0.4212, test_accuracy: 86.4900\n",
            "epoch 51/200:\n",
            "iteration: 100/500, train_loss: 0.2653, train_accuracy: 90.6500\n",
            "iteration: 200/500, train_loss: 0.2717, train_accuracy: 90.5400\n",
            "iteration: 300/500, train_loss: 0.2672, train_accuracy: 90.6433\n",
            "iteration: 400/500, train_loss: 0.2701, train_accuracy: 90.5650\n",
            "iteration: 500/500, train_loss: 0.2687, train_accuracy: 90.6340, test_loss: 0.4146, test_accuracy: 86.6600\n",
            "epoch 52/200:\n",
            "iteration: 100/500, train_loss: 0.2562, train_accuracy: 91.0700\n",
            "iteration: 200/500, train_loss: 0.2624, train_accuracy: 90.9750\n",
            "iteration: 300/500, train_loss: 0.2606, train_accuracy: 91.0067\n",
            "iteration: 400/500, train_loss: 0.2600, train_accuracy: 91.0000\n",
            "iteration: 500/500, train_loss: 0.2595, train_accuracy: 90.9880, test_loss: 0.4138, test_accuracy: 86.6800\n",
            "epoch 53/200:\n",
            "iteration: 100/500, train_loss: 0.2503, train_accuracy: 91.4200\n",
            "iteration: 200/500, train_loss: 0.2519, train_accuracy: 91.4000\n",
            "iteration: 300/500, train_loss: 0.2509, train_accuracy: 91.4433\n",
            "iteration: 400/500, train_loss: 0.2503, train_accuracy: 91.4300\n",
            "iteration: 500/500, train_loss: 0.2509, train_accuracy: 91.3520, test_loss: 0.4204, test_accuracy: 86.5500\n",
            "epoch 54/200:\n",
            "iteration: 100/500, train_loss: 0.2475, train_accuracy: 91.4800\n",
            "iteration: 200/500, train_loss: 0.2485, train_accuracy: 91.2600\n",
            "iteration: 300/500, train_loss: 0.2479, train_accuracy: 91.3200\n",
            "iteration: 400/500, train_loss: 0.2499, train_accuracy: 91.3225\n",
            "iteration: 500/500, train_loss: 0.2494, train_accuracy: 91.3100, test_loss: 0.4120, test_accuracy: 86.9400\n",
            "epoch 55/200:\n",
            "iteration: 100/500, train_loss: 0.2510, train_accuracy: 91.0100\n",
            "iteration: 200/500, train_loss: 0.2540, train_accuracy: 90.9650\n",
            "iteration: 300/500, train_loss: 0.2492, train_accuracy: 91.1367\n",
            "iteration: 400/500, train_loss: 0.2491, train_accuracy: 91.2350\n",
            "iteration: 500/500, train_loss: 0.2491, train_accuracy: 91.3120, test_loss: 0.4260, test_accuracy: 86.6200\n",
            "epoch 56/200:\n",
            "iteration: 100/500, train_loss: 0.2447, train_accuracy: 91.5300\n",
            "iteration: 200/500, train_loss: 0.2516, train_accuracy: 91.1600\n",
            "iteration: 300/500, train_loss: 0.2488, train_accuracy: 91.2667\n",
            "iteration: 400/500, train_loss: 0.2538, train_accuracy: 91.0600\n",
            "iteration: 500/500, train_loss: 0.2527, train_accuracy: 91.1480, test_loss: 0.4131, test_accuracy: 86.5700\n",
            "epoch 57/200:\n",
            "iteration: 100/500, train_loss: 0.2390, train_accuracy: 91.7300\n",
            "iteration: 200/500, train_loss: 0.2448, train_accuracy: 91.4900\n",
            "iteration: 300/500, train_loss: 0.2446, train_accuracy: 91.4467\n",
            "iteration: 400/500, train_loss: 0.2974, train_accuracy: 90.1875\n",
            "iteration: 500/500, train_loss: 0.3440, train_accuracy: 88.8880, test_loss: 0.5199, test_accuracy: 83.1500\n",
            "epoch 58/200:\n",
            "iteration: 100/500, train_loss: 0.3561, train_accuracy: 87.7500\n",
            "iteration: 200/500, train_loss: 0.3424, train_accuracy: 88.2650\n",
            "iteration: 300/500, train_loss: 0.3234, train_accuracy: 88.8967\n",
            "iteration: 400/500, train_loss: 0.3258, train_accuracy: 88.7950\n",
            "iteration: 500/500, train_loss: 0.3163, train_accuracy: 89.1220, test_loss: 0.4230, test_accuracy: 86.0200\n",
            "epoch 59/200:\n",
            "iteration: 100/500, train_loss: 0.2645, train_accuracy: 91.0100\n",
            "iteration: 200/500, train_loss: 0.2662, train_accuracy: 90.8750\n",
            "iteration: 300/500, train_loss: 0.2567, train_accuracy: 91.1433\n",
            "iteration: 400/500, train_loss: 0.2556, train_accuracy: 91.2375\n",
            "iteration: 500/500, train_loss: 0.2541, train_accuracy: 91.3240, test_loss: 0.4037, test_accuracy: 87.4000\n",
            "epoch 60/200:\n",
            "iteration: 100/500, train_loss: 0.2328, train_accuracy: 92.0700\n",
            "iteration: 200/500, train_loss: 0.2399, train_accuracy: 91.7350\n",
            "iteration: 300/500, train_loss: 0.2364, train_accuracy: 91.7800\n",
            "iteration: 400/500, train_loss: 0.2366, train_accuracy: 91.7300\n",
            "iteration: 500/500, train_loss: 0.2364, train_accuracy: 91.7600, test_loss: 0.4047, test_accuracy: 87.4000\n",
            "epoch 61/200:\n",
            "iteration: 100/500, train_loss: 0.2196, train_accuracy: 92.5000\n",
            "iteration: 200/500, train_loss: 0.2291, train_accuracy: 92.0800\n",
            "iteration: 300/500, train_loss: 0.2280, train_accuracy: 91.9933\n",
            "iteration: 400/500, train_loss: 0.2295, train_accuracy: 91.9475\n",
            "iteration: 500/500, train_loss: 0.2302, train_accuracy: 91.9160, test_loss: 0.4028, test_accuracy: 87.2900\n",
            "epoch 62/200:\n",
            "iteration: 100/500, train_loss: 0.2567, train_accuracy: 91.6300\n",
            "iteration: 200/500, train_loss: 0.2534, train_accuracy: 91.4300\n",
            "iteration: 300/500, train_loss: 0.2424, train_accuracy: 91.6833\n",
            "iteration: 400/500, train_loss: 0.2398, train_accuracy: 91.6700\n",
            "iteration: 500/500, train_loss: 0.2388, train_accuracy: 91.7300, test_loss: 0.4051, test_accuracy: 87.3600\n",
            "epoch 63/200:\n",
            "iteration: 100/500, train_loss: 0.2188, train_accuracy: 92.5900\n",
            "iteration: 200/500, train_loss: 0.2254, train_accuracy: 92.2950\n",
            "iteration: 300/500, train_loss: 0.2202, train_accuracy: 92.4600\n",
            "iteration: 400/500, train_loss: 0.2248, train_accuracy: 92.2700\n",
            "iteration: 500/500, train_loss: 0.2254, train_accuracy: 92.2860, test_loss: 0.3990, test_accuracy: 87.1700\n",
            "epoch 64/200:\n",
            "iteration: 100/500, train_loss: 0.2169, train_accuracy: 92.5200\n",
            "iteration: 200/500, train_loss: 0.2215, train_accuracy: 92.3000\n",
            "iteration: 300/500, train_loss: 0.2214, train_accuracy: 92.3000\n",
            "iteration: 400/500, train_loss: 0.2228, train_accuracy: 92.2775\n",
            "iteration: 500/500, train_loss: 0.2226, train_accuracy: 92.2380, test_loss: 0.3917, test_accuracy: 87.4800\n",
            "epoch 65/200:\n",
            "iteration: 100/500, train_loss: 0.2227, train_accuracy: 92.2400\n",
            "iteration: 200/500, train_loss: 0.2256, train_accuracy: 92.0650\n",
            "iteration: 300/500, train_loss: 0.2220, train_accuracy: 92.2333\n",
            "iteration: 400/500, train_loss: 0.2234, train_accuracy: 92.2200\n",
            "iteration: 500/500, train_loss: 0.2231, train_accuracy: 92.2580, test_loss: 0.4082, test_accuracy: 87.1600\n",
            "epoch 66/200:\n",
            "iteration: 100/500, train_loss: 0.2209, train_accuracy: 92.5700\n",
            "iteration: 200/500, train_loss: 0.2219, train_accuracy: 92.6150\n",
            "iteration: 300/500, train_loss: 0.2199, train_accuracy: 92.5533\n",
            "iteration: 400/500, train_loss: 0.2213, train_accuracy: 92.4575\n",
            "iteration: 500/500, train_loss: 0.2216, train_accuracy: 92.4440, test_loss: 0.4070, test_accuracy: 87.3100\n",
            "epoch 67/200:\n",
            "iteration: 100/500, train_loss: 0.2210, train_accuracy: 92.4500\n",
            "iteration: 200/500, train_loss: 0.2213, train_accuracy: 92.4500\n",
            "iteration: 300/500, train_loss: 0.2167, train_accuracy: 92.5767\n",
            "iteration: 400/500, train_loss: 0.2195, train_accuracy: 92.4675\n",
            "iteration: 500/500, train_loss: 0.2198, train_accuracy: 92.4220, test_loss: 0.4060, test_accuracy: 87.0600\n",
            "epoch 68/200:\n",
            "iteration: 100/500, train_loss: 0.2138, train_accuracy: 92.7900\n",
            "iteration: 200/500, train_loss: 0.2215, train_accuracy: 92.4700\n",
            "iteration: 300/500, train_loss: 0.2169, train_accuracy: 92.4867\n",
            "iteration: 400/500, train_loss: 0.2213, train_accuracy: 92.2525\n",
            "iteration: 500/500, train_loss: 0.2204, train_accuracy: 92.2680, test_loss: 0.3944, test_accuracy: 87.5400\n",
            "epoch 69/200:\n",
            "iteration: 100/500, train_loss: 0.2052, train_accuracy: 92.9300\n",
            "iteration: 200/500, train_loss: 0.2105, train_accuracy: 92.5950\n",
            "iteration: 300/500, train_loss: 0.2075, train_accuracy: 92.8033\n",
            "iteration: 400/500, train_loss: 0.2120, train_accuracy: 92.6375\n",
            "iteration: 500/500, train_loss: 0.2126, train_accuracy: 92.5920, test_loss: 0.3982, test_accuracy: 87.5600\n",
            "epoch 70/200:\n",
            "iteration: 100/500, train_loss: 0.2052, train_accuracy: 92.8300\n",
            "iteration: 200/500, train_loss: 0.2182, train_accuracy: 92.4200\n",
            "iteration: 300/500, train_loss: 0.2107, train_accuracy: 92.6833\n",
            "iteration: 400/500, train_loss: 0.2142, train_accuracy: 92.5750\n",
            "iteration: 500/500, train_loss: 0.2126, train_accuracy: 92.6280, test_loss: 0.4136, test_accuracy: 87.1900\n",
            "epoch 71/200:\n",
            "iteration: 100/500, train_loss: 0.2102, train_accuracy: 92.5400\n",
            "iteration: 200/500, train_loss: 0.2171, train_accuracy: 92.2950\n",
            "iteration: 300/500, train_loss: 0.2139, train_accuracy: 92.4467\n",
            "iteration: 400/500, train_loss: 0.2151, train_accuracy: 92.4225\n",
            "iteration: 500/500, train_loss: 0.2118, train_accuracy: 92.5000, test_loss: 0.4278, test_accuracy: 87.0200\n",
            "epoch 72/200:\n",
            "iteration: 100/500, train_loss: 0.2020, train_accuracy: 92.9700\n",
            "iteration: 200/500, train_loss: 0.2104, train_accuracy: 92.5750\n",
            "iteration: 300/500, train_loss: 0.2047, train_accuracy: 92.7800\n",
            "iteration: 400/500, train_loss: 0.2089, train_accuracy: 92.6050\n",
            "iteration: 500/500, train_loss: 0.2070, train_accuracy: 92.6720, test_loss: 0.4065, test_accuracy: 87.5200\n",
            "epoch 73/200:\n",
            "iteration: 100/500, train_loss: 0.2006, train_accuracy: 93.2300\n",
            "iteration: 200/500, train_loss: 0.2042, train_accuracy: 92.9350\n",
            "iteration: 300/500, train_loss: 0.2021, train_accuracy: 92.9467\n",
            "iteration: 400/500, train_loss: 0.2047, train_accuracy: 92.8250\n",
            "iteration: 500/500, train_loss: 0.2048, train_accuracy: 92.8220, test_loss: 0.4199, test_accuracy: 87.4700\n",
            "epoch 74/200:\n",
            "iteration: 100/500, train_loss: 0.2048, train_accuracy: 93.0300\n",
            "iteration: 200/500, train_loss: 0.2083, train_accuracy: 92.8000\n",
            "iteration: 300/500, train_loss: 0.2043, train_accuracy: 92.9167\n",
            "iteration: 400/500, train_loss: 0.2053, train_accuracy: 92.8900\n",
            "iteration: 500/500, train_loss: 0.2024, train_accuracy: 92.9700, test_loss: 0.4258, test_accuracy: 87.6100\n",
            "epoch 75/200:\n",
            "iteration: 100/500, train_loss: 0.1995, train_accuracy: 93.0600\n",
            "iteration: 200/500, train_loss: 0.2000, train_accuracy: 92.8950\n",
            "iteration: 300/500, train_loss: 0.1968, train_accuracy: 93.0600\n",
            "iteration: 400/500, train_loss: 0.1998, train_accuracy: 92.9500\n",
            "iteration: 500/500, train_loss: 0.2001, train_accuracy: 92.9500, test_loss: 0.3880, test_accuracy: 87.9500\n",
            "epoch 76/200:\n",
            "iteration: 100/500, train_loss: 0.1827, train_accuracy: 93.7300\n",
            "iteration: 200/500, train_loss: 0.1944, train_accuracy: 93.3100\n",
            "iteration: 300/500, train_loss: 0.1937, train_accuracy: 93.3333\n",
            "iteration: 400/500, train_loss: 0.1961, train_accuracy: 93.2050\n",
            "iteration: 500/500, train_loss: 0.1944, train_accuracy: 93.2780, test_loss: 0.4164, test_accuracy: 87.7700\n",
            "epoch 77/200:\n",
            "iteration: 100/500, train_loss: 0.1920, train_accuracy: 93.3800\n",
            "iteration: 200/500, train_loss: 0.1975, train_accuracy: 93.1250\n",
            "iteration: 300/500, train_loss: 0.1975, train_accuracy: 93.0933\n",
            "iteration: 400/500, train_loss: 0.1992, train_accuracy: 93.0525\n",
            "iteration: 500/500, train_loss: 0.1968, train_accuracy: 93.1240, test_loss: 0.4094, test_accuracy: 87.5500\n",
            "epoch 78/200:\n",
            "iteration: 100/500, train_loss: 0.2022, train_accuracy: 93.0700\n",
            "iteration: 200/500, train_loss: 0.2068, train_accuracy: 92.9250\n",
            "iteration: 300/500, train_loss: 0.2018, train_accuracy: 92.9500\n",
            "iteration: 400/500, train_loss: 0.1999, train_accuracy: 93.0400\n",
            "iteration: 500/500, train_loss: 0.1971, train_accuracy: 93.1280, test_loss: 0.4248, test_accuracy: 87.7000\n",
            "epoch 79/200:\n",
            "iteration: 100/500, train_loss: 0.1904, train_accuracy: 93.3500\n",
            "iteration: 200/500, train_loss: 0.1973, train_accuracy: 93.1300\n",
            "iteration: 300/500, train_loss: 0.1959, train_accuracy: 93.1400\n",
            "iteration: 400/500, train_loss: 0.1942, train_accuracy: 93.1675\n",
            "iteration: 500/500, train_loss: 0.1928, train_accuracy: 93.2280, test_loss: 0.3924, test_accuracy: 87.9600\n",
            "epoch 80/200:\n",
            "iteration: 100/500, train_loss: 0.1736, train_accuracy: 93.8300\n",
            "iteration: 200/500, train_loss: 0.1837, train_accuracy: 93.5800\n",
            "iteration: 300/500, train_loss: 0.1837, train_accuracy: 93.6100\n",
            "iteration: 400/500, train_loss: 0.1868, train_accuracy: 93.4875\n",
            "iteration: 500/500, train_loss: 0.1858, train_accuracy: 93.4940, test_loss: 0.4028, test_accuracy: 87.6600\n",
            "epoch 81/200:\n",
            "iteration: 100/500, train_loss: 0.1836, train_accuracy: 93.6100\n",
            "iteration: 200/500, train_loss: 0.1863, train_accuracy: 93.5800\n",
            "iteration: 300/500, train_loss: 0.1824, train_accuracy: 93.7200\n",
            "iteration: 400/500, train_loss: 0.1830, train_accuracy: 93.6250\n",
            "iteration: 500/500, train_loss: 0.1828, train_accuracy: 93.6280, test_loss: 0.4019, test_accuracy: 87.8300\n",
            "epoch 82/200:\n",
            "iteration: 100/500, train_loss: 0.1796, train_accuracy: 93.6000\n",
            "iteration: 200/500, train_loss: 0.1889, train_accuracy: 93.3850\n",
            "iteration: 300/500, train_loss: 0.1860, train_accuracy: 93.4767\n",
            "iteration: 400/500, train_loss: 0.1890, train_accuracy: 93.4075\n",
            "iteration: 500/500, train_loss: 0.1868, train_accuracy: 93.4920, test_loss: 0.3993, test_accuracy: 87.9100\n",
            "epoch 83/200:\n",
            "iteration: 100/500, train_loss: 0.1756, train_accuracy: 93.6900\n",
            "iteration: 200/500, train_loss: 0.1836, train_accuracy: 93.4850\n",
            "iteration: 300/500, train_loss: 0.1783, train_accuracy: 93.6867\n",
            "iteration: 400/500, train_loss: 0.1820, train_accuracy: 93.5975\n",
            "iteration: 500/500, train_loss: 0.1808, train_accuracy: 93.6100, test_loss: 0.4081, test_accuracy: 87.7200\n",
            "epoch 84/200:\n",
            "iteration: 100/500, train_loss: 0.1782, train_accuracy: 93.7700\n",
            "iteration: 200/500, train_loss: 0.1833, train_accuracy: 93.5050\n",
            "iteration: 300/500, train_loss: 0.1842, train_accuracy: 93.4833\n",
            "iteration: 400/500, train_loss: 0.1839, train_accuracy: 93.4975\n",
            "iteration: 500/500, train_loss: 0.1828, train_accuracy: 93.5340, test_loss: 0.4141, test_accuracy: 87.5100\n",
            "epoch 85/200:\n",
            "iteration: 100/500, train_loss: 0.1759, train_accuracy: 93.7500\n",
            "iteration: 200/500, train_loss: 0.1823, train_accuracy: 93.4850\n",
            "iteration: 300/500, train_loss: 0.1787, train_accuracy: 93.6300\n",
            "iteration: 400/500, train_loss: 0.1799, train_accuracy: 93.5950\n",
            "iteration: 500/500, train_loss: 0.1778, train_accuracy: 93.6800, test_loss: 0.4203, test_accuracy: 88.0800\n",
            "epoch 86/200:\n",
            "iteration: 100/500, train_loss: 0.1746, train_accuracy: 94.0200\n",
            "iteration: 200/500, train_loss: 0.2399, train_accuracy: 92.1850\n",
            "iteration: 300/500, train_loss: 0.2297, train_accuracy: 92.3600\n",
            "iteration: 400/500, train_loss: 0.2284, train_accuracy: 92.2825\n",
            "iteration: 500/500, train_loss: 0.2223, train_accuracy: 92.4620, test_loss: 0.4164, test_accuracy: 87.3700\n",
            "epoch 87/200:\n",
            "iteration: 100/500, train_loss: 0.1774, train_accuracy: 93.8000\n",
            "iteration: 200/500, train_loss: 0.1788, train_accuracy: 93.7200\n",
            "iteration: 300/500, train_loss: 0.1761, train_accuracy: 93.8333\n",
            "iteration: 400/500, train_loss: 0.1769, train_accuracy: 93.7975\n",
            "iteration: 500/500, train_loss: 0.1767, train_accuracy: 93.7880, test_loss: 0.4301, test_accuracy: 87.3100\n",
            "epoch 88/200:\n",
            "iteration: 100/500, train_loss: 0.1730, train_accuracy: 93.7300\n",
            "iteration: 200/500, train_loss: 0.1788, train_accuracy: 93.5850\n",
            "iteration: 300/500, train_loss: 0.1731, train_accuracy: 93.8200\n",
            "iteration: 400/500, train_loss: 0.1726, train_accuracy: 93.8900\n",
            "iteration: 500/500, train_loss: 0.1939, train_accuracy: 93.3980, test_loss: 0.4509, test_accuracy: 86.0600\n",
            "epoch 89/200:\n",
            "iteration: 100/500, train_loss: 0.2125, train_accuracy: 92.6500\n",
            "iteration: 200/500, train_loss: 0.2056, train_accuracy: 92.8200\n",
            "iteration: 300/500, train_loss: 0.1969, train_accuracy: 93.0333\n",
            "iteration: 400/500, train_loss: 0.2309, train_accuracy: 92.1925\n",
            "iteration: 500/500, train_loss: 0.2313, train_accuracy: 92.1760, test_loss: 0.4028, test_accuracy: 87.3400\n",
            "epoch 90/200:\n",
            "iteration: 100/500, train_loss: 0.2161, train_accuracy: 92.7100\n",
            "iteration: 200/500, train_loss: 0.2048, train_accuracy: 92.9200\n",
            "iteration: 300/500, train_loss: 0.1967, train_accuracy: 93.1000\n",
            "iteration: 400/500, train_loss: 0.1942, train_accuracy: 93.1775\n",
            "iteration: 500/500, train_loss: 0.1891, train_accuracy: 93.3960, test_loss: 0.3976, test_accuracy: 87.9900\n",
            "epoch 91/200:\n",
            "iteration: 100/500, train_loss: 0.1675, train_accuracy: 93.8900\n",
            "iteration: 200/500, train_loss: 0.1690, train_accuracy: 93.9850\n",
            "iteration: 300/500, train_loss: 0.1655, train_accuracy: 94.1133\n",
            "iteration: 400/500, train_loss: 0.1677, train_accuracy: 94.0525\n",
            "iteration: 500/500, train_loss: 0.1674, train_accuracy: 94.0640, test_loss: 0.4035, test_accuracy: 88.0700\n",
            "epoch 92/200:\n",
            "iteration: 100/500, train_loss: 0.1547, train_accuracy: 94.6500\n",
            "iteration: 200/500, train_loss: 0.1596, train_accuracy: 94.4000\n",
            "iteration: 300/500, train_loss: 0.1580, train_accuracy: 94.4833\n",
            "iteration: 400/500, train_loss: 0.1603, train_accuracy: 94.3975\n",
            "iteration: 500/500, train_loss: 0.1610, train_accuracy: 94.3260, test_loss: 0.4083, test_accuracy: 88.1500\n",
            "epoch 93/200:\n",
            "iteration: 100/500, train_loss: 0.1675, train_accuracy: 94.1300\n",
            "iteration: 200/500, train_loss: 0.1674, train_accuracy: 94.2100\n",
            "iteration: 300/500, train_loss: 0.1635, train_accuracy: 94.3767\n",
            "iteration: 400/500, train_loss: 0.1653, train_accuracy: 94.2625\n",
            "iteration: 500/500, train_loss: 0.1651, train_accuracy: 94.2180, test_loss: 0.3967, test_accuracy: 88.2700\n",
            "epoch 94/200:\n",
            "iteration: 100/500, train_loss: 0.1573, train_accuracy: 94.4800\n",
            "iteration: 200/500, train_loss: 0.1625, train_accuracy: 94.2200\n",
            "iteration: 300/500, train_loss: 0.1604, train_accuracy: 94.3167\n",
            "iteration: 400/500, train_loss: 0.1630, train_accuracy: 94.2925\n",
            "iteration: 500/500, train_loss: 0.1649, train_accuracy: 94.2080, test_loss: 0.3943, test_accuracy: 88.0400\n",
            "epoch 95/200:\n",
            "iteration: 100/500, train_loss: 0.1660, train_accuracy: 94.1100\n",
            "iteration: 200/500, train_loss: 0.1691, train_accuracy: 94.0700\n",
            "iteration: 300/500, train_loss: 0.1656, train_accuracy: 94.1233\n",
            "iteration: 400/500, train_loss: 0.1640, train_accuracy: 94.2150\n",
            "iteration: 500/500, train_loss: 0.1642, train_accuracy: 94.2060, test_loss: 0.3956, test_accuracy: 88.0600\n",
            "epoch 96/200:\n",
            "iteration: 100/500, train_loss: 0.1521, train_accuracy: 94.5700\n",
            "iteration: 200/500, train_loss: 0.1577, train_accuracy: 94.3950\n",
            "iteration: 300/500, train_loss: 0.1565, train_accuracy: 94.4333\n",
            "iteration: 400/500, train_loss: 0.1594, train_accuracy: 94.3475\n",
            "iteration: 500/500, train_loss: 0.1586, train_accuracy: 94.4000, test_loss: 0.4079, test_accuracy: 87.8800\n",
            "epoch 97/200:\n",
            "iteration: 100/500, train_loss: 0.1706, train_accuracy: 93.9600\n",
            "iteration: 200/500, train_loss: 0.1724, train_accuracy: 93.8900\n",
            "iteration: 300/500, train_loss: 0.1697, train_accuracy: 93.9800\n",
            "iteration: 400/500, train_loss: 0.1706, train_accuracy: 93.9325\n",
            "iteration: 500/500, train_loss: 0.1667, train_accuracy: 94.1000, test_loss: 0.4032, test_accuracy: 88.2600\n",
            "epoch 98/200:\n",
            "iteration: 100/500, train_loss: 0.1659, train_accuracy: 94.3900\n",
            "iteration: 200/500, train_loss: 0.1668, train_accuracy: 94.2700\n",
            "iteration: 300/500, train_loss: 0.1618, train_accuracy: 94.4500\n",
            "iteration: 400/500, train_loss: 0.1629, train_accuracy: 94.3625\n",
            "iteration: 500/500, train_loss: 0.1611, train_accuracy: 94.3780, test_loss: 0.4130, test_accuracy: 87.9800\n",
            "epoch 99/200:\n",
            "iteration: 100/500, train_loss: 0.1593, train_accuracy: 94.4600\n",
            "iteration: 200/500, train_loss: 0.1612, train_accuracy: 94.4250\n",
            "iteration: 300/500, train_loss: 0.1584, train_accuracy: 94.4400\n",
            "iteration: 400/500, train_loss: 0.1594, train_accuracy: 94.4025\n",
            "iteration: 500/500, train_loss: 0.1577, train_accuracy: 94.4640, test_loss: 0.4321, test_accuracy: 87.8900\n",
            "epoch 100/200:\n",
            "iteration: 100/500, train_loss: 0.1582, train_accuracy: 94.6200\n",
            "iteration: 200/500, train_loss: 0.1675, train_accuracy: 94.2850\n",
            "iteration: 300/500, train_loss: 0.1639, train_accuracy: 94.4133\n",
            "iteration: 400/500, train_loss: 0.1652, train_accuracy: 94.3450\n",
            "iteration: 500/500, train_loss: 0.1635, train_accuracy: 94.3840, test_loss: 0.4259, test_accuracy: 87.7400\n",
            "epoch 101/200:\n",
            "iteration: 100/500, train_loss: 0.1607, train_accuracy: 94.3000\n",
            "iteration: 200/500, train_loss: 0.1572, train_accuracy: 94.4250\n",
            "iteration: 300/500, train_loss: 0.1562, train_accuracy: 94.5400\n",
            "iteration: 400/500, train_loss: 0.1554, train_accuracy: 94.5600\n",
            "iteration: 500/500, train_loss: 0.1559, train_accuracy: 94.5560, test_loss: 0.4250, test_accuracy: 87.7700\n",
            "epoch 102/200:\n",
            "iteration: 100/500, train_loss: 0.1475, train_accuracy: 94.7800\n",
            "iteration: 200/500, train_loss: 0.1588, train_accuracy: 94.4750\n",
            "iteration: 300/500, train_loss: 0.1522, train_accuracy: 94.5667\n",
            "iteration: 400/500, train_loss: 0.1542, train_accuracy: 94.5200\n",
            "iteration: 500/500, train_loss: 0.1531, train_accuracy: 94.5200, test_loss: 0.4108, test_accuracy: 88.2400\n",
            "epoch 103/200:\n",
            "iteration: 100/500, train_loss: 0.1506, train_accuracy: 94.6100\n",
            "iteration: 200/500, train_loss: 0.1540, train_accuracy: 94.4350\n",
            "iteration: 300/500, train_loss: 0.1533, train_accuracy: 94.4967\n",
            "iteration: 400/500, train_loss: 0.1553, train_accuracy: 94.4175\n",
            "iteration: 500/500, train_loss: 0.1566, train_accuracy: 94.4280, test_loss: 0.4143, test_accuracy: 87.5700\n",
            "epoch 104/200:\n",
            "iteration: 100/500, train_loss: 0.1532, train_accuracy: 94.5600\n",
            "iteration: 200/500, train_loss: 0.1537, train_accuracy: 94.5850\n",
            "iteration: 300/500, train_loss: 0.1487, train_accuracy: 94.7200\n",
            "iteration: 400/500, train_loss: 0.1504, train_accuracy: 94.6625\n",
            "iteration: 500/500, train_loss: 0.1519, train_accuracy: 94.6660, test_loss: 0.3975, test_accuracy: 88.2400\n",
            "epoch 105/200:\n",
            "iteration: 100/500, train_loss: 0.1539, train_accuracy: 94.9600\n",
            "iteration: 200/500, train_loss: 0.1902, train_accuracy: 93.6150\n",
            "iteration: 300/500, train_loss: 0.1835, train_accuracy: 93.7567\n",
            "iteration: 400/500, train_loss: 0.1782, train_accuracy: 93.8725\n",
            "iteration: 500/500, train_loss: 0.1731, train_accuracy: 94.0260, test_loss: 0.4082, test_accuracy: 88.1700\n",
            "epoch 106/200:\n",
            "iteration: 100/500, train_loss: 0.1568, train_accuracy: 94.5000\n",
            "iteration: 200/500, train_loss: 0.1574, train_accuracy: 94.4700\n",
            "iteration: 300/500, train_loss: 0.1506, train_accuracy: 94.7033\n",
            "iteration: 400/500, train_loss: 0.1517, train_accuracy: 94.6425\n",
            "iteration: 500/500, train_loss: 0.1503, train_accuracy: 94.7220, test_loss: 0.3854, test_accuracy: 88.4700\n",
            "epoch 107/200:\n",
            "iteration: 100/500, train_loss: 0.1526, train_accuracy: 94.8400\n",
            "iteration: 200/500, train_loss: 0.1568, train_accuracy: 94.6800\n",
            "iteration: 300/500, train_loss: 0.1548, train_accuracy: 94.7033\n",
            "iteration: 400/500, train_loss: 0.1518, train_accuracy: 94.7875\n",
            "iteration: 500/500, train_loss: 0.1521, train_accuracy: 94.7900, test_loss: 0.4131, test_accuracy: 87.9400\n",
            "epoch 108/200:\n",
            "iteration: 100/500, train_loss: 0.1440, train_accuracy: 94.9000\n",
            "iteration: 200/500, train_loss: 0.1478, train_accuracy: 94.6950\n",
            "iteration: 300/500, train_loss: 0.1462, train_accuracy: 94.7767\n",
            "iteration: 400/500, train_loss: 0.1482, train_accuracy: 94.7275\n",
            "iteration: 500/500, train_loss: 0.1456, train_accuracy: 94.8560, test_loss: 0.4215, test_accuracy: 87.9400\n",
            "epoch 109/200:\n",
            "iteration: 100/500, train_loss: 0.1434, train_accuracy: 95.0800\n",
            "iteration: 200/500, train_loss: 0.1493, train_accuracy: 94.7250\n",
            "iteration: 300/500, train_loss: 0.1422, train_accuracy: 94.9033\n",
            "iteration: 400/500, train_loss: 0.1417, train_accuracy: 94.9600\n",
            "iteration: 500/500, train_loss: 0.1411, train_accuracy: 95.0280, test_loss: 0.4273, test_accuracy: 87.8600\n",
            "epoch 110/200:\n",
            "iteration: 100/500, train_loss: 0.1415, train_accuracy: 95.1000\n",
            "iteration: 200/500, train_loss: 0.1459, train_accuracy: 94.9150\n",
            "iteration: 300/500, train_loss: 0.1432, train_accuracy: 94.9867\n",
            "iteration: 400/500, train_loss: 0.1438, train_accuracy: 95.0100\n",
            "iteration: 500/500, train_loss: 0.1424, train_accuracy: 95.0280, test_loss: 0.4136, test_accuracy: 88.0800\n",
            "epoch 111/200:\n",
            "iteration: 100/500, train_loss: 0.1501, train_accuracy: 94.6300\n",
            "iteration: 200/500, train_loss: 0.1520, train_accuracy: 94.6800\n",
            "iteration: 300/500, train_loss: 0.1464, train_accuracy: 94.8233\n",
            "iteration: 400/500, train_loss: 0.1473, train_accuracy: 94.7950\n",
            "iteration: 500/500, train_loss: 0.1459, train_accuracy: 94.8740, test_loss: 0.4128, test_accuracy: 88.0300\n",
            "epoch 112/200:\n",
            "iteration: 100/500, train_loss: 0.1441, train_accuracy: 95.0100\n",
            "iteration: 200/500, train_loss: 0.1484, train_accuracy: 94.6950\n",
            "iteration: 300/500, train_loss: 0.1458, train_accuracy: 94.8400\n",
            "iteration: 400/500, train_loss: 0.1452, train_accuracy: 94.8175\n",
            "iteration: 500/500, train_loss: 0.1468, train_accuracy: 94.7620, test_loss: 0.4045, test_accuracy: 87.9100\n",
            "epoch 113/200:\n",
            "iteration: 100/500, train_loss: 0.1463, train_accuracy: 94.9600\n",
            "iteration: 200/500, train_loss: 0.1477, train_accuracy: 94.8350\n",
            "iteration: 300/500, train_loss: 0.1443, train_accuracy: 94.9667\n",
            "iteration: 400/500, train_loss: 0.1447, train_accuracy: 94.9575\n",
            "iteration: 500/500, train_loss: 0.1429, train_accuracy: 94.9980, test_loss: 0.4199, test_accuracy: 88.0000\n",
            "epoch 114/200:\n",
            "iteration: 100/500, train_loss: 0.1315, train_accuracy: 95.2200\n",
            "iteration: 200/500, train_loss: 0.1376, train_accuracy: 95.1300\n",
            "iteration: 300/500, train_loss: 0.1387, train_accuracy: 95.0600\n",
            "iteration: 400/500, train_loss: 0.1411, train_accuracy: 95.0025\n",
            "iteration: 500/500, train_loss: 0.1415, train_accuracy: 94.9980, test_loss: 0.4088, test_accuracy: 87.7000\n",
            "epoch 115/200:\n",
            "iteration: 100/500, train_loss: 0.1385, train_accuracy: 94.9600\n",
            "iteration: 200/500, train_loss: 0.1424, train_accuracy: 94.9800\n",
            "iteration: 300/500, train_loss: 0.1409, train_accuracy: 95.0167\n",
            "iteration: 400/500, train_loss: 0.1419, train_accuracy: 94.9975\n",
            "iteration: 500/500, train_loss: 0.1404, train_accuracy: 95.0560, test_loss: 0.4237, test_accuracy: 87.9400\n",
            "epoch 116/200:\n",
            "iteration: 100/500, train_loss: 0.1400, train_accuracy: 95.0900\n",
            "iteration: 200/500, train_loss: 0.1396, train_accuracy: 95.1250\n",
            "iteration: 300/500, train_loss: 0.1373, train_accuracy: 95.1733\n",
            "iteration: 400/500, train_loss: 0.1401, train_accuracy: 95.1175\n",
            "iteration: 500/500, train_loss: 0.1387, train_accuracy: 95.1040, test_loss: 0.4077, test_accuracy: 88.2700\n",
            "epoch 117/200:\n",
            "iteration: 100/500, train_loss: 0.1311, train_accuracy: 95.4500\n",
            "iteration: 200/500, train_loss: 0.1457, train_accuracy: 94.8950\n",
            "iteration: 300/500, train_loss: 0.1421, train_accuracy: 95.0100\n",
            "iteration: 400/500, train_loss: 0.1431, train_accuracy: 94.9475\n",
            "iteration: 500/500, train_loss: 0.1420, train_accuracy: 94.9480, test_loss: 0.4046, test_accuracy: 88.4600\n",
            "epoch 118/200:\n",
            "iteration: 100/500, train_loss: 0.1319, train_accuracy: 95.4300\n",
            "iteration: 200/500, train_loss: 0.1419, train_accuracy: 94.9550\n",
            "iteration: 300/500, train_loss: 0.1389, train_accuracy: 95.0367\n",
            "iteration: 400/500, train_loss: 0.1391, train_accuracy: 95.0700\n",
            "iteration: 500/500, train_loss: 0.1399, train_accuracy: 95.0600, test_loss: 0.4001, test_accuracy: 88.2700\n",
            "epoch 119/200:\n",
            "iteration: 100/500, train_loss: 0.1426, train_accuracy: 94.9100\n",
            "iteration: 200/500, train_loss: 0.1442, train_accuracy: 94.9000\n",
            "iteration: 300/500, train_loss: 0.1397, train_accuracy: 95.1167\n",
            "iteration: 400/500, train_loss: 0.1497, train_accuracy: 94.8300\n",
            "iteration: 500/500, train_loss: 0.1615, train_accuracy: 94.5800, test_loss: 0.4134, test_accuracy: 87.5900\n",
            "epoch 120/200:\n",
            "iteration: 100/500, train_loss: 0.1460, train_accuracy: 94.5600\n",
            "iteration: 200/500, train_loss: 0.2724, train_accuracy: 92.3100\n",
            "iteration: 300/500, train_loss: 0.2483, train_accuracy: 92.4533\n",
            "iteration: 400/500, train_loss: 0.2321, train_accuracy: 92.7125\n",
            "iteration: 500/500, train_loss: 0.2204, train_accuracy: 92.9280, test_loss: 0.4037, test_accuracy: 87.8600\n",
            "epoch 121/200:\n",
            "iteration: 100/500, train_loss: 0.1595, train_accuracy: 94.2900\n",
            "iteration: 200/500, train_loss: 0.1561, train_accuracy: 94.4300\n",
            "iteration: 300/500, train_loss: 0.1528, train_accuracy: 94.6133\n",
            "iteration: 400/500, train_loss: 0.1496, train_accuracy: 94.7150\n",
            "iteration: 500/500, train_loss: 0.1488, train_accuracy: 94.7240, test_loss: 0.4059, test_accuracy: 87.8800\n",
            "epoch 122/200:\n",
            "iteration: 100/500, train_loss: 0.1336, train_accuracy: 95.1500\n",
            "iteration: 200/500, train_loss: 0.1374, train_accuracy: 95.0600\n",
            "iteration: 300/500, train_loss: 0.1375, train_accuracy: 95.1000\n",
            "iteration: 400/500, train_loss: 0.1367, train_accuracy: 95.1350\n",
            "iteration: 500/500, train_loss: 0.1350, train_accuracy: 95.2440, test_loss: 0.4068, test_accuracy: 88.4300\n",
            "epoch 123/200:\n",
            "iteration: 100/500, train_loss: 0.1324, train_accuracy: 95.3100\n",
            "iteration: 200/500, train_loss: 0.1302, train_accuracy: 95.3600\n",
            "iteration: 300/500, train_loss: 0.1290, train_accuracy: 95.3767\n",
            "iteration: 400/500, train_loss: 0.1315, train_accuracy: 95.3100\n",
            "iteration: 500/500, train_loss: 0.1314, train_accuracy: 95.3420, test_loss: 0.4184, test_accuracy: 88.2600\n",
            "epoch 124/200:\n",
            "iteration: 100/500, train_loss: 0.1236, train_accuracy: 95.4900\n",
            "iteration: 200/500, train_loss: 0.1316, train_accuracy: 95.2900\n",
            "iteration: 300/500, train_loss: 0.1287, train_accuracy: 95.4133\n",
            "iteration: 400/500, train_loss: 0.1294, train_accuracy: 95.3950\n",
            "iteration: 500/500, train_loss: 0.1294, train_accuracy: 95.3940, test_loss: 0.4231, test_accuracy: 87.9700\n",
            "epoch 125/200:\n",
            "iteration: 100/500, train_loss: 0.1258, train_accuracy: 95.5200\n",
            "iteration: 200/500, train_loss: 0.1290, train_accuracy: 95.4000\n",
            "iteration: 300/500, train_loss: 0.1288, train_accuracy: 95.3333\n",
            "iteration: 400/500, train_loss: 0.1315, train_accuracy: 95.2975\n",
            "iteration: 500/500, train_loss: 0.1297, train_accuracy: 95.4080, test_loss: 0.4082, test_accuracy: 88.4400\n",
            "epoch 126/200:\n",
            "iteration: 100/500, train_loss: 0.1216, train_accuracy: 95.7500\n",
            "iteration: 200/500, train_loss: 0.1269, train_accuracy: 95.6050\n",
            "iteration: 300/500, train_loss: 0.1268, train_accuracy: 95.5367\n",
            "iteration: 400/500, train_loss: 0.1380, train_accuracy: 95.2925\n",
            "iteration: 500/500, train_loss: 0.1391, train_accuracy: 95.2420, test_loss: 0.4114, test_accuracy: 88.0100\n",
            "epoch 127/200:\n",
            "iteration: 100/500, train_loss: 0.1255, train_accuracy: 95.7000\n",
            "iteration: 200/500, train_loss: 0.1285, train_accuracy: 95.4850\n",
            "iteration: 300/500, train_loss: 0.1267, train_accuracy: 95.5400\n",
            "iteration: 400/500, train_loss: 0.1270, train_accuracy: 95.5675\n",
            "iteration: 500/500, train_loss: 0.1275, train_accuracy: 95.4780, test_loss: 0.4285, test_accuracy: 88.0600\n",
            "epoch 128/200:\n",
            "iteration: 100/500, train_loss: 0.1256, train_accuracy: 95.6600\n",
            "iteration: 200/500, train_loss: 0.1298, train_accuracy: 95.5500\n",
            "iteration: 300/500, train_loss: 0.1286, train_accuracy: 95.5533\n",
            "iteration: 400/500, train_loss: 0.1284, train_accuracy: 95.5275\n",
            "iteration: 500/500, train_loss: 0.1310, train_accuracy: 95.4680, test_loss: 0.5146, test_accuracy: 85.8100\n",
            "epoch 129/200:\n",
            "iteration: 100/500, train_loss: 0.2198, train_accuracy: 92.9200\n",
            "iteration: 200/500, train_loss: 0.1858, train_accuracy: 93.7550\n",
            "iteration: 300/500, train_loss: 0.1701, train_accuracy: 94.2433\n",
            "iteration: 400/500, train_loss: 0.1616, train_accuracy: 94.4600\n",
            "iteration: 500/500, train_loss: 0.1565, train_accuracy: 94.6480, test_loss: 0.4154, test_accuracy: 87.7200\n",
            "epoch 130/200:\n",
            "iteration: 100/500, train_loss: 0.1314, train_accuracy: 95.4200\n",
            "iteration: 200/500, train_loss: 0.1326, train_accuracy: 95.3300\n",
            "iteration: 300/500, train_loss: 0.1284, train_accuracy: 95.4533\n",
            "iteration: 400/500, train_loss: 0.1297, train_accuracy: 95.4625\n",
            "iteration: 500/500, train_loss: 0.1303, train_accuracy: 95.3780, test_loss: 0.4156, test_accuracy: 88.3800\n",
            "epoch 131/200:\n",
            "iteration: 100/500, train_loss: 0.1312, train_accuracy: 95.4000\n",
            "iteration: 200/500, train_loss: 0.1275, train_accuracy: 95.4650\n",
            "iteration: 300/500, train_loss: 0.1253, train_accuracy: 95.5233\n",
            "iteration: 400/500, train_loss: 0.1250, train_accuracy: 95.5675\n",
            "iteration: 500/500, train_loss: 0.1255, train_accuracy: 95.5880, test_loss: 0.4195, test_accuracy: 87.8900\n",
            "epoch 132/200:\n",
            "iteration: 100/500, train_loss: 0.1436, train_accuracy: 95.0300\n",
            "iteration: 200/500, train_loss: 0.1375, train_accuracy: 95.2150\n",
            "iteration: 300/500, train_loss: 0.1329, train_accuracy: 95.3700\n",
            "iteration: 400/500, train_loss: 0.1322, train_accuracy: 95.4000\n",
            "iteration: 500/500, train_loss: 0.1286, train_accuracy: 95.5200, test_loss: 0.4133, test_accuracy: 88.4200\n",
            "epoch 133/200:\n",
            "iteration: 100/500, train_loss: 0.1267, train_accuracy: 95.4800\n",
            "iteration: 200/500, train_loss: 0.1287, train_accuracy: 95.4800\n",
            "iteration: 300/500, train_loss: 0.1278, train_accuracy: 95.4533\n",
            "iteration: 400/500, train_loss: 0.1282, train_accuracy: 95.5025\n",
            "iteration: 500/500, train_loss: 0.1283, train_accuracy: 95.5040, test_loss: 0.3964, test_accuracy: 88.4600\n",
            "epoch 134/200:\n",
            "iteration: 100/500, train_loss: 0.1187, train_accuracy: 95.8900\n",
            "iteration: 200/500, train_loss: 0.1209, train_accuracy: 95.6450\n",
            "iteration: 300/500, train_loss: 0.1256, train_accuracy: 95.5167\n",
            "iteration: 400/500, train_loss: 0.1284, train_accuracy: 95.3750\n",
            "iteration: 500/500, train_loss: 0.1271, train_accuracy: 95.4640, test_loss: 0.4334, test_accuracy: 88.1600\n",
            "epoch 135/200:\n",
            "iteration: 100/500, train_loss: 0.1703, train_accuracy: 94.3100\n",
            "iteration: 200/500, train_loss: 0.1587, train_accuracy: 94.5750\n",
            "iteration: 300/500, train_loss: 0.1500, train_accuracy: 94.8700\n",
            "iteration: 400/500, train_loss: 0.1474, train_accuracy: 94.9400\n",
            "iteration: 500/500, train_loss: 0.1428, train_accuracy: 95.0680, test_loss: 0.4048, test_accuracy: 88.4200\n",
            "epoch 136/200:\n",
            "iteration: 100/500, train_loss: 0.1333, train_accuracy: 95.4100\n",
            "iteration: 200/500, train_loss: 0.1257, train_accuracy: 95.6050\n",
            "iteration: 300/500, train_loss: 0.1242, train_accuracy: 95.6600\n",
            "iteration: 400/500, train_loss: 0.1247, train_accuracy: 95.5975\n",
            "iteration: 500/500, train_loss: 0.1235, train_accuracy: 95.6300, test_loss: 0.4166, test_accuracy: 88.1600\n",
            "epoch 137/200:\n",
            "iteration: 100/500, train_loss: 0.1185, train_accuracy: 95.8400\n",
            "iteration: 200/500, train_loss: 0.1194, train_accuracy: 95.7600\n",
            "iteration: 300/500, train_loss: 0.1191, train_accuracy: 95.8000\n",
            "iteration: 400/500, train_loss: 0.1195, train_accuracy: 95.7750\n",
            "iteration: 500/500, train_loss: 0.1187, train_accuracy: 95.7980, test_loss: 0.4214, test_accuracy: 88.3600\n",
            "epoch 138/200:\n",
            "iteration: 100/500, train_loss: 0.1203, train_accuracy: 95.6900\n",
            "iteration: 200/500, train_loss: 0.1188, train_accuracy: 95.8300\n",
            "iteration: 300/500, train_loss: 0.1192, train_accuracy: 95.8033\n",
            "iteration: 400/500, train_loss: 0.1221, train_accuracy: 95.7400\n",
            "iteration: 500/500, train_loss: 0.1235, train_accuracy: 95.6900, test_loss: 0.4193, test_accuracy: 88.2100\n",
            "epoch 139/200:\n",
            "iteration: 100/500, train_loss: 0.1183, train_accuracy: 95.8100\n",
            "iteration: 200/500, train_loss: 0.1165, train_accuracy: 95.7450\n",
            "iteration: 300/500, train_loss: 0.1167, train_accuracy: 95.8000\n",
            "iteration: 400/500, train_loss: 0.1198, train_accuracy: 95.7275\n",
            "iteration: 500/500, train_loss: 0.1186, train_accuracy: 95.7740, test_loss: 0.4168, test_accuracy: 88.2200\n",
            "epoch 140/200:\n",
            "iteration: 100/500, train_loss: 0.1304, train_accuracy: 95.3200\n",
            "iteration: 200/500, train_loss: 0.1223, train_accuracy: 95.6050\n",
            "iteration: 300/500, train_loss: 0.1189, train_accuracy: 95.7500\n",
            "iteration: 400/500, train_loss: 0.1213, train_accuracy: 95.6850\n",
            "iteration: 500/500, train_loss: 0.1212, train_accuracy: 95.7400, test_loss: 0.4157, test_accuracy: 88.5100\n",
            "epoch 141/200:\n",
            "iteration: 100/500, train_loss: 0.1252, train_accuracy: 95.6100\n",
            "iteration: 200/500, train_loss: 0.1231, train_accuracy: 95.6850\n",
            "iteration: 300/500, train_loss: 0.1208, train_accuracy: 95.7167\n",
            "iteration: 400/500, train_loss: 0.1226, train_accuracy: 95.6550\n",
            "iteration: 500/500, train_loss: 0.1202, train_accuracy: 95.7340, test_loss: 0.4234, test_accuracy: 88.4600\n",
            "epoch 142/200:\n",
            "iteration: 100/500, train_loss: 0.1171, train_accuracy: 95.8300\n",
            "iteration: 200/500, train_loss: 0.1171, train_accuracy: 95.9450\n",
            "iteration: 300/500, train_loss: 0.1174, train_accuracy: 95.9400\n",
            "iteration: 400/500, train_loss: 0.1191, train_accuracy: 95.8100\n",
            "iteration: 500/500, train_loss: 0.1204, train_accuracy: 95.7700, test_loss: 0.4190, test_accuracy: 88.2100\n",
            "epoch 143/200:\n",
            "iteration: 100/500, train_loss: 0.1299, train_accuracy: 95.3500\n",
            "iteration: 200/500, train_loss: 0.1264, train_accuracy: 95.4300\n",
            "iteration: 300/500, train_loss: 0.1239, train_accuracy: 95.5567\n",
            "iteration: 400/500, train_loss: 0.1248, train_accuracy: 95.5450\n",
            "iteration: 500/500, train_loss: 0.1249, train_accuracy: 95.5220, test_loss: 0.4081, test_accuracy: 88.8900\n",
            "epoch 144/200:\n",
            "iteration: 100/500, train_loss: 0.1698, train_accuracy: 94.5800\n",
            "iteration: 200/500, train_loss: 0.1663, train_accuracy: 94.4050\n",
            "iteration: 300/500, train_loss: 0.1541, train_accuracy: 94.7867\n",
            "iteration: 400/500, train_loss: 0.1495, train_accuracy: 94.8350\n",
            "iteration: 500/500, train_loss: 0.1439, train_accuracy: 95.0060, test_loss: 0.4167, test_accuracy: 88.5600\n",
            "epoch 145/200:\n",
            "iteration: 100/500, train_loss: 0.1169, train_accuracy: 95.8400\n",
            "iteration: 200/500, train_loss: 0.1210, train_accuracy: 95.7600\n",
            "iteration: 300/500, train_loss: 0.1192, train_accuracy: 95.8167\n",
            "iteration: 400/500, train_loss: 0.1175, train_accuracy: 95.8775\n",
            "iteration: 500/500, train_loss: 0.1148, train_accuracy: 95.9760, test_loss: 0.4360, test_accuracy: 88.4100\n",
            "epoch 146/200:\n",
            "iteration: 100/500, train_loss: 0.1094, train_accuracy: 95.9400\n",
            "iteration: 200/500, train_loss: 0.1180, train_accuracy: 95.6400\n",
            "iteration: 300/500, train_loss: 0.1157, train_accuracy: 95.7667\n",
            "iteration: 400/500, train_loss: 0.1175, train_accuracy: 95.7550\n",
            "iteration: 500/500, train_loss: 0.1165, train_accuracy: 95.8060, test_loss: 0.4284, test_accuracy: 88.2700\n",
            "epoch 147/200:\n",
            "iteration: 100/500, train_loss: 0.1147, train_accuracy: 96.0200\n",
            "iteration: 200/500, train_loss: 0.1170, train_accuracy: 95.9500\n",
            "iteration: 300/500, train_loss: 0.1159, train_accuracy: 95.9633\n",
            "iteration: 400/500, train_loss: 0.1170, train_accuracy: 95.8750\n",
            "iteration: 500/500, train_loss: 0.1163, train_accuracy: 95.8880, test_loss: 0.4270, test_accuracy: 88.4400\n",
            "epoch 148/200:\n",
            "iteration: 100/500, train_loss: 0.1166, train_accuracy: 96.0200\n",
            "iteration: 200/500, train_loss: 0.1176, train_accuracy: 95.9500\n",
            "iteration: 300/500, train_loss: 0.1138, train_accuracy: 96.0267\n",
            "iteration: 400/500, train_loss: 0.1158, train_accuracy: 95.9600\n",
            "iteration: 500/500, train_loss: 0.1151, train_accuracy: 95.9680, test_loss: 0.4455, test_accuracy: 88.0500\n",
            "epoch 149/200:\n",
            "iteration: 100/500, train_loss: 0.1296, train_accuracy: 95.5200\n",
            "iteration: 200/500, train_loss: 0.1256, train_accuracy: 95.6550\n",
            "iteration: 300/500, train_loss: 0.1211, train_accuracy: 95.7133\n",
            "iteration: 400/500, train_loss: 0.1202, train_accuracy: 95.7550\n",
            "iteration: 500/500, train_loss: 0.1188, train_accuracy: 95.7940, test_loss: 0.4252, test_accuracy: 88.5200\n",
            "epoch 150/200:\n",
            "iteration: 100/500, train_loss: 0.1123, train_accuracy: 95.8500\n",
            "iteration: 200/500, train_loss: 0.1115, train_accuracy: 95.9500\n",
            "iteration: 300/500, train_loss: 0.1123, train_accuracy: 95.9833\n",
            "iteration: 400/500, train_loss: 0.1125, train_accuracy: 96.0325\n",
            "iteration: 500/500, train_loss: 0.1118, train_accuracy: 96.0560, test_loss: 0.4313, test_accuracy: 88.4100\n",
            "epoch 151/200:\n",
            "iteration: 100/500, train_loss: 0.1144, train_accuracy: 95.8600\n",
            "iteration: 200/500, train_loss: 0.1173, train_accuracy: 95.8500\n",
            "iteration: 300/500, train_loss: 0.1143, train_accuracy: 96.0100\n",
            "iteration: 400/500, train_loss: 0.1157, train_accuracy: 95.9800\n",
            "iteration: 500/500, train_loss: 0.1139, train_accuracy: 96.0200, test_loss: 0.4406, test_accuracy: 88.1900\n",
            "epoch 152/200:\n",
            "iteration: 100/500, train_loss: 0.1079, train_accuracy: 96.1400\n",
            "iteration: 200/500, train_loss: 0.1116, train_accuracy: 96.0550\n",
            "iteration: 300/500, train_loss: 0.1125, train_accuracy: 96.0667\n",
            "iteration: 400/500, train_loss: 0.1137, train_accuracy: 96.0050\n",
            "iteration: 500/500, train_loss: 0.1138, train_accuracy: 96.0040, test_loss: 0.4151, test_accuracy: 88.8900\n",
            "epoch 153/200:\n",
            "iteration: 100/500, train_loss: 0.1105, train_accuracy: 96.2000\n",
            "iteration: 200/500, train_loss: 0.1129, train_accuracy: 96.1350\n",
            "iteration: 300/500, train_loss: 0.1111, train_accuracy: 96.1233\n",
            "iteration: 400/500, train_loss: 0.1133, train_accuracy: 96.0400\n",
            "iteration: 500/500, train_loss: 0.1132, train_accuracy: 95.9920, test_loss: 0.4312, test_accuracy: 88.6100\n",
            "epoch 154/200:\n",
            "iteration: 100/500, train_loss: 0.1092, train_accuracy: 96.2400\n",
            "iteration: 200/500, train_loss: 0.1097, train_accuracy: 96.1300\n",
            "iteration: 300/500, train_loss: 0.1094, train_accuracy: 96.1100\n",
            "iteration: 400/500, train_loss: 0.1109, train_accuracy: 96.0325\n",
            "iteration: 500/500, train_loss: 0.1113, train_accuracy: 96.0700, test_loss: 0.4106, test_accuracy: 88.7000\n",
            "epoch 155/200:\n",
            "iteration: 100/500, train_loss: 0.1085, train_accuracy: 96.0700\n",
            "iteration: 200/500, train_loss: 0.1146, train_accuracy: 95.8100\n",
            "iteration: 300/500, train_loss: 0.1100, train_accuracy: 96.0467\n",
            "iteration: 400/500, train_loss: 0.1096, train_accuracy: 96.0550\n",
            "iteration: 500/500, train_loss: 0.1098, train_accuracy: 96.0840, test_loss: 0.4154, test_accuracy: 88.7300\n",
            "epoch 156/200:\n",
            "iteration: 100/500, train_loss: 0.1086, train_accuracy: 96.2100\n",
            "iteration: 200/500, train_loss: 0.1162, train_accuracy: 96.0000\n",
            "iteration: 300/500, train_loss: 0.1138, train_accuracy: 96.1100\n",
            "iteration: 400/500, train_loss: 0.1158, train_accuracy: 96.0075\n",
            "iteration: 500/500, train_loss: 0.1149, train_accuracy: 95.9740, test_loss: 0.4013, test_accuracy: 89.1500\n",
            "epoch 157/200:\n",
            "iteration: 100/500, train_loss: 0.1193, train_accuracy: 95.8700\n",
            "iteration: 200/500, train_loss: 0.1177, train_accuracy: 95.8800\n",
            "iteration: 300/500, train_loss: 0.1135, train_accuracy: 96.0167\n",
            "iteration: 400/500, train_loss: 0.1125, train_accuracy: 96.0400\n",
            "iteration: 500/500, train_loss: 0.1106, train_accuracy: 96.0900, test_loss: 0.4444, test_accuracy: 88.7600\n",
            "epoch 158/200:\n",
            "iteration: 100/500, train_loss: 0.1064, train_accuracy: 96.2300\n",
            "iteration: 200/500, train_loss: 0.1112, train_accuracy: 96.0850\n",
            "iteration: 300/500, train_loss: 0.1108, train_accuracy: 96.1133\n",
            "iteration: 400/500, train_loss: 0.1129, train_accuracy: 96.0550\n",
            "iteration: 500/500, train_loss: 0.1112, train_accuracy: 96.0960, test_loss: 0.4329, test_accuracy: 88.6900\n",
            "epoch 159/200:\n",
            "iteration: 100/500, train_loss: 0.1042, train_accuracy: 96.1200\n",
            "iteration: 200/500, train_loss: 0.1086, train_accuracy: 96.1050\n",
            "iteration: 300/500, train_loss: 0.1065, train_accuracy: 96.2567\n",
            "iteration: 400/500, train_loss: 0.1071, train_accuracy: 96.2400\n",
            "iteration: 500/500, train_loss: 0.1108, train_accuracy: 96.1320, test_loss: 0.4428, test_accuracy: 88.0100\n",
            "epoch 160/200:\n",
            "iteration: 100/500, train_loss: 0.1252, train_accuracy: 95.6900\n",
            "iteration: 200/500, train_loss: 0.1209, train_accuracy: 95.7550\n",
            "iteration: 300/500, train_loss: 0.1162, train_accuracy: 95.8967\n",
            "iteration: 400/500, train_loss: 0.1168, train_accuracy: 95.8900\n",
            "iteration: 500/500, train_loss: 0.1161, train_accuracy: 95.9000, test_loss: 0.4067, test_accuracy: 89.0300\n",
            "epoch 161/200:\n",
            "iteration: 100/500, train_loss: 0.1083, train_accuracy: 96.1700\n",
            "iteration: 200/500, train_loss: 0.1084, train_accuracy: 96.1800\n",
            "iteration: 300/500, train_loss: 0.1082, train_accuracy: 96.1533\n",
            "iteration: 400/500, train_loss: 0.1092, train_accuracy: 96.1150\n",
            "iteration: 500/500, train_loss: 0.1089, train_accuracy: 96.1120, test_loss: 0.4233, test_accuracy: 88.6600\n",
            "epoch 162/200:\n",
            "iteration: 100/500, train_loss: 0.1122, train_accuracy: 96.1400\n",
            "iteration: 200/500, train_loss: 0.1086, train_accuracy: 96.2100\n",
            "iteration: 300/500, train_loss: 0.1068, train_accuracy: 96.2567\n",
            "iteration: 400/500, train_loss: 0.1074, train_accuracy: 96.2700\n",
            "iteration: 500/500, train_loss: 0.1056, train_accuracy: 96.3260, test_loss: 0.4388, test_accuracy: 88.5700\n",
            "epoch 163/200:\n",
            "iteration: 100/500, train_loss: 0.1011, train_accuracy: 96.3800\n",
            "iteration: 200/500, train_loss: 0.1080, train_accuracy: 96.1350\n",
            "iteration: 300/500, train_loss: 0.1054, train_accuracy: 96.2167\n",
            "iteration: 400/500, train_loss: 0.1070, train_accuracy: 96.1425\n",
            "iteration: 500/500, train_loss: 0.1051, train_accuracy: 96.2220, test_loss: 0.4439, test_accuracy: 88.5400\n",
            "epoch 164/200:\n",
            "iteration: 100/500, train_loss: 0.0980, train_accuracy: 96.6200\n",
            "iteration: 200/500, train_loss: 0.1016, train_accuracy: 96.4050\n",
            "iteration: 300/500, train_loss: 0.1043, train_accuracy: 96.3433\n",
            "iteration: 400/500, train_loss: 0.1049, train_accuracy: 96.3625\n",
            "iteration: 500/500, train_loss: 0.1053, train_accuracy: 96.3420, test_loss: 0.4187, test_accuracy: 88.6200\n",
            "epoch 165/200:\n",
            "iteration: 100/500, train_loss: 0.0997, train_accuracy: 96.3700\n",
            "iteration: 200/500, train_loss: 0.1084, train_accuracy: 96.2550\n",
            "iteration: 300/500, train_loss: 0.1075, train_accuracy: 96.2500\n",
            "iteration: 400/500, train_loss: 0.1070, train_accuracy: 96.2575\n",
            "iteration: 500/500, train_loss: 0.1066, train_accuracy: 96.2600, test_loss: 0.4380, test_accuracy: 88.6300\n",
            "epoch 166/200:\n",
            "iteration: 100/500, train_loss: 0.1035, train_accuracy: 96.3700\n",
            "iteration: 200/500, train_loss: 0.1066, train_accuracy: 96.2750\n",
            "iteration: 300/500, train_loss: 0.1036, train_accuracy: 96.3800\n",
            "iteration: 400/500, train_loss: 0.1071, train_accuracy: 96.2400\n",
            "iteration: 500/500, train_loss: 0.1071, train_accuracy: 96.2440, test_loss: 0.4276, test_accuracy: 88.4900\n",
            "epoch 167/200:\n",
            "iteration: 100/500, train_loss: 0.1081, train_accuracy: 96.2900\n",
            "iteration: 200/500, train_loss: 0.1057, train_accuracy: 96.3850\n",
            "iteration: 300/500, train_loss: 0.1028, train_accuracy: 96.4733\n",
            "iteration: 400/500, train_loss: 0.1040, train_accuracy: 96.4400\n",
            "iteration: 500/500, train_loss: 0.1028, train_accuracy: 96.4920, test_loss: 0.4430, test_accuracy: 88.6300\n",
            "epoch 168/200:\n",
            "iteration: 100/500, train_loss: 0.1009, train_accuracy: 96.7300\n",
            "iteration: 200/500, train_loss: 0.1046, train_accuracy: 96.4700\n",
            "iteration: 300/500, train_loss: 0.1046, train_accuracy: 96.4933\n",
            "iteration: 400/500, train_loss: 0.1048, train_accuracy: 96.4325\n",
            "iteration: 500/500, train_loss: 0.1057, train_accuracy: 96.4080, test_loss: 0.4124, test_accuracy: 88.7500\n",
            "epoch 169/200:\n",
            "iteration: 100/500, train_loss: 0.1060, train_accuracy: 96.2400\n",
            "iteration: 200/500, train_loss: 0.1091, train_accuracy: 96.1300\n",
            "iteration: 300/500, train_loss: 0.1062, train_accuracy: 96.1900\n",
            "iteration: 400/500, train_loss: 0.1092, train_accuracy: 96.0675\n",
            "iteration: 500/500, train_loss: 0.1090, train_accuracy: 96.1020, test_loss: 0.5575, test_accuracy: 87.0100\n",
            "epoch 170/200:\n",
            "iteration: 100/500, train_loss: 0.1263, train_accuracy: 95.5300\n",
            "iteration: 200/500, train_loss: 0.1174, train_accuracy: 95.8350\n",
            "iteration: 300/500, train_loss: 0.1129, train_accuracy: 96.0100\n",
            "iteration: 400/500, train_loss: 0.1095, train_accuracy: 96.1125\n",
            "iteration: 500/500, train_loss: 0.1072, train_accuracy: 96.1860, test_loss: 0.4274, test_accuracy: 89.1400\n",
            "epoch 171/200:\n",
            "iteration: 100/500, train_loss: 0.1057, train_accuracy: 96.5200\n",
            "iteration: 200/500, train_loss: 0.1066, train_accuracy: 96.3650\n",
            "iteration: 300/500, train_loss: 0.1043, train_accuracy: 96.3467\n",
            "iteration: 400/500, train_loss: 0.1029, train_accuracy: 96.4100\n",
            "iteration: 500/500, train_loss: 0.1025, train_accuracy: 96.4020, test_loss: 0.4213, test_accuracy: 88.6100\n",
            "epoch 172/200:\n",
            "iteration: 100/500, train_loss: 0.1033, train_accuracy: 96.4000\n",
            "iteration: 200/500, train_loss: 0.1006, train_accuracy: 96.5250\n",
            "iteration: 300/500, train_loss: 0.1023, train_accuracy: 96.4033\n",
            "iteration: 400/500, train_loss: 0.1031, train_accuracy: 96.3875\n",
            "iteration: 500/500, train_loss: 0.1017, train_accuracy: 96.4400, test_loss: 0.4154, test_accuracy: 89.1200\n",
            "epoch 173/200:\n",
            "iteration: 100/500, train_loss: 0.1031, train_accuracy: 96.2600\n",
            "iteration: 200/500, train_loss: 0.1005, train_accuracy: 96.4050\n",
            "iteration: 300/500, train_loss: 0.0991, train_accuracy: 96.4967\n",
            "iteration: 400/500, train_loss: 0.1044, train_accuracy: 96.2950\n",
            "iteration: 500/500, train_loss: 0.1048, train_accuracy: 96.2800, test_loss: 0.4397, test_accuracy: 88.5600\n",
            "epoch 174/200:\n",
            "iteration: 100/500, train_loss: 0.1092, train_accuracy: 96.2900\n",
            "iteration: 200/500, train_loss: 0.1029, train_accuracy: 96.4000\n",
            "iteration: 300/500, train_loss: 0.1000, train_accuracy: 96.5233\n",
            "iteration: 400/500, train_loss: 0.1006, train_accuracy: 96.5700\n",
            "iteration: 500/500, train_loss: 0.0996, train_accuracy: 96.6120, test_loss: 0.4172, test_accuracy: 88.8100\n",
            "epoch 175/200:\n",
            "iteration: 100/500, train_loss: 0.0977, train_accuracy: 96.5400\n",
            "iteration: 200/500, train_loss: 0.1002, train_accuracy: 96.4350\n",
            "iteration: 300/500, train_loss: 0.0979, train_accuracy: 96.4733\n",
            "iteration: 400/500, train_loss: 0.0994, train_accuracy: 96.4675\n",
            "iteration: 500/500, train_loss: 0.1002, train_accuracy: 96.4460, test_loss: 0.4236, test_accuracy: 88.9600\n",
            "epoch 176/200:\n",
            "iteration: 100/500, train_loss: 0.1469, train_accuracy: 95.4000\n",
            "iteration: 200/500, train_loss: 0.1412, train_accuracy: 95.1950\n",
            "iteration: 300/500, train_loss: 0.1326, train_accuracy: 95.4667\n",
            "iteration: 400/500, train_loss: 0.1429, train_accuracy: 95.2250\n",
            "iteration: 500/500, train_loss: 0.1445, train_accuracy: 95.1780, test_loss: 0.4524, test_accuracy: 87.6900\n",
            "epoch 177/200:\n",
            "iteration: 100/500, train_loss: 0.1342, train_accuracy: 95.2900\n",
            "iteration: 200/500, train_loss: 0.1271, train_accuracy: 95.5350\n",
            "iteration: 300/500, train_loss: 0.1178, train_accuracy: 95.8800\n",
            "iteration: 400/500, train_loss: 0.1159, train_accuracy: 95.9050\n",
            "iteration: 500/500, train_loss: 0.1131, train_accuracy: 96.0180, test_loss: 0.4180, test_accuracy: 88.9500\n",
            "epoch 178/200:\n",
            "iteration: 100/500, train_loss: 0.1017, train_accuracy: 96.5200\n",
            "iteration: 200/500, train_loss: 0.1028, train_accuracy: 96.4100\n",
            "iteration: 300/500, train_loss: 0.0968, train_accuracy: 96.6133\n",
            "iteration: 400/500, train_loss: 0.0988, train_accuracy: 96.5500\n",
            "iteration: 500/500, train_loss: 0.0987, train_accuracy: 96.5740, test_loss: 0.4266, test_accuracy: 88.9400\n",
            "epoch 179/200:\n",
            "iteration: 100/500, train_loss: 0.0952, train_accuracy: 96.5500\n",
            "iteration: 200/500, train_loss: 0.0949, train_accuracy: 96.5850\n",
            "iteration: 300/500, train_loss: 0.0965, train_accuracy: 96.5433\n",
            "iteration: 400/500, train_loss: 0.0974, train_accuracy: 96.5300\n",
            "iteration: 500/500, train_loss: 0.0969, train_accuracy: 96.5280, test_loss: 0.4273, test_accuracy: 88.8400\n",
            "epoch 180/200:\n",
            "iteration: 100/500, train_loss: 0.1019, train_accuracy: 96.3600\n",
            "iteration: 200/500, train_loss: 0.0983, train_accuracy: 96.5150\n",
            "iteration: 300/500, train_loss: 0.0962, train_accuracy: 96.5500\n",
            "iteration: 400/500, train_loss: 0.0975, train_accuracy: 96.5100\n",
            "iteration: 500/500, train_loss: 0.1037, train_accuracy: 96.3100, test_loss: 0.4262, test_accuracy: 88.1300\n",
            "epoch 181/200:\n",
            "iteration: 100/500, train_loss: 0.1083, train_accuracy: 96.0800\n",
            "iteration: 200/500, train_loss: 0.1084, train_accuracy: 96.1450\n",
            "iteration: 300/500, train_loss: 0.1060, train_accuracy: 96.3000\n",
            "iteration: 400/500, train_loss: 0.1044, train_accuracy: 96.3775\n",
            "iteration: 500/500, train_loss: 0.1047, train_accuracy: 96.3660, test_loss: 0.4286, test_accuracy: 88.5900\n",
            "epoch 182/200:\n",
            "iteration: 100/500, train_loss: 0.0956, train_accuracy: 96.6400\n",
            "iteration: 200/500, train_loss: 0.0964, train_accuracy: 96.6450\n",
            "iteration: 300/500, train_loss: 0.0941, train_accuracy: 96.6800\n",
            "iteration: 400/500, train_loss: 0.0950, train_accuracy: 96.6850\n",
            "iteration: 500/500, train_loss: 0.0944, train_accuracy: 96.6780, test_loss: 0.4346, test_accuracy: 88.9800\n",
            "epoch 183/200:\n",
            "iteration: 100/500, train_loss: 0.0947, train_accuracy: 96.7100\n",
            "iteration: 200/500, train_loss: 0.0946, train_accuracy: 96.6700\n",
            "iteration: 300/500, train_loss: 0.0928, train_accuracy: 96.7433\n",
            "iteration: 400/500, train_loss: 0.0946, train_accuracy: 96.6600\n",
            "iteration: 500/500, train_loss: 0.0943, train_accuracy: 96.6680, test_loss: 0.4508, test_accuracy: 88.5600\n",
            "epoch 184/200:\n",
            "iteration: 100/500, train_loss: 0.0979, train_accuracy: 96.6000\n",
            "iteration: 200/500, train_loss: 0.0975, train_accuracy: 96.5200\n",
            "iteration: 300/500, train_loss: 0.0981, train_accuracy: 96.5467\n",
            "iteration: 400/500, train_loss: 0.0986, train_accuracy: 96.4950\n",
            "iteration: 500/500, train_loss: 0.0968, train_accuracy: 96.5640, test_loss: 0.4541, test_accuracy: 88.7600\n",
            "epoch 185/200:\n",
            "iteration: 100/500, train_loss: 0.0991, train_accuracy: 96.5900\n",
            "iteration: 200/500, train_loss: 0.0989, train_accuracy: 96.5250\n",
            "iteration: 300/500, train_loss: 0.0950, train_accuracy: 96.6367\n",
            "iteration: 400/500, train_loss: 0.0960, train_accuracy: 96.6075\n",
            "iteration: 500/500, train_loss: 0.0962, train_accuracy: 96.6020, test_loss: 0.4416, test_accuracy: 88.8100\n",
            "epoch 186/200:\n",
            "iteration: 100/500, train_loss: 0.0963, train_accuracy: 96.5300\n",
            "iteration: 200/500, train_loss: 0.0976, train_accuracy: 96.4700\n",
            "iteration: 300/500, train_loss: 0.0949, train_accuracy: 96.5933\n",
            "iteration: 400/500, train_loss: 0.0938, train_accuracy: 96.6250\n",
            "iteration: 500/500, train_loss: 0.0941, train_accuracy: 96.6460, test_loss: 0.4332, test_accuracy: 89.3400\n",
            "epoch 187/200:\n",
            "iteration: 100/500, train_loss: 0.1004, train_accuracy: 96.5100\n",
            "iteration: 200/500, train_loss: 0.0987, train_accuracy: 96.5150\n",
            "iteration: 300/500, train_loss: 0.0953, train_accuracy: 96.6067\n",
            "iteration: 400/500, train_loss: 0.0970, train_accuracy: 96.5550\n",
            "iteration: 500/500, train_loss: 0.0977, train_accuracy: 96.5120, test_loss: 0.4210, test_accuracy: 88.8200\n",
            "epoch 188/200:\n",
            "iteration: 100/500, train_loss: 0.0998, train_accuracy: 96.5100\n",
            "iteration: 200/500, train_loss: 0.1028, train_accuracy: 96.3850\n",
            "iteration: 300/500, train_loss: 0.0995, train_accuracy: 96.5767\n",
            "iteration: 400/500, train_loss: 0.0990, train_accuracy: 96.5900\n",
            "iteration: 500/500, train_loss: 0.1180, train_accuracy: 96.1100, test_loss: 0.4335, test_accuracy: 87.8800\n",
            "epoch 189/200:\n",
            "iteration: 100/500, train_loss: 0.1378, train_accuracy: 95.6900\n",
            "iteration: 200/500, train_loss: 0.1300, train_accuracy: 95.6550\n",
            "iteration: 300/500, train_loss: 0.1237, train_accuracy: 95.8267\n",
            "iteration: 400/500, train_loss: 0.1246, train_accuracy: 95.8550\n",
            "iteration: 500/500, train_loss: 0.1212, train_accuracy: 95.9440, test_loss: 0.4227, test_accuracy: 88.4100\n",
            "epoch 190/200:\n",
            "iteration: 100/500, train_loss: 0.0950, train_accuracy: 96.8400\n",
            "iteration: 200/500, train_loss: 0.0968, train_accuracy: 96.6700\n",
            "iteration: 300/500, train_loss: 0.0953, train_accuracy: 96.7167\n",
            "iteration: 400/500, train_loss: 0.0949, train_accuracy: 96.7325\n",
            "iteration: 500/500, train_loss: 0.0938, train_accuracy: 96.7320, test_loss: 0.4356, test_accuracy: 88.7900\n",
            "epoch 191/200:\n",
            "iteration: 100/500, train_loss: 0.0931, train_accuracy: 96.4400\n",
            "iteration: 200/500, train_loss: 0.0948, train_accuracy: 96.5350\n",
            "iteration: 300/500, train_loss: 0.0919, train_accuracy: 96.7200\n",
            "iteration: 400/500, train_loss: 0.0925, train_accuracy: 96.7425\n",
            "iteration: 500/500, train_loss: 0.0926, train_accuracy: 96.7300, test_loss: 0.4360, test_accuracy: 88.8100\n",
            "epoch 192/200:\n",
            "iteration: 100/500, train_loss: 0.0864, train_accuracy: 97.1200\n",
            "iteration: 200/500, train_loss: 0.0898, train_accuracy: 96.9550\n",
            "iteration: 300/500, train_loss: 0.0896, train_accuracy: 96.8833\n",
            "iteration: 400/500, train_loss: 0.0912, train_accuracy: 96.8075\n",
            "iteration: 500/500, train_loss: 0.0901, train_accuracy: 96.8140, test_loss: 0.4596, test_accuracy: 88.5800\n",
            "epoch 193/200:\n",
            "iteration: 100/500, train_loss: 0.0935, train_accuracy: 96.7600\n",
            "iteration: 200/500, train_loss: 0.0954, train_accuracy: 96.7000\n",
            "iteration: 300/500, train_loss: 0.0921, train_accuracy: 96.8167\n",
            "iteration: 400/500, train_loss: 0.0931, train_accuracy: 96.8325\n",
            "iteration: 500/500, train_loss: 0.0937, train_accuracy: 96.8140, test_loss: 0.4288, test_accuracy: 88.7300\n",
            "epoch 194/200:\n",
            "iteration: 100/500, train_loss: 0.0905, train_accuracy: 96.8200\n",
            "iteration: 200/500, train_loss: 0.0923, train_accuracy: 96.7150\n",
            "iteration: 300/500, train_loss: 0.0923, train_accuracy: 96.7167\n",
            "iteration: 400/500, train_loss: 0.0932, train_accuracy: 96.6850\n",
            "iteration: 500/500, train_loss: 0.0921, train_accuracy: 96.7120, test_loss: 0.4597, test_accuracy: 88.2700\n",
            "epoch 195/200:\n",
            "iteration: 100/500, train_loss: 0.0956, train_accuracy: 96.6600\n",
            "iteration: 200/500, train_loss: 0.0939, train_accuracy: 96.6450\n",
            "iteration: 300/500, train_loss: 0.0919, train_accuracy: 96.7000\n",
            "iteration: 400/500, train_loss: 0.0923, train_accuracy: 96.7225\n",
            "iteration: 500/500, train_loss: 0.0928, train_accuracy: 96.6960, test_loss: 0.4476, test_accuracy: 88.6700\n",
            "epoch 196/200:\n",
            "iteration: 100/500, train_loss: 0.0922, train_accuracy: 96.7900\n",
            "iteration: 200/500, train_loss: 0.0968, train_accuracy: 96.6350\n",
            "iteration: 300/500, train_loss: 0.0948, train_accuracy: 96.7567\n",
            "iteration: 400/500, train_loss: 0.0953, train_accuracy: 96.7400\n",
            "iteration: 500/500, train_loss: 0.0956, train_accuracy: 96.6900, test_loss: 0.4243, test_accuracy: 88.6700\n",
            "epoch 197/200:\n",
            "iteration: 100/500, train_loss: 0.0897, train_accuracy: 96.9700\n",
            "iteration: 200/500, train_loss: 0.0911, train_accuracy: 96.9600\n",
            "iteration: 300/500, train_loss: 0.0920, train_accuracy: 96.8800\n",
            "iteration: 400/500, train_loss: 0.0933, train_accuracy: 96.8025\n",
            "iteration: 500/500, train_loss: 0.0918, train_accuracy: 96.8340, test_loss: 0.4414, test_accuracy: 88.4900\n",
            "epoch 198/200:\n",
            "iteration: 100/500, train_loss: 0.0861, train_accuracy: 96.9200\n",
            "iteration: 200/500, train_loss: 0.0880, train_accuracy: 96.9150\n",
            "iteration: 300/500, train_loss: 0.0876, train_accuracy: 96.9033\n",
            "iteration: 400/500, train_loss: 0.0889, train_accuracy: 96.8875\n",
            "iteration: 500/500, train_loss: 0.0911, train_accuracy: 96.8100, test_loss: 0.4339, test_accuracy: 88.5000\n",
            "epoch 199/200:\n",
            "iteration: 100/500, train_loss: 0.0804, train_accuracy: 97.1000\n",
            "iteration: 200/500, train_loss: 0.0885, train_accuracy: 96.7350\n",
            "iteration: 300/500, train_loss: 0.0870, train_accuracy: 96.8600\n",
            "iteration: 400/500, train_loss: 0.0880, train_accuracy: 96.8500\n",
            "iteration: 500/500, train_loss: 0.0875, train_accuracy: 96.8700, test_loss: 0.4338, test_accuracy: 89.1100\n",
            "epoch 200/200:\n",
            "iteration: 100/500, train_loss: 0.0966, train_accuracy: 96.5700\n",
            "iteration: 200/500, train_loss: 0.0941, train_accuracy: 96.6800\n",
            "iteration: 300/500, train_loss: 0.0948, train_accuracy: 96.6500\n",
            "iteration: 400/500, train_loss: 0.0936, train_accuracy: 96.6950\n",
            "iteration: 500/500, train_loss: 0.0930, train_accuracy: 96.7220, test_loss: 0.4468, test_accuracy: 88.6400\n",
            "Model saved in file: /content/drive/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2ih0dBwC4to",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "210b9807-9147-4c3c-9093-2531481c57d6"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "LOG_DIR = \"/content/drive/logs/\"\n",
        "get_ipython().system_raw(\n",
        "    \"tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\"\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-17 05:41:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.226.180.131, 34.206.36.121, 34.206.130.40, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.226.180.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  20.6MB/s    in 0.2s    \n",
            "\n",
            "2018-12-17 05:41:32 (20.6 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "http://88b05e51.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbfrKEMPshry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}