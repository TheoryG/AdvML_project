{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_14&16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OXvu369Wlos_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Colab setup"
      ]
    },
    {
      "metadata": {
        "id": "0OgeTxpROCCB",
        "colab_type": "code",
        "outputId": "e2059a55-f166-4617-9800-7599fad9870c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110377 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GGC2nho7EYXL",
        "colab_type": "code",
        "outputId": "f61445cd-c9a5-4ee6-8fb3-669795cf9be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls drive/Deep_learning_project"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive’: File exists\n",
            " cache.py\t\t\t\t       ngrok-stable-linux-amd64.zip.1\n",
            " cifar-10-batches-py\t\t\t       ngrok-stable-linux-amd64.zip.2\n",
            " cifar10.py\t\t\t\t       ngrok-stable-linux-amd64.zip.3\n",
            "'CIFAR10 with logistic regression 4.0.ipynb'   ngrok-stable-linux-amd64.zip.4\n",
            " CNN_Scratch.ipynb\t\t\t       ngrok-stable-linux-amd64.zip.5\n",
            " data\t\t\t\t\t       ngrok-stable-linux-amd64.zip.6\n",
            "'Data Exploration and Preprocessing.ipynb'     ngrok-stable-linux-amd64.zip.7\n",
            " dataset.py\t\t\t\t       __pycache__\n",
            " data_utility.py\t\t\t       Untitled0.ipynb\n",
            " download.py\t\t\t\t      'VGG_14&16.ipynb'\n",
            " drive\t\t\t\t\t       VGG_19.ipynb\n",
            " GR5242_Final_Project_Report.ipynb\t       vgg_logs\n",
            " logs\t\t\t\t\t       vgg_logs14\n",
            " model\t\t\t\t\t       vgg_logs16\n",
            " ngrok\t\t\t\t\t       vgg_total\n",
            " ngrok-stable-linux-amd64.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8RoZmdgeElgC",
        "colab_type": "code",
        "outputId": "c145e94f-4b61-410b-a9f2-65c2e064a86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/Deep_learning_project"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Deep_learning_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rb2V7MERltLa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Code initial"
      ]
    },
    {
      "metadata": {
        "id": "PSYUsEWQDexx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from data_utility import *\n",
        "\n",
        "iterations      = 200\n",
        "batch_size      = 250\n",
        "total_epoch     = 164\n",
        "weight_decay    = 0.0003\n",
        "dropout_rate    = 0.5\n",
        "momentum_rate   = 0.9\n",
        "log_save_path   = './vgg_logs16'\n",
        "model_save_path = './model/'\n",
        "\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ bias_variable()\n",
        "# ├─ conv2d()           With Batch Normalization\n",
        "# ├─ max_pool()\n",
        "# └─ global_avg_pool()\n",
        "# ========================================================== #\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "def max_pool(input, k_size=1, stride=1, name=None):\n",
        "    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME',name=name)\n",
        "\n",
        "def batch_norm(input):\n",
        "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)\n",
        "\n",
        "# ========================================================== #\n",
        "# ├─ _random_crop() \n",
        "# ├─ _random_flip_leftright()\n",
        "# ├─ data_augmentation()\n",
        "# ├─ data_preprocessing()\n",
        "# └─ learning_rate_schedule()\n",
        "# ========================================================== #\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "        oshape = np.shape(batch[0])\n",
        "        \n",
        "        if padding:\n",
        "            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
        "        new_batch = []\n",
        "        npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "        for i in range(len(batch)):\n",
        "            new_batch.append(batch[i])\n",
        "            if padding:\n",
        "                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                          mode='constant', constant_values=0)\n",
        "            nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "            nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                                        nw:nw + crop_shape[1]]\n",
        "        return new_batch\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "        for i in range(len(batch)):\n",
        "            if bool(random.getrandbits(1)):\n",
        "                batch[i] = np.fliplr(batch[i])\n",
        "        return batch\n",
        "\n",
        "def data_preprocessing(x_train,x_test):\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    x_train[:,:,:,0] = (x_train[:,:,:,0] - np.mean(x_train[:,:,:,0])) / np.std(x_train[:,:,:,0])\n",
        "    x_train[:,:,:,1] = (x_train[:,:,:,1] - np.mean(x_train[:,:,:,1])) / np.std(x_train[:,:,:,1])\n",
        "    x_train[:,:,:,2] = (x_train[:,:,:,2] - np.mean(x_train[:,:,:,2])) / np.std(x_train[:,:,:,2])\n",
        "\n",
        "    x_test[:,:,:,0] = (x_test[:,:,:,0] - np.mean(x_test[:,:,:,0])) / np.std(x_test[:,:,:,0])\n",
        "    x_test[:,:,:,1] = (x_test[:,:,:,1] - np.mean(x_test[:,:,:,1])) / np.std(x_test[:,:,:,1])\n",
        "    x_test[:,:,:,2] = (x_test[:,:,:,2] - np.mean(x_test[:,:,:,2])) / np.std(x_test[:,:,:,2])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "def learning_rate_schedule(epoch_num):\n",
        "      if epoch_num < 81:\n",
        "        return 0.1\n",
        "      elif epoch_num < 121:\n",
        "        return 0.01\n",
        "      else:\n",
        "        return 0.001\n",
        "\n",
        "def data_augmentation(batch):\n",
        "    batch = _random_flip_leftright(batch)\n",
        "    batch = _random_crop(batch, [32,32], 4)\n",
        "    return batch\n",
        "\n",
        "def run_testing(sess,ep):\n",
        "    acc = 0.0\n",
        "    loss = 0.0\n",
        "    pre_index = 0\n",
        "    add = 1000\n",
        "    for it in range(10):\n",
        "        batch_x = test_x[pre_index:pre_index+add]\n",
        "        batch_y = test_y[pre_index:pre_index+add]\n",
        "        pre_index = pre_index + add\n",
        "        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False})\n",
        "        loss += loss_ / 10.0\n",
        "        acc += acc_ / 10.0\n",
        "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=loss), \n",
        "                            tf.Summary.Value(tag=\"test_accuracy\", simple_value=acc)])\n",
        "    return acc, loss, summary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umo37IqI2gMk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##vgg14"
      ]
    },
    {
      "metadata": {
        "id": "evqLqKWgDeyC",
        "colab_type": "code",
        "outputId": "8c4cd6c3-aa7e-4d0b-aa82-a2df0b16650b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8619
        }
      },
      "cell_type": "code",
      "source": [
        "# ========================================================== #\n",
        "# ├─ main()\n",
        "# Training and Testing \n",
        "# Save train/teset loss and acc for visualization\n",
        "# Save Model in ./model\n",
        "# ========================================================== #\n",
        "tf.reset_default_graph() \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_x, train_y, test_x, test_y = prepare_data()\n",
        "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
        "\n",
        "    # define placeholder x, y_ , keep_prob, learning_rate\n",
        "    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
        "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "    learning_rate = tf.placeholder(tf.float32)\n",
        "    train_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "    # build_network\n",
        "\n",
        "    W_conv1_1 = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_1 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1) + b_conv1_1))\n",
        "    \n",
        "    W_conv1_2 = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_2 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2) + b_conv1_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool1\")\n",
        "\n",
        "    W_conv2_1 = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_1 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1) + b_conv2_1))\n",
        "    \n",
        "\n",
        "    W_conv2_2 = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_2 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2) + b_conv2_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool2\")\n",
        "\n",
        "    W_conv3_1 = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_1 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\n",
        "\n",
        "\n",
        "    W_conv3_2 = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_2 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2) + b_conv3_2))\n",
        "    \n",
        "\n",
        "    W_conv3_3 = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_3 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3) + b_conv3_3))\n",
        "    output  = max_pool(output, 2, 2, \"pool3\")\n",
        "\n",
        "    W_conv4_1 = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1) + b_conv4_1))\n",
        "\n",
        "\n",
        "    W_conv4_2 = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2) + b_conv4_2))\n",
        "\n",
        "    W_conv4_3 = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3) + b_conv4_3))\n",
        "    output  = max_pool(output, 2, 2,\"pool4\")\n",
        "\n",
        "    W_conv5_1 = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1) + b_conv5_1))\n",
        "\n",
        "    W_conv5_2 = tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2) + b_conv5_2))\n",
        "    \n",
        "    W_conv5_3 = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3) + b_conv5_3))\n",
        "    output  = max_pool(output, 2, 2, \"pool5\")\n",
        "    \n",
        "\n",
        "    # output = tf.contrib.layers.flatten(output)\n",
        "    shape=int(np.prod(output.get_shape()[1:]))\n",
        "    output = tf.reshape(output,[-1,shape])\n",
        "\n",
        "    W_fc1 = tf.get_variable('fc1', shape=[shape,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc1 = bias_variable([4096])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1) + b_fc1) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    \n",
        "    W_fc = tf.get_variable('fc2', [4096, 10])\n",
        "    b_fc = bias_variable([10])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc) + b_fc) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    \n",
        "    # output  = tf.reshape(output,[-1,10])\n",
        "\n",
        "\n",
        "\n",
        "    # loss function: cross_entropy\n",
        "    # train_step: training operation\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
        "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\n",
        "    \n",
        "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "    # initial an saver to save model\n",
        "    saver = tf.train.Saver()\n",
        "   \n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
        "\n",
        "        # epoch = 164 \n",
        "        # make sure [bath_size * iteration = data_set_number]\n",
        "\n",
        "        for ep in range(1,total_epoch+1):\n",
        "            lr = learning_rate_schedule(ep)\n",
        "            pre_index = 0\n",
        "            train_acc = 0.0\n",
        "            train_loss = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"\\nepoch %d/%d:\" %(ep,total_epoch))\n",
        "\n",
        "            for it in range(1,iterations+1):\n",
        "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
        "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
        "\n",
        "                batch_x = data_augmentation(batch_x)\n",
        "\n",
        "                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True})\n",
        "                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "\n",
        "                train_loss += batch_loss\n",
        "                train_acc  += batch_acc\n",
        "                pre_index  += batch_size\n",
        "\n",
        "                if it == iterations:\n",
        "                    train_loss /= iterations\n",
        "                    train_acc /= iterations\n",
        "\n",
        "                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
        "                                          tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
        "\n",
        "                    val_acc, val_loss, test_summary = run_testing(sess,ep)\n",
        "\n",
        "                    summary_writer.add_summary(train_summary, ep)\n",
        "                    summary_writer.add_summary(test_summary, ep)\n",
        "                    summary_writer.flush()\n",
        "\n",
        "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
        "                else:\n",
        "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\" %(it, iterations, train_loss / it, train_acc / it) , end='\\r')\n",
        "\n",
        "        save_path = saver.save(sess, model_save_path)\n",
        "        print(\"Model saved in file: %s\" % save_path)  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======Loading data======\n",
            "DataSet aready exist!\n",
            "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
            "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
            "Train data: (50000, 32, 32, 3) (50000, 10)\n",
            "Test data : (10000, 32, 32, 3) (10000, 10)\n",
            "======Load finished======\n",
            "======Shuffling data======\n",
            "======Prepare Finished======\n",
            "\n",
            "epoch 1/164:\n",
            "iteration: 200/200, cost_time: 75s, train_loss: 2.2411, train_acc: 0.2248, test_loss: 2.0797, test_acc: 0.3192\n",
            "\n",
            "epoch 2/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 2.0213, train_acc: 0.4559, test_loss: 1.6975, test_acc: 0.5204\n",
            "\n",
            "epoch 3/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.8550, train_acc: 0.6068, test_loss: 1.4860, test_acc: 0.6093\n",
            "\n",
            "epoch 4/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.7514, train_acc: 0.6909, test_loss: 1.3134, test_acc: 0.6953\n",
            "\n",
            "epoch 5/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.6626, train_acc: 0.7498, test_loss: 1.1960, test_acc: 0.7155\n",
            "\n",
            "epoch 6/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.6150, train_acc: 0.7836, test_loss: 1.0813, test_acc: 0.7682\n",
            "\n",
            "epoch 7/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.5802, train_acc: 0.8049, test_loss: 1.0641, test_acc: 0.7629\n",
            "\n",
            "epoch 8/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.5544, train_acc: 0.8247, test_loss: 0.9804, test_acc: 0.7726\n",
            "\n",
            "epoch 9/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.5171, train_acc: 0.8414, test_loss: 0.9936, test_acc: 0.7874\n",
            "\n",
            "epoch 10/164:\n",
            "iteration: 200/200, cost_time: 71s, train_loss: 1.5066, train_acc: 0.8539, test_loss: 0.9937, test_acc: 0.7793\n",
            "\n",
            "epoch 11/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4740, train_acc: 0.8657, test_loss: 0.9094, test_acc: 0.8136\n",
            "\n",
            "epoch 12/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4713, train_acc: 0.8711, test_loss: 0.8709, test_acc: 0.8157\n",
            "\n",
            "epoch 13/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4570, train_acc: 0.8798, test_loss: 0.8932, test_acc: 0.8126\n",
            "\n",
            "epoch 14/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4442, train_acc: 0.8852, test_loss: 0.8647, test_acc: 0.8211\n",
            "\n",
            "epoch 15/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4502, train_acc: 0.8928, test_loss: 0.8957, test_acc: 0.8092\n",
            "\n",
            "epoch 16/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4371, train_acc: 0.8946, test_loss: 0.8354, test_acc: 0.8232\n",
            "\n",
            "epoch 17/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4308, train_acc: 0.8993, test_loss: 0.8514, test_acc: 0.8246\n",
            "\n",
            "epoch 18/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4239, train_acc: 0.9045, test_loss: 0.8408, test_acc: 0.8222\n",
            "\n",
            "epoch 19/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4208, train_acc: 0.9074, test_loss: 0.8056, test_acc: 0.8316\n",
            "\n",
            "epoch 20/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4038, train_acc: 0.9118, test_loss: 0.8082, test_acc: 0.8285\n",
            "\n",
            "epoch 21/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4001, train_acc: 0.9137, test_loss: 0.8042, test_acc: 0.8298\n",
            "\n",
            "epoch 22/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.4005, train_acc: 0.9145, test_loss: 0.7929, test_acc: 0.8417\n",
            "\n",
            "epoch 23/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3941, train_acc: 0.9174, test_loss: 0.7250, test_acc: 0.8542\n",
            "\n",
            "epoch 24/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3850, train_acc: 0.9231, test_loss: 0.7614, test_acc: 0.8478\n",
            "\n",
            "epoch 25/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3897, train_acc: 0.9219, test_loss: 0.7568, test_acc: 0.8418\n",
            "\n",
            "epoch 26/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3899, train_acc: 0.9230, test_loss: 0.7527, test_acc: 0.8488\n",
            "\n",
            "epoch 27/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3834, train_acc: 0.9269, test_loss: 0.7720, test_acc: 0.8330\n",
            "\n",
            "epoch 28/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3813, train_acc: 0.9254, test_loss: 0.7401, test_acc: 0.8606\n",
            "\n",
            "epoch 29/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3860, train_acc: 0.9267, test_loss: 0.7125, test_acc: 0.8671\n",
            "\n",
            "epoch 30/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3700, train_acc: 0.9306, test_loss: 0.7120, test_acc: 0.8609\n",
            "\n",
            "epoch 31/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3621, train_acc: 0.9321, test_loss: 0.7443, test_acc: 0.8501\n",
            "\n",
            "epoch 32/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3701, train_acc: 0.9323, test_loss: 0.7562, test_acc: 0.8436\n",
            "\n",
            "epoch 33/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3782, train_acc: 0.9317, test_loss: 0.7251, test_acc: 0.8544\n",
            "\n",
            "epoch 34/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3625, train_acc: 0.9334, test_loss: 0.7460, test_acc: 0.8442\n",
            "\n",
            "epoch 35/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3659, train_acc: 0.9347, test_loss: 0.7824, test_acc: 0.8287\n",
            "\n",
            "epoch 36/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3651, train_acc: 0.9353, test_loss: 0.7454, test_acc: 0.8416\n",
            "\n",
            "epoch 37/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3623, train_acc: 0.9364, test_loss: 0.7367, test_acc: 0.8557\n",
            "\n",
            "epoch 38/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3536, train_acc: 0.9396, test_loss: 0.7819, test_acc: 0.8269\n",
            "\n",
            "epoch 39/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3502, train_acc: 0.9393, test_loss: 0.7148, test_acc: 0.8455\n",
            "\n",
            "epoch 40/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3470, train_acc: 0.9391, test_loss: 0.7548, test_acc: 0.8366\n",
            "\n",
            "epoch 41/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3598, train_acc: 0.9382, test_loss: 0.6886, test_acc: 0.8636\n",
            "\n",
            "epoch 42/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3485, train_acc: 0.9405, test_loss: 0.6995, test_acc: 0.8600\n",
            "\n",
            "epoch 43/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3415, train_acc: 0.9412, test_loss: 0.6798, test_acc: 0.8663\n",
            "\n",
            "epoch 44/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3490, train_acc: 0.9405, test_loss: 0.6780, test_acc: 0.8618\n",
            "\n",
            "epoch 45/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3385, train_acc: 0.9435, test_loss: 0.7002, test_acc: 0.8589\n",
            "\n",
            "epoch 46/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3483, train_acc: 0.9421, test_loss: 0.7112, test_acc: 0.8544\n",
            "\n",
            "epoch 47/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3385, train_acc: 0.9431, test_loss: 0.6977, test_acc: 0.8592\n",
            "\n",
            "epoch 48/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3420, train_acc: 0.9440, test_loss: 0.6684, test_acc: 0.8649\n",
            "\n",
            "epoch 49/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3329, train_acc: 0.9452, test_loss: 0.6719, test_acc: 0.8651\n",
            "\n",
            "epoch 50/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3404, train_acc: 0.9452, test_loss: 0.6767, test_acc: 0.8602\n",
            "\n",
            "epoch 51/164:\n",
            "iteration: 200/200, cost_time: 73s, train_loss: 1.3466, train_acc: 0.9440, test_loss: 0.6735, test_acc: 0.8624\n",
            "\n",
            "epoch 52/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3379, train_acc: 0.9458, test_loss: 0.7153, test_acc: 0.8568\n",
            "\n",
            "epoch 53/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3359, train_acc: 0.9450, test_loss: 0.6703, test_acc: 0.8634\n",
            "\n",
            "epoch 54/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3376, train_acc: 0.9445, test_loss: 0.6667, test_acc: 0.8643\n",
            "\n",
            "epoch 55/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3426, train_acc: 0.9470, test_loss: 0.6547, test_acc: 0.8688\n",
            "\n",
            "epoch 56/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3411, train_acc: 0.9437, test_loss: 0.6258, test_acc: 0.8742\n",
            "\n",
            "epoch 57/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3468, train_acc: 0.9455, test_loss: 0.6758, test_acc: 0.8652\n",
            "\n",
            "epoch 58/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3447, train_acc: 0.9440, test_loss: 0.6481, test_acc: 0.8719\n",
            "\n",
            "epoch 59/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3213, train_acc: 0.9495, test_loss: 0.6377, test_acc: 0.8742\n",
            "\n",
            "epoch 60/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3387, train_acc: 0.9458, test_loss: 0.6413, test_acc: 0.8742\n",
            "\n",
            "epoch 61/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3315, train_acc: 0.9471, test_loss: 0.6328, test_acc: 0.8696\n",
            "\n",
            "epoch 62/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3339, train_acc: 0.9457, test_loss: 0.7044, test_acc: 0.8470\n",
            "\n",
            "epoch 63/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3339, train_acc: 0.9460, test_loss: 0.6722, test_acc: 0.8589\n",
            "\n",
            "epoch 64/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3273, train_acc: 0.9484, test_loss: 0.7005, test_acc: 0.8454\n",
            "\n",
            "epoch 65/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3351, train_acc: 0.9469, test_loss: 0.6441, test_acc: 0.8680\n",
            "\n",
            "epoch 66/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3294, train_acc: 0.9491, test_loss: 0.6576, test_acc: 0.8622\n",
            "\n",
            "epoch 67/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3344, train_acc: 0.9466, test_loss: 0.6252, test_acc: 0.8764\n",
            "\n",
            "epoch 68/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3363, train_acc: 0.9472, test_loss: 0.6555, test_acc: 0.8664\n",
            "\n",
            "epoch 69/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3200, train_acc: 0.9497, test_loss: 0.6401, test_acc: 0.8671\n",
            "\n",
            "epoch 70/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3213, train_acc: 0.9503, test_loss: 0.6658, test_acc: 0.8586\n",
            "\n",
            "epoch 71/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3293, train_acc: 0.9476, test_loss: 0.6190, test_acc: 0.8806\n",
            "\n",
            "epoch 72/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3358, train_acc: 0.9499, test_loss: 0.7040, test_acc: 0.8575\n",
            "\n",
            "epoch 73/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3266, train_acc: 0.9505, test_loss: 0.6597, test_acc: 0.8636\n",
            "\n",
            "epoch 74/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3314, train_acc: 0.9502, test_loss: 0.6312, test_acc: 0.8682\n",
            "\n",
            "epoch 75/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3269, train_acc: 0.9506, test_loss: 0.6555, test_acc: 0.8685\n",
            "\n",
            "epoch 76/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3299, train_acc: 0.9503, test_loss: 0.6702, test_acc: 0.8627\n",
            "\n",
            "epoch 77/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3297, train_acc: 0.9523, test_loss: 0.6879, test_acc: 0.8506\n",
            "\n",
            "epoch 78/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3299, train_acc: 0.9513, test_loss: 0.6561, test_acc: 0.8615\n",
            "\n",
            "epoch 79/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3166, train_acc: 0.9503, test_loss: 0.6332, test_acc: 0.8673\n",
            "\n",
            "epoch 80/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.3324, train_acc: 0.9494, test_loss: 0.6376, test_acc: 0.8727\n",
            "\n",
            "epoch 81/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2654, train_acc: 0.9471, test_loss: 0.4972, test_acc: 0.9149\n",
            "\n",
            "epoch 82/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2325, train_acc: 0.9619, test_loss: 0.4684, test_acc: 0.9204\n",
            "\n",
            "epoch 83/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2249, train_acc: 0.9688, test_loss: 0.4467, test_acc: 0.9239\n",
            "\n",
            "epoch 84/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2323, train_acc: 0.9718, test_loss: 0.4353, test_acc: 0.9244\n",
            "\n",
            "epoch 85/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2071, train_acc: 0.9747, test_loss: 0.4239, test_acc: 0.9243\n",
            "\n",
            "epoch 86/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2241, train_acc: 0.9772, test_loss: 0.4193, test_acc: 0.9244\n",
            "\n",
            "epoch 87/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2023, train_acc: 0.9791, test_loss: 0.4072, test_acc: 0.9268\n",
            "\n",
            "epoch 88/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2107, train_acc: 0.9806, test_loss: 0.4053, test_acc: 0.9255\n",
            "\n",
            "epoch 89/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2053, train_acc: 0.9825, test_loss: 0.3987, test_acc: 0.9264\n",
            "\n",
            "epoch 90/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2008, train_acc: 0.9837, test_loss: 0.3959, test_acc: 0.9260\n",
            "\n",
            "epoch 91/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.2065, train_acc: 0.9844, test_loss: 0.3908, test_acc: 0.9276\n",
            "\n",
            "epoch 92/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1981, train_acc: 0.9850, test_loss: 0.3912, test_acc: 0.9276\n",
            "\n",
            "epoch 93/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1889, train_acc: 0.9866, test_loss: 0.3892, test_acc: 0.9267\n",
            "\n",
            "epoch 94/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1897, train_acc: 0.9870, test_loss: 0.3890, test_acc: 0.9262\n",
            "\n",
            "epoch 95/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1902, train_acc: 0.9878, test_loss: 0.3816, test_acc: 0.9260\n",
            "\n",
            "epoch 96/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1875, train_acc: 0.9886, test_loss: 0.3769, test_acc: 0.9280\n",
            "\n",
            "epoch 97/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1962, train_acc: 0.9894, test_loss: 0.3783, test_acc: 0.9291\n",
            "\n",
            "epoch 98/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1946, train_acc: 0.9895, test_loss: 0.3748, test_acc: 0.9276\n",
            "\n",
            "epoch 99/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1908, train_acc: 0.9892, test_loss: 0.3785, test_acc: 0.9273\n",
            "\n",
            "epoch 100/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1842, train_acc: 0.9898, test_loss: 0.3723, test_acc: 0.9288\n",
            "\n",
            "epoch 101/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1742, train_acc: 0.9905, test_loss: 0.3749, test_acc: 0.9287\n",
            "\n",
            "epoch 102/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1807, train_acc: 0.9920, test_loss: 0.3730, test_acc: 0.9257\n",
            "\n",
            "epoch 103/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1886, train_acc: 0.9913, test_loss: 0.3694, test_acc: 0.9285\n",
            "\n",
            "epoch 104/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1734, train_acc: 0.9923, test_loss: 0.3705, test_acc: 0.9267\n",
            "\n",
            "epoch 105/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1754, train_acc: 0.9923, test_loss: 0.3732, test_acc: 0.9258\n",
            "\n",
            "epoch 106/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1772, train_acc: 0.9921, test_loss: 0.3714, test_acc: 0.9276\n",
            "\n",
            "epoch 107/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1854, train_acc: 0.9923, test_loss: 0.3736, test_acc: 0.9258\n",
            "\n",
            "epoch 108/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1821, train_acc: 0.9931, test_loss: 0.3717, test_acc: 0.9267\n",
            "\n",
            "epoch 109/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1764, train_acc: 0.9934, test_loss: 0.3627, test_acc: 0.9286\n",
            "\n",
            "epoch 110/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1773, train_acc: 0.9937, test_loss: 0.3670, test_acc: 0.9280\n",
            "\n",
            "epoch 111/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1810, train_acc: 0.9932, test_loss: 0.3733, test_acc: 0.9255\n",
            "\n",
            "epoch 112/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1778, train_acc: 0.9935, test_loss: 0.3717, test_acc: 0.9249\n",
            "\n",
            "epoch 113/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1734, train_acc: 0.9937, test_loss: 0.3629, test_acc: 0.9278\n",
            "\n",
            "epoch 114/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1738, train_acc: 0.9941, test_loss: 0.3644, test_acc: 0.9275\n",
            "\n",
            "epoch 115/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1770, train_acc: 0.9933, test_loss: 0.3734, test_acc: 0.9250\n",
            "\n",
            "epoch 116/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1772, train_acc: 0.9943, test_loss: 0.3727, test_acc: 0.9266\n",
            "\n",
            "epoch 117/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1828, train_acc: 0.9936, test_loss: 0.3737, test_acc: 0.9252\n",
            "\n",
            "epoch 118/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1679, train_acc: 0.9943, test_loss: 0.3664, test_acc: 0.9295\n",
            "\n",
            "epoch 119/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1740, train_acc: 0.9943, test_loss: 0.3703, test_acc: 0.9299\n",
            "\n",
            "epoch 120/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1788, train_acc: 0.9943, test_loss: 0.3709, test_acc: 0.9267\n",
            "\n",
            "epoch 121/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1765, train_acc: 0.9928, test_loss: 0.3637, test_acc: 0.9297\n",
            "\n",
            "epoch 122/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1709, train_acc: 0.9931, test_loss: 0.3605, test_acc: 0.9296\n",
            "\n",
            "epoch 123/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1674, train_acc: 0.9945, test_loss: 0.3605, test_acc: 0.9298\n",
            "\n",
            "epoch 124/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1787, train_acc: 0.9945, test_loss: 0.3594, test_acc: 0.9297\n",
            "\n",
            "epoch 125/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1575, train_acc: 0.9949, test_loss: 0.3583, test_acc: 0.9291\n",
            "\n",
            "epoch 126/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1627, train_acc: 0.9951, test_loss: 0.3561, test_acc: 0.9305\n",
            "\n",
            "epoch 127/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1644, train_acc: 0.9952, test_loss: 0.3560, test_acc: 0.9307\n",
            "\n",
            "epoch 128/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1607, train_acc: 0.9951, test_loss: 0.3548, test_acc: 0.9314\n",
            "\n",
            "epoch 129/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1707, train_acc: 0.9957, test_loss: 0.3529, test_acc: 0.9310\n",
            "\n",
            "epoch 130/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1551, train_acc: 0.9954, test_loss: 0.3534, test_acc: 0.9313\n",
            "\n",
            "epoch 131/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1606, train_acc: 0.9959, test_loss: 0.3541, test_acc: 0.9309\n",
            "\n",
            "epoch 132/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1541, train_acc: 0.9961, test_loss: 0.3546, test_acc: 0.9309\n",
            "\n",
            "epoch 133/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1665, train_acc: 0.9963, test_loss: 0.3520, test_acc: 0.9315\n",
            "\n",
            "epoch 134/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1574, train_acc: 0.9959, test_loss: 0.3514, test_acc: 0.9317\n",
            "\n",
            "epoch 135/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1671, train_acc: 0.9960, test_loss: 0.3514, test_acc: 0.9317\n",
            "\n",
            "epoch 136/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1762, train_acc: 0.9960, test_loss: 0.3509, test_acc: 0.9316\n",
            "\n",
            "epoch 137/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1581, train_acc: 0.9960, test_loss: 0.3510, test_acc: 0.9318\n",
            "\n",
            "epoch 138/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1643, train_acc: 0.9963, test_loss: 0.3523, test_acc: 0.9326\n",
            "\n",
            "epoch 139/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1586, train_acc: 0.9959, test_loss: 0.3505, test_acc: 0.9331\n",
            "\n",
            "epoch 140/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1641, train_acc: 0.9967, test_loss: 0.3527, test_acc: 0.9314\n",
            "\n",
            "epoch 141/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1574, train_acc: 0.9964, test_loss: 0.3518, test_acc: 0.9332\n",
            "\n",
            "epoch 142/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1679, train_acc: 0.9965, test_loss: 0.3499, test_acc: 0.9330\n",
            "\n",
            "epoch 143/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1637, train_acc: 0.9967, test_loss: 0.3521, test_acc: 0.9328\n",
            "\n",
            "epoch 144/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1529, train_acc: 0.9967, test_loss: 0.3518, test_acc: 0.9334\n",
            "\n",
            "epoch 145/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1571, train_acc: 0.9967, test_loss: 0.3524, test_acc: 0.9332\n",
            "\n",
            "epoch 146/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1617, train_acc: 0.9965, test_loss: 0.3529, test_acc: 0.9325\n",
            "\n",
            "epoch 147/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1619, train_acc: 0.9967, test_loss: 0.3511, test_acc: 0.9334\n",
            "\n",
            "epoch 148/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1592, train_acc: 0.9965, test_loss: 0.3515, test_acc: 0.9317\n",
            "\n",
            "epoch 149/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1543, train_acc: 0.9970, test_loss: 0.3520, test_acc: 0.9326\n",
            "\n",
            "epoch 150/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1661, train_acc: 0.9970, test_loss: 0.3532, test_acc: 0.9320\n",
            "\n",
            "epoch 151/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1604, train_acc: 0.9970, test_loss: 0.3516, test_acc: 0.9316\n",
            "\n",
            "epoch 152/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1551, train_acc: 0.9970, test_loss: 0.3513, test_acc: 0.9333\n",
            "\n",
            "epoch 153/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1655, train_acc: 0.9969, test_loss: 0.3521, test_acc: 0.9316\n",
            "\n",
            "epoch 154/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1675, train_acc: 0.9970, test_loss: 0.3512, test_acc: 0.9317\n",
            "\n",
            "epoch 155/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1609, train_acc: 0.9971, test_loss: 0.3520, test_acc: 0.9326\n",
            "\n",
            "epoch 156/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1594, train_acc: 0.9968, test_loss: 0.3522, test_acc: 0.9321\n",
            "\n",
            "epoch 157/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1594, train_acc: 0.9971, test_loss: 0.3516, test_acc: 0.9320\n",
            "\n",
            "epoch 158/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1629, train_acc: 0.9972, test_loss: 0.3525, test_acc: 0.9317\n",
            "\n",
            "epoch 159/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1590, train_acc: 0.9974, test_loss: 0.3504, test_acc: 0.9315\n",
            "\n",
            "epoch 160/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1631, train_acc: 0.9973, test_loss: 0.3512, test_acc: 0.9307\n",
            "\n",
            "epoch 161/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1604, train_acc: 0.9971, test_loss: 0.3507, test_acc: 0.9317\n",
            "\n",
            "epoch 162/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1656, train_acc: 0.9974, test_loss: 0.3507, test_acc: 0.9324\n",
            "\n",
            "epoch 163/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1692, train_acc: 0.9975, test_loss: 0.3506, test_acc: 0.9322\n",
            "\n",
            "epoch 164/164:\n",
            "iteration: 200/200, cost_time: 72s, train_loss: 1.1728, train_acc: 0.9971, test_loss: 0.3514, test_acc: 0.9319\n",
            "Model saved in file: ./model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KEiuuU5I3HLg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##vgg16"
      ]
    },
    {
      "metadata": {
        "id": "yDh92Zef3KCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8755
        },
        "outputId": "463ec1f7-3bfa-451f-e9f6-3cb60625dab9"
      },
      "cell_type": "code",
      "source": [
        "log_save_path   = './vgg_logs16'\n",
        "model_save_path = './model/'\n",
        "# ========================================================== #\n",
        "# ├─ main()\n",
        "# Training and Testing \n",
        "# Save train/teset loss and acc for visualization\n",
        "# Save Model in ./model\n",
        "# ========================================================== #\n",
        "tf.reset_default_graph() \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_x, train_y, test_x, test_y = prepare_data()\n",
        "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
        "\n",
        "    # define placeholder x, y_ , keep_prob, learning_rate\n",
        "    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
        "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "    learning_rate = tf.placeholder(tf.float32)\n",
        "    train_flag = tf.placeholder(tf.bool)\n",
        "\n",
        "    # build_network\n",
        "\n",
        "    W_conv1_1 = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_1 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1) + b_conv1_1))\n",
        "    \n",
        "    W_conv1_2 = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv1_2 = bias_variable([64])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2) + b_conv1_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool1\")\n",
        "\n",
        "    W_conv2_1 = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_1 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1) + b_conv2_1))\n",
        "    \n",
        "\n",
        "    W_conv2_2 = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv2_2 = bias_variable([128])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2) + b_conv2_2))\n",
        "    output  = max_pool(output, 2, 2, \"pool2\")\n",
        "\n",
        "    W_conv3_1 = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_1 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\n",
        "\n",
        "\n",
        "    W_conv3_2 = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_2 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2) + b_conv3_2))\n",
        "    \n",
        "\n",
        "    W_conv3_3 = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv3_3 = bias_variable([256])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3) + b_conv3_3))\n",
        "    output  = max_pool(output, 2, 2, \"pool3\")\n",
        "\n",
        "    W_conv4_1 = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1) + b_conv4_1))\n",
        "\n",
        "\n",
        "    W_conv4_2 = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2) + b_conv4_2))\n",
        "\n",
        "    W_conv4_3 = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv4_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3) + b_conv4_3))\n",
        "    output  = max_pool(output, 2, 2,\"pool4\")\n",
        "\n",
        "    W_conv5_1 = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_1 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1) + b_conv5_1))\n",
        "\n",
        "    W_conv5_2 = tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_2 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2) + b_conv5_2))\n",
        "    \n",
        "    W_conv5_3 = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_conv5_3 = bias_variable([512])\n",
        "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3) + b_conv5_3))\n",
        "    output  = max_pool(output, 2, 2, \"pool5\")\n",
        "    \n",
        "\n",
        "    # output = tf.contrib.layers.flatten(output)\n",
        "    shape=int(np.prod(output.get_shape()[1:]))\n",
        "    output = tf.reshape(output,[-1,shape])\n",
        "\n",
        "    W_fc1 = tf.get_variable('fc1', shape=[shape,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc1 = bias_variable([4096])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1) + b_fc1) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    \n",
        "    W_fc2 = tf.get_variable('fc2', shape=[4096,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc2 = bias_variable([4096])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2) + b_fc2) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "\n",
        "\n",
        "    W_fc3 = tf.get_variable('fc3', shape=[4096,1000], initializer=tf.contrib.keras.initializers.he_normal())\n",
        "    b_fc3 = bias_variable([1000])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3) + b_fc3) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    \n",
        "    W_fc = tf.get_variable('W_fc', [1000, 10])\n",
        "    b_fc = tf.get_variable('b_fc', [10])\n",
        "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc) + b_fc) )\n",
        "    output  = tf.nn.dropout(output,keep_prob)\n",
        "    # output  = tf.reshape(output,[-1,10])\n",
        "\n",
        "\n",
        "\n",
        "    # loss function: cross_entropy\n",
        "    # train_step: training operation\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
        "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
        "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + l2 * weight_decay)\n",
        "    \n",
        "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "    # initial an saver to save model\n",
        "    saver = tf.train.Saver()\n",
        "   \n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
        "\n",
        "        # epoch = 164 \n",
        "        # make sure [bath_size * iteration = data_set_number]\n",
        "\n",
        "        for ep in range(1,total_epoch+1):\n",
        "            lr = learning_rate_schedule(ep)\n",
        "            pre_index = 0\n",
        "            train_acc = 0.0\n",
        "            train_loss = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"\\nepoch %d/%d:\" %(ep,total_epoch))\n",
        "\n",
        "            for it in range(1,iterations+1):\n",
        "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
        "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
        "\n",
        "                batch_x = data_augmentation(batch_x)\n",
        "\n",
        "                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True})\n",
        "                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "\n",
        "                train_loss += batch_loss\n",
        "                train_acc  += batch_acc\n",
        "                pre_index  += batch_size\n",
        "\n",
        "                if it == iterations:\n",
        "                    train_loss /= iterations\n",
        "                    train_acc /= iterations\n",
        "\n",
        "                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True})\n",
        "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
        "                                          tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
        "\n",
        "                    val_acc, val_loss, test_summary = run_testing(sess,ep)\n",
        "\n",
        "                    summary_writer.add_summary(train_summary, ep)\n",
        "                    summary_writer.add_summary(test_summary, ep)\n",
        "                    summary_writer.flush()\n",
        "\n",
        "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
        "                else:\n",
        "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\" %(it, iterations, train_loss / it, train_acc / it) , end='\\r')\n",
        "\n",
        "        save_path = saver.save(sess, model_save_path)\n",
        "        print(\"Model saved in file: %s\" % save_path)  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======Loading data======\n",
            "DataSet aready exist!\n",
            "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
            "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
            "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
            "Train data: (50000, 32, 32, 3) (50000, 10)\n",
            "Test data : (10000, 32, 32, 3) (10000, 10)\n",
            "======Load finished======\n",
            "======Shuffling data======\n",
            "======Prepare Finished======\n",
            "WARNING:tensorflow:From <ipython-input-10-db54704caaec>:117: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "\n",
            "epoch 1/164:\n",
            "iteration: 200/200, cost_time: 89s, train_loss: 2.3090, train_acc: 0.1097, test_loss: 2.3009, test_acc: 0.1124\n",
            "\n",
            "epoch 2/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 2.3017, train_acc: 0.1171, test_loss: 2.2983, test_acc: 0.1264\n",
            "\n",
            "epoch 3/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 2.2975, train_acc: 0.1332, test_loss: 2.2688, test_acc: 0.1407\n",
            "\n",
            "epoch 4/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 2.2655, train_acc: 0.1547, test_loss: 2.2154, test_acc: 0.1662\n",
            "\n",
            "epoch 5/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 2.2228, train_acc: 0.2104, test_loss: 2.1204, test_acc: 0.2548\n",
            "\n",
            "epoch 6/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 2.0835, train_acc: 0.3905, test_loss: 1.8378, test_acc: 0.4331\n",
            "\n",
            "epoch 7/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.9711, train_acc: 0.5110, test_loss: 1.6797, test_acc: 0.5321\n",
            "\n",
            "epoch 8/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.8497, train_acc: 0.6130, test_loss: 1.5924, test_acc: 0.5328\n",
            "\n",
            "epoch 9/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.7527, train_acc: 0.6836, test_loss: 1.3537, test_acc: 0.6390\n",
            "\n",
            "epoch 10/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.6916, train_acc: 0.7315, test_loss: 1.1864, test_acc: 0.7157\n",
            "\n",
            "epoch 11/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.6403, train_acc: 0.7672, test_loss: 1.1561, test_acc: 0.7357\n",
            "\n",
            "epoch 12/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.6020, train_acc: 0.7923, test_loss: 1.1070, test_acc: 0.7499\n",
            "\n",
            "epoch 13/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.5878, train_acc: 0.8093, test_loss: 1.0763, test_acc: 0.7492\n",
            "\n",
            "epoch 14/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.5436, train_acc: 0.8242, test_loss: 1.0274, test_acc: 0.7652\n",
            "\n",
            "epoch 15/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.5296, train_acc: 0.8400, test_loss: 0.9867, test_acc: 0.7697\n",
            "\n",
            "epoch 16/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.5057, train_acc: 0.8488, test_loss: 0.9685, test_acc: 0.7927\n",
            "\n",
            "epoch 17/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.4909, train_acc: 0.8595, test_loss: 0.9390, test_acc: 0.7917\n",
            "\n",
            "epoch 18/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.4828, train_acc: 0.8663, test_loss: 0.9027, test_acc: 0.8030\n",
            "\n",
            "epoch 19/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4798, train_acc: 0.8728, test_loss: 0.9018, test_acc: 0.8138\n",
            "\n",
            "epoch 20/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.4685, train_acc: 0.8782, test_loss: 0.8892, test_acc: 0.8155\n",
            "\n",
            "epoch 21/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4512, train_acc: 0.8855, test_loss: 0.8637, test_acc: 0.8247\n",
            "\n",
            "epoch 22/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4467, train_acc: 0.8873, test_loss: 0.8216, test_acc: 0.8356\n",
            "\n",
            "epoch 23/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.4356, train_acc: 0.8906, test_loss: 0.8351, test_acc: 0.8320\n",
            "\n",
            "epoch 24/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4406, train_acc: 0.8951, test_loss: 0.8517, test_acc: 0.8253\n",
            "\n",
            "epoch 25/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4340, train_acc: 0.8981, test_loss: 0.7972, test_acc: 0.8344\n",
            "\n",
            "epoch 26/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4300, train_acc: 0.9013, test_loss: 0.7530, test_acc: 0.8497\n",
            "\n",
            "epoch 27/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4168, train_acc: 0.9067, test_loss: 0.7613, test_acc: 0.8463\n",
            "\n",
            "epoch 28/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4184, train_acc: 0.9067, test_loss: 0.7567, test_acc: 0.8492\n",
            "\n",
            "epoch 29/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4176, train_acc: 0.9071, test_loss: 0.8045, test_acc: 0.8301\n",
            "\n",
            "epoch 30/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4054, train_acc: 0.9116, test_loss: 0.7443, test_acc: 0.8483\n",
            "\n",
            "epoch 31/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.4099, train_acc: 0.9111, test_loss: 0.7718, test_acc: 0.8479\n",
            "\n",
            "epoch 32/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3962, train_acc: 0.9156, test_loss: 0.7458, test_acc: 0.8484\n",
            "\n",
            "epoch 33/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.4008, train_acc: 0.9155, test_loss: 0.7412, test_acc: 0.8478\n",
            "\n",
            "epoch 34/164:\n",
            "iteration: 200/200, cost_time: 82s, train_loss: 1.4008, train_acc: 0.9174, test_loss: 0.7953, test_acc: 0.8404\n",
            "\n",
            "epoch 35/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3864, train_acc: 0.9211, test_loss: 0.7492, test_acc: 0.8429\n",
            "\n",
            "epoch 36/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3959, train_acc: 0.9200, test_loss: 0.7577, test_acc: 0.8433\n",
            "\n",
            "epoch 37/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3836, train_acc: 0.9207, test_loss: 0.7678, test_acc: 0.8437\n",
            "\n",
            "epoch 38/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3845, train_acc: 0.9223, test_loss: 0.7380, test_acc: 0.8469\n",
            "\n",
            "epoch 39/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3869, train_acc: 0.9233, test_loss: 0.7534, test_acc: 0.8503\n",
            "\n",
            "epoch 40/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3866, train_acc: 0.9237, test_loss: 0.7645, test_acc: 0.8394\n",
            "\n",
            "epoch 41/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3800, train_acc: 0.9254, test_loss: 0.7317, test_acc: 0.8549\n",
            "\n",
            "epoch 42/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3831, train_acc: 0.9256, test_loss: 0.7445, test_acc: 0.8486\n",
            "\n",
            "epoch 43/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3831, train_acc: 0.9278, test_loss: 0.7771, test_acc: 0.8401\n",
            "\n",
            "epoch 44/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3837, train_acc: 0.9233, test_loss: 0.7666, test_acc: 0.8400\n",
            "\n",
            "epoch 45/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3705, train_acc: 0.9276, test_loss: 0.7299, test_acc: 0.8516\n",
            "\n",
            "epoch 46/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3744, train_acc: 0.9316, test_loss: 0.7193, test_acc: 0.8544\n",
            "\n",
            "epoch 47/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3668, train_acc: 0.9297, test_loss: 0.7260, test_acc: 0.8513\n",
            "\n",
            "epoch 48/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3780, train_acc: 0.9296, test_loss: 0.7163, test_acc: 0.8569\n",
            "\n",
            "epoch 49/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3820, train_acc: 0.9297, test_loss: 0.7135, test_acc: 0.8499\n",
            "\n",
            "epoch 50/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3723, train_acc: 0.9305, test_loss: 0.7163, test_acc: 0.8606\n",
            "\n",
            "epoch 51/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3666, train_acc: 0.9317, test_loss: 0.7229, test_acc: 0.8595\n",
            "\n",
            "epoch 52/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3718, train_acc: 0.9325, test_loss: 0.7557, test_acc: 0.8405\n",
            "\n",
            "epoch 53/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3679, train_acc: 0.9320, test_loss: 0.7465, test_acc: 0.8458\n",
            "\n",
            "epoch 54/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3580, train_acc: 0.9329, test_loss: 0.7251, test_acc: 0.8548\n",
            "\n",
            "epoch 55/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3606, train_acc: 0.9338, test_loss: 0.6866, test_acc: 0.8716\n",
            "\n",
            "epoch 56/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3644, train_acc: 0.9342, test_loss: 0.7551, test_acc: 0.8492\n",
            "\n",
            "epoch 57/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3717, train_acc: 0.9336, test_loss: 0.6889, test_acc: 0.8656\n",
            "\n",
            "epoch 58/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3724, train_acc: 0.9342, test_loss: 0.6865, test_acc: 0.8645\n",
            "\n",
            "epoch 59/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3621, train_acc: 0.9359, test_loss: 0.6888, test_acc: 0.8601\n",
            "\n",
            "epoch 60/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.3646, train_acc: 0.9349, test_loss: 0.6847, test_acc: 0.8659\n",
            "\n",
            "epoch 61/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.3548, train_acc: 0.9368, test_loss: 0.7039, test_acc: 0.8617\n",
            "\n",
            "epoch 62/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.3542, train_acc: 0.9352, test_loss: 0.7062, test_acc: 0.8532\n",
            "\n",
            "epoch 63/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3570, train_acc: 0.9362, test_loss: 0.6958, test_acc: 0.8629\n",
            "\n",
            "epoch 64/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3463, train_acc: 0.9365, test_loss: 0.6645, test_acc: 0.8657\n",
            "\n",
            "epoch 65/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3546, train_acc: 0.9368, test_loss: 0.7119, test_acc: 0.8541\n",
            "\n",
            "epoch 66/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.3634, train_acc: 0.9371, test_loss: 0.7253, test_acc: 0.8463\n",
            "\n",
            "epoch 67/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3567, train_acc: 0.9398, test_loss: 0.7274, test_acc: 0.8498\n",
            "\n",
            "epoch 68/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3704, train_acc: 0.9364, test_loss: 0.6978, test_acc: 0.8566\n",
            "\n",
            "epoch 69/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3490, train_acc: 0.9368, test_loss: 0.6611, test_acc: 0.8626\n",
            "\n",
            "epoch 70/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3563, train_acc: 0.9372, test_loss: 0.6842, test_acc: 0.8671\n",
            "\n",
            "epoch 71/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3475, train_acc: 0.9380, test_loss: 0.6572, test_acc: 0.8650\n",
            "\n",
            "epoch 72/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3617, train_acc: 0.9403, test_loss: 0.6884, test_acc: 0.8572\n",
            "\n",
            "epoch 73/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3496, train_acc: 0.9409, test_loss: 0.7062, test_acc: 0.8477\n",
            "\n",
            "epoch 74/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3612, train_acc: 0.9374, test_loss: 0.6626, test_acc: 0.8665\n",
            "\n",
            "epoch 75/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3602, train_acc: 0.9365, test_loss: 0.6836, test_acc: 0.8572\n",
            "\n",
            "epoch 76/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3499, train_acc: 0.9382, test_loss: 0.6414, test_acc: 0.8716\n",
            "\n",
            "epoch 77/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3578, train_acc: 0.9393, test_loss: 0.7225, test_acc: 0.8506\n",
            "\n",
            "epoch 78/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.3488, train_acc: 0.9402, test_loss: 0.6726, test_acc: 0.8648\n",
            "\n",
            "epoch 79/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3598, train_acc: 0.9382, test_loss: 0.6447, test_acc: 0.8779\n",
            "\n",
            "epoch 80/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.3453, train_acc: 0.9392, test_loss: 0.6775, test_acc: 0.8627\n",
            "\n",
            "epoch 81/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2914, train_acc: 0.9410, test_loss: 0.5294, test_acc: 0.9090\n",
            "\n",
            "epoch 82/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2625, train_acc: 0.9549, test_loss: 0.5005, test_acc: 0.9133\n",
            "\n",
            "epoch 83/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.2481, train_acc: 0.9596, test_loss: 0.4777, test_acc: 0.9159\n",
            "\n",
            "epoch 84/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2459, train_acc: 0.9645, test_loss: 0.4693, test_acc: 0.9140\n",
            "\n",
            "epoch 85/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2366, train_acc: 0.9667, test_loss: 0.4608, test_acc: 0.9189\n",
            "\n",
            "epoch 86/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2328, train_acc: 0.9698, test_loss: 0.4555, test_acc: 0.9186\n",
            "\n",
            "epoch 87/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.2355, train_acc: 0.9711, test_loss: 0.4472, test_acc: 0.9176\n",
            "\n",
            "epoch 88/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.2235, train_acc: 0.9739, test_loss: 0.4428, test_acc: 0.9192\n",
            "\n",
            "epoch 89/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.2138, train_acc: 0.9744, test_loss: 0.4339, test_acc: 0.9205\n",
            "\n",
            "epoch 90/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2174, train_acc: 0.9759, test_loss: 0.4353, test_acc: 0.9193\n",
            "\n",
            "epoch 91/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2212, train_acc: 0.9776, test_loss: 0.4271, test_acc: 0.9191\n",
            "\n",
            "epoch 92/164:\n",
            "iteration: 200/200, cost_time: 80s, train_loss: 1.2063, train_acc: 0.9794, test_loss: 0.4165, test_acc: 0.9217\n",
            "\n",
            "epoch 93/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2107, train_acc: 0.9804, test_loss: 0.4157, test_acc: 0.9222\n",
            "\n",
            "epoch 94/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2099, train_acc: 0.9806, test_loss: 0.4126, test_acc: 0.9237\n",
            "\n",
            "epoch 95/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.2070, train_acc: 0.9812, test_loss: 0.4057, test_acc: 0.9209\n",
            "\n",
            "epoch 96/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2057, train_acc: 0.9822, test_loss: 0.4037, test_acc: 0.9240\n",
            "\n",
            "epoch 97/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2001, train_acc: 0.9833, test_loss: 0.4104, test_acc: 0.9187\n",
            "\n",
            "epoch 98/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2060, train_acc: 0.9836, test_loss: 0.4102, test_acc: 0.9205\n",
            "\n",
            "epoch 99/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2021, train_acc: 0.9848, test_loss: 0.4047, test_acc: 0.9207\n",
            "\n",
            "epoch 100/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1964, train_acc: 0.9855, test_loss: 0.3987, test_acc: 0.9223\n",
            "\n",
            "epoch 101/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1993, train_acc: 0.9860, test_loss: 0.4024, test_acc: 0.9209\n",
            "\n",
            "epoch 102/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1938, train_acc: 0.9866, test_loss: 0.4039, test_acc: 0.9201\n",
            "\n",
            "epoch 103/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.2031, train_acc: 0.9865, test_loss: 0.3986, test_acc: 0.9219\n",
            "\n",
            "epoch 104/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1960, train_acc: 0.9867, test_loss: 0.3921, test_acc: 0.9225\n",
            "\n",
            "epoch 105/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1942, train_acc: 0.9874, test_loss: 0.3973, test_acc: 0.9196\n",
            "\n",
            "epoch 106/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1890, train_acc: 0.9879, test_loss: 0.4001, test_acc: 0.9198\n",
            "\n",
            "epoch 107/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1914, train_acc: 0.9883, test_loss: 0.3962, test_acc: 0.9200\n",
            "\n",
            "epoch 108/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1893, train_acc: 0.9885, test_loss: 0.3963, test_acc: 0.9180\n",
            "\n",
            "epoch 109/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1973, train_acc: 0.9876, test_loss: 0.3990, test_acc: 0.9199\n",
            "\n",
            "epoch 110/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1914, train_acc: 0.9890, test_loss: 0.3926, test_acc: 0.9186\n",
            "\n",
            "epoch 111/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1860, train_acc: 0.9889, test_loss: 0.4000, test_acc: 0.9177\n",
            "\n",
            "epoch 112/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1880, train_acc: 0.9892, test_loss: 0.3832, test_acc: 0.9210\n",
            "\n",
            "epoch 113/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1892, train_acc: 0.9911, test_loss: 0.3964, test_acc: 0.9179\n",
            "\n",
            "epoch 114/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1804, train_acc: 0.9914, test_loss: 0.3941, test_acc: 0.9197\n",
            "\n",
            "epoch 115/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1872, train_acc: 0.9907, test_loss: 0.3843, test_acc: 0.9210\n",
            "\n",
            "epoch 116/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1833, train_acc: 0.9913, test_loss: 0.3961, test_acc: 0.9195\n",
            "\n",
            "epoch 117/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1822, train_acc: 0.9909, test_loss: 0.3963, test_acc: 0.9175\n",
            "\n",
            "epoch 118/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1826, train_acc: 0.9910, test_loss: 0.3903, test_acc: 0.9177\n",
            "\n",
            "epoch 119/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1733, train_acc: 0.9911, test_loss: 0.3998, test_acc: 0.9160\n",
            "\n",
            "epoch 120/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1867, train_acc: 0.9912, test_loss: 0.3957, test_acc: 0.9181\n",
            "\n",
            "epoch 121/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1793, train_acc: 0.9887, test_loss: 0.3763, test_acc: 0.9250\n",
            "\n",
            "epoch 122/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1721, train_acc: 0.9910, test_loss: 0.3720, test_acc: 0.9261\n",
            "\n",
            "epoch 123/164:\n",
            "iteration: 200/200, cost_time: 79s, train_loss: 1.1781, train_acc: 0.9911, test_loss: 0.3687, test_acc: 0.9274\n",
            "\n",
            "epoch 124/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1769, train_acc: 0.9919, test_loss: 0.3673, test_acc: 0.9275\n",
            "\n",
            "epoch 125/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1689, train_acc: 0.9928, test_loss: 0.3654, test_acc: 0.9284\n",
            "\n",
            "epoch 126/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1792, train_acc: 0.9924, test_loss: 0.3644, test_acc: 0.9285\n",
            "\n",
            "epoch 127/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1673, train_acc: 0.9932, test_loss: 0.3642, test_acc: 0.9281\n",
            "\n",
            "epoch 128/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1674, train_acc: 0.9932, test_loss: 0.3645, test_acc: 0.9280\n",
            "\n",
            "epoch 129/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1768, train_acc: 0.9934, test_loss: 0.3651, test_acc: 0.9279\n",
            "\n",
            "epoch 130/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1697, train_acc: 0.9939, test_loss: 0.3623, test_acc: 0.9283\n",
            "\n",
            "epoch 131/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1621, train_acc: 0.9939, test_loss: 0.3628, test_acc: 0.9276\n",
            "\n",
            "epoch 132/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1722, train_acc: 0.9932, test_loss: 0.3625, test_acc: 0.9274\n",
            "\n",
            "epoch 133/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1719, train_acc: 0.9941, test_loss: 0.3645, test_acc: 0.9267\n",
            "\n",
            "epoch 134/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1658, train_acc: 0.9941, test_loss: 0.3627, test_acc: 0.9271\n",
            "\n",
            "epoch 135/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1692, train_acc: 0.9945, test_loss: 0.3638, test_acc: 0.9279\n",
            "\n",
            "epoch 136/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1709, train_acc: 0.9941, test_loss: 0.3617, test_acc: 0.9284\n",
            "\n",
            "epoch 137/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1662, train_acc: 0.9946, test_loss: 0.3610, test_acc: 0.9284\n",
            "\n",
            "epoch 138/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1703, train_acc: 0.9946, test_loss: 0.3616, test_acc: 0.9270\n",
            "\n",
            "epoch 139/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1702, train_acc: 0.9948, test_loss: 0.3607, test_acc: 0.9273\n",
            "\n",
            "epoch 140/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1645, train_acc: 0.9950, test_loss: 0.3611, test_acc: 0.9270\n",
            "\n",
            "epoch 141/164:\n",
            "iteration: 200/200, cost_time: 84s, train_loss: 1.1627, train_acc: 0.9953, test_loss: 0.3593, test_acc: 0.9281\n",
            "\n",
            "epoch 142/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1675, train_acc: 0.9949, test_loss: 0.3604, test_acc: 0.9279\n",
            "\n",
            "epoch 143/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1687, train_acc: 0.9948, test_loss: 0.3598, test_acc: 0.9280\n",
            "\n",
            "epoch 144/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1589, train_acc: 0.9950, test_loss: 0.3602, test_acc: 0.9273\n",
            "\n",
            "epoch 145/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1587, train_acc: 0.9956, test_loss: 0.3598, test_acc: 0.9273\n",
            "\n",
            "epoch 146/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1646, train_acc: 0.9952, test_loss: 0.3607, test_acc: 0.9273\n",
            "\n",
            "epoch 147/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1682, train_acc: 0.9955, test_loss: 0.3610, test_acc: 0.9271\n",
            "\n",
            "epoch 148/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1737, train_acc: 0.9951, test_loss: 0.3602, test_acc: 0.9279\n",
            "\n",
            "epoch 149/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1604, train_acc: 0.9954, test_loss: 0.3611, test_acc: 0.9270\n",
            "\n",
            "epoch 150/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1524, train_acc: 0.9957, test_loss: 0.3580, test_acc: 0.9273\n",
            "\n",
            "epoch 151/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1671, train_acc: 0.9958, test_loss: 0.3579, test_acc: 0.9271\n",
            "\n",
            "epoch 152/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1638, train_acc: 0.9954, test_loss: 0.3578, test_acc: 0.9280\n",
            "\n",
            "epoch 153/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1626, train_acc: 0.9956, test_loss: 0.3579, test_acc: 0.9279\n",
            "\n",
            "epoch 154/164:\n",
            "iteration: 200/200, cost_time: 77s, train_loss: 1.1625, train_acc: 0.9955, test_loss: 0.3585, test_acc: 0.9277\n",
            "\n",
            "epoch 155/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1621, train_acc: 0.9957, test_loss: 0.3573, test_acc: 0.9280\n",
            "\n",
            "epoch 156/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1672, train_acc: 0.9956, test_loss: 0.3581, test_acc: 0.9283\n",
            "\n",
            "epoch 157/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1636, train_acc: 0.9958, test_loss: 0.3576, test_acc: 0.9280\n",
            "\n",
            "epoch 158/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1627, train_acc: 0.9956, test_loss: 0.3574, test_acc: 0.9279\n",
            "\n",
            "epoch 159/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1639, train_acc: 0.9962, test_loss: 0.3588, test_acc: 0.9272\n",
            "\n",
            "epoch 160/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1609, train_acc: 0.9957, test_loss: 0.3573, test_acc: 0.9268\n",
            "\n",
            "epoch 161/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1605, train_acc: 0.9959, test_loss: 0.3576, test_acc: 0.9278\n",
            "\n",
            "epoch 162/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1758, train_acc: 0.9957, test_loss: 0.3575, test_acc: 0.9279\n",
            "\n",
            "epoch 163/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1535, train_acc: 0.9961, test_loss: 0.3588, test_acc: 0.9271\n",
            "\n",
            "epoch 164/164:\n",
            "iteration: 200/200, cost_time: 78s, train_loss: 1.1638, train_acc: 0.9962, test_loss: 0.3582, test_acc: 0.9283\n",
            "Model saved in file: ./model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iMOrFTUM3KUM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "9GmycezuI1ov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run Tensorflow in the background - note that we specify the log \n",
        "# directory we want to look at\n",
        "# vgg_logs14\n",
        "LOG_DIR = 'vgg_logs16'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "na3znlzmI2xL",
        "colab_type": "code",
        "outputId": "7a100f7d-ce1b-4dc7-e995-293e003e2da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# Download and unzip ngrok - you will only need to do this once per session\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-11 23:07:30--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.22.145.207, 52.203.53.176, 52.203.102.189, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.22.145.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.9’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  8.57MB/s    in 0.6s    \n",
            "\n",
            "2018-12-11 23:07:35 (8.57 MB/s) - ‘ngrok-stable-linux-amd64.zip.9’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "addiYD_SI26U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Launch the ngrok background process\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwMMwLarI3DY",
        "colab_type": "code",
        "outputId": "1ae11186-7288-4889-dbe0-1819140b51fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the public URL and be sorted!\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://61f7d0c8.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AF4fPmM7DezN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}